(back to [README.md](README.md) and [README_2.md](README_2.md) for other categories)
## Overview

- [Citation](#citation)
- [Multi-Modality](#multi-modality)
    - [Visual Captioning](#visual-captioning)
    - [Visual Question Answering](#visual-question-answering)
    - [Visual Grounding](#visual-grounding)
    - [Multi-Modal Representation Learning](#multi-modal-representation-learning)
    - [Multi-Modal Retrieval](#multi-modal-retrieval)
    - [Multi-Modal Generation](#multi-modal-generation)
    - [Prompt Learning/Tuning](#prompt-learningtuning)
    - [Visual Document Understanding](#visual-document-understanding)
    - [Other Multi-Modal Tasks](#other-multi-modal-tasks)

---

## Citation
If you find this repository useful, please consider citing this list:
```
@misc{chen2022transformerpaperlist,
    title = {Ultimate awesome paper list: transformer and attention},
    author = {Chen, Min-Hung},
    journal = {GitHub repository},
    url = {https://github.com/cmhungsteve/Awesome-Transformer-Attention},
    year = {2022},
}
```

---

## Multi-Modality
### Visual Captioning
* General:
    * **SAT**: "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", ICML, 2015. [[paper](https://arxiv.org/abs/1502.03044)] 
    * **ETA-Transformer**: "Entangled Transformer for Image Captioning", ICCV, 2019 (*UTS*). [[Paper](https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Entangled_Transformer_for_Image_Captioning_ICCV_2019_paper.html)]
    * **M2-Transformer**: "Meshed-Memory Transformer for Image Captioning", CVPR, 2020 (*UniMoRE*). [[Paper](https://arxiv.org/abs/1912.08226)][[PyTorch](https://github.com/aimagelab/meshed-memory-transformer)] 
    * **MCCFormers**: "Describing and Localizing Multiple Changes with Transformers", ICCV, 2021 (*AIST*). [[Paper](https://arxiv.org/abs/2103.14146)][[Website](https://cvpaperchallenge.github.io/Describing-and-Localizing-Multiple-Change-with-Transformers/)]
    * **SATIC**: "Semi-Autoregressive Transformer for Image Captioning", ICCVW, 2021 (*Hefei University of Technology*). [[Paper](https://arxiv.org/abs/2106.09436)][[PyTorch](https://github.com/YuanEZhou/satic)]
    * **DGCN**: "Dual Graph Convolutional Networks with Transformer and Curriculum Learning for Image Captioning", ACMMM, 2021 (*Wuhan University*). [[Paper](https://arxiv.org/abs/2108.02366)]
    * **CPTR**: "CPTR: Full Transformer Network for Image Captioning", arXiv, 2021 (*CAS*). [[Paper](https://arxiv.org/abs/2101.10804)] 
    * **ReFormer**: "ReFormer: The Relational Transformer for Image Captioning", arXiv, 2021 (*Stony Brook University*). [[Paper](https://arxiv.org/abs/2107.14178)]
    * **LAViTeR**: "LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation", arXiv, 2021 (*University at Buffalo*). [[Paper](https://arxiv.org/abs/2109.04993)]
    * **LATGeO**: "Label-Attention Transformer with Geometrically Coherent Objects for Image Captioning", arXiv, 2021 (*Gwangju Institute of Science and Technology*). [[Paper](https://arxiv.org/abs/2109.07799)]
    * **GEVST**: "Geometry-Entangled Visual Semantic Transformer for Image Captioning", arXiv, 2021 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2109.14137)]
    * **GAT**: "Geometry Attention Transformer with Position-aware LSTMs for Image Captioning", arXiv, 2021 (*University of Electronic Science and Technology of China*). [[Paper](https://arxiv.org/abs/2110.00335)]
    * **PureT**: "End-to-End Transformer Based Model for Image Captioning", AAAI, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2203.15350)]
    * **VisualGPT**: "VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning", CVPR, 2022 (*KAUST*). [[Paper](https://arxiv.org/abs/2102.10407)][[PyTorch](https://github.com/Vision-CAIR/VisualGPT)]
    * **ViTCAP**: "Injecting Semantic Concepts into End-to-End Image Captioning", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2112.05230)]
    * **CLIP-Event**: "CLIP-Event: Connecting Text and Images with Event Structures", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2201.05078)][[PyTorch](https://github.com/limanling/clip-event)]
    * **?**: "Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning", CVPR, 2022 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2205.04363)][[PyTorch](https://github.com/GT-RIPL/Xmodal-Ctx)]
    * **CLIP4IDC**: "CLIP4IDC: CLIP for Image Difference Captioning", CVPRW, 2022 (*Aalto University, Finland*). [[Paper](https://arxiv.org/abs/2206.00629)][[Code (in construction)](https://github.com/sushizixin/CLIP4IDC)]
    * **?**: "A Dual-Attentive Approach to Style-Based Image Captioning Using a CNN-Transformer Model", CVPRW, 2022 (*The University of the West Indies, Jamaica*). [[Paper](https://drive.google.com/file/d/1QYq69dBFMBKHYDUolZqPaermiFz67k77/view)]
    * **SpaCap3D**: "Spatiality-guided Transformer for 3D Dense Captioning on Point Clouds", IJCAI, 2022 (*University of Sydney*). [[Paper](https://arxiv.org/abs/2204.10688)][[Code (in construction)](https://github.com/heng-hw/SpaCap3D)][[Website](https://spacap3d.github.io/)]
    * **RA-Transformer**: "Retrieval-Augmented Transformer for Image Captioning", International Conference on Content-based Multimedia Indexing (CMBI), 2022 (*University of Modena and Reggio Emilia, Italy*). [[Paper](https://arxiv.org/abs/2207.13162)]
    * **GRIT**: "GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features", ECCV, 2022 (*Tohoku University + RIKEN AIP*). [[Paper](https://arxiv.org/abs/2207.09666)][[PyTorch](https://github.com/davidnvq/grit)]
    * **?**: "Object-Centric Unsupervised Image Captioning", ECCV, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2112.00969)][[PyTorch](https://github.com/zihangm/obj-centric-unsup-caption)]
    * **UEDVC**: "Unifying Event Detection and Captioning as Sequence Generation via Pre-Training", ECCV, 2022 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2207.08625)][[PyTorch](https://github.com/QiQAng/UEDVC)]
    * **TIger**: "Explicit Image Caption Editing", ECCV, 2022 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2207.09625)][[Code](https://github.com/baaaad/ECE)]
    * **DML**: "Learning Distinct and Representative Modes for Image Captioning", NeurIPS, 2022 (*University of Adelaide, Australia*). [[Paper](https://arxiv.org/abs/2209.08231)]
    * **P2C**: "Paraphrasing Is All You Need for Novel Object Captioning", NeurIPS, 2022 (*NTU + CMU*). [[Paper](https://arxiv.org/abs/2209.12343)]
    * **BEST**: "Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning", NeurIPS, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2206.01843)]
    * **CapDec**: "Text-Only Training for Image Captioning using Noise-Injected CLIP", EMNLP, 2022 (*Tel Aviv*). [[Paper](https://arxiv.org/abs/2211.00575)][[Pytorch](https://github.com/DavidHuji/CapDec)]
    * **?**: "Focus! Relevant and Sufficient Context Selection for News Image Captioning", EMNLP Findings, 2022 (*UC Davis*). [[Paper](https://arxiv.org/abs/2212.00843)]
    * **CVLNM**: "Learning to Collocate Visual-Linguistic Neural Modules for Image Captioning", IJCV, 2022 (*Southeast University, China*). [[Paper](https://arxiv.org/abs/2210.01338)][[PyTorch](https://github.com/GCYZSL/CVLMN)]
    * **ViNTER**: "ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer", arXiv, 2022 (*The University of Tokyo*). [[Paper](https://arxiv.org/abs/2202.07305)]
    * **VaT**: "Variational Transformer: A Framework Beyond the Trade-off between Accuracy and Diversity for Image Captioning", arXiv, 2022 (*Tongji University*). [[Paper](https://arxiv.org/abs/2205.14458)]
    * **SCST-GEG**: "Distincive Image Captioning via CLIP Guided Group Optimization", arXiv, 2022 (*McGill University*). [[Paper](https://arxiv.org/abs/2208.04254)]
    * **?**: "Vision Transformer Based Model for Describing a Set of Images as a Story", arXiv, 2022 (*The University of Western Australia*). [[Paper](https://arxiv.org/abs/2210.02762)]
    * **CLM**: "Zero-shot Image Captioning by Anchor-augmented Vision-Language Space Alignment", arXiv, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2211.07275)]
    * **PromptCap**: "PromptCap: Prompt-Guided Task-Aware Image Captioning", arXiv, 2022 (*UW*). [[Paper](https://arxiv.org/abs/2211.09699)]
    * **PTSN**: "Progressive Tree-Structured Prototype Network for End-to-End Image Captioning", arXiv, 2022 (*University of Electronic Science and Technology of China (UESTC)*). [[Paper](https://arxiv.org/abs/2211.09460)][[PyTorch (in construction)](https://github.com/NovaMind-Z/PTSN)]
    * **DDCap**: "Exploring Discrete Diffusion Models for Image Captioning", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2211.11694)][[PyTorch](https://github.com/buxiangzhiren/DDCap)]
    * **ARIC**: "Aesthetically Relevant Image Captioning", AAAI, 2023 (*Shenzhen University*). [[Paper](https://arxiv.org/abs/2211.15378)][[Code (in construction)](https://github.com/PengZai/ARIC)]
    * **UAIC**: "Uncertainty-Aware Image Captioning", AAAI, 2023 (*Meituan*). [[Paper](https://arxiv.org/abs/2211.16769)]
    * **LiMBeR**: "Linearly Mapping from Image to Text Space", ICLR, 2023 (*Brown University*). [[Paper](https://arxiv.org/abs/2209.15162)]
    * **DiscriTune**: "Cross-Domain Image Captioning with Discriminative Finetuning", CVPR, 2023 (*Universitat Pompeu Fabra (UPF), Spain*). [[Paper](https://arxiv.org/abs/2304.01662)]
    * **LIBRA**: "Model-Agnostic Gender Debiased Image Captioning", CVPR, 2023 (*Osaka University*). [[Paper](https://arxiv.org/abs/2304.03693)]
    * **A-CAP**: "A-CAP: Anticipation Captioning with Commonsense Knowledge", CVPR, 2023 (*The University of Tokyo*). [[Paper](https://arxiv.org/abs/2304.06602)]
    * **HAAV**: "HAAV: Hierarchical Aggregation of Augmented Views for Image Captioning", CVPR, 2023 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2305.16295)][[Website](https://sites.google.com/view/chiawen-kuo/home/haav)]
    * **?**: "Cross-Domain Image Captioning with Discriminative Finetuning", CVPR, 2023 (*Universitat Pompeu Fabra (UPF), Spain*). [[Paper](https://arxiv.org/abs/2304.01662)]
    * **PAC-S**: "Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation", CVPR, 2023 (*UniMoRE, Italy*). [[Paper](https://arxiv.org/abs/2303.12112)][[PyTorch](https://github.com/aimagelab/pacscore)]
    * **SCD-Net**: "Semantic-Conditional Diffusion Networks for Image Captioning", CVPR, 2023 (*JD*). [[Paper](https://arxiv.org/abs/2212.03099)][[PyTorch](https://github.com/jianjieluo/SCD-Net)]
    * **ConZIC**: "ConZIC: Controllable Zero-shot Image Captioning by Sampling-Based Polishing", CVPR, 2023 (*Xidian University*). [[Paper](https://arxiv.org/abs/2303.02437)][[PyTorch](https://github.com/joeyz0z/ConZIC)]
    * **SmallCap**: "SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation", CVPR, 2023 (*University of Lisbon, Portugal*). [[Paper](https://arxiv.org/abs/2209.15323)][[PyTorch](https://github.com/RitaRamo/smallcap)]
    * **LSML**: "Crossing the Gap: Domain Generalization for Image Captioning", CVPR, 2023 (*USTC*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Ren_Crossing_the_Gap_Domain_Generalization_for_Image_Captioning_CVPR_2023_paper.html)]
    * **MuE**: "You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model", CVPR, 2023 (*NC State*). [[Paper](https://arxiv.org/abs/2211.11152)]
    * **OxfordTVG-HIC**: "OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?", ICCV, 2023 (*Oxford*). [[Paper](https://arxiv.org/abs/2307.11636)][[Website](https://torrvision.com/tvghic/)]
    * **?**: "Guiding Image Captioning Models Toward More Specific Captions", ICCV, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2307.16686)]
    * **ViECap**: "Transferable Decoding with Visual Entities for Zero-Shot Image Captioning", ICCV, 2023 (*Southern University of Science and Technology*). [[Paper](https://arxiv.org/abs/2307.16525)][[Code (in construction)](https://github.com/FeiElysia/ViECap)]
    * **PMA-Net**: "With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning", ICCV, 2023 (*University of Modena and Reggio Emilia (UniMoRE), Italy*). [[Paper](https://arxiv.org/abs/2308.12383)][[Code (in construction)](https://github.com/aimagelab/PMA-Net)]
    * **TSG**: "Transforming Visual Scene Graphs to Image Captions", ACL, 2023 (*Southeast University, China*). [[Paper](https://arxiv.org/abs/2305.02177)][[PyTorch](https://anonymous.4open.science/r/ACL23_TSG/README.md)]
    * **InfoMetIC**: "InfoMetIC: An Informative Metric for Reference-free Image Caption Evaluation", ACL, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2305.06002)][[Code (in construction)](https://github.com/HAWLYQ/InfoMetIC)]
    * **MultiCapCLIP**: "MultiCapCLIP: Auto-Encoding Prompts for Zero-Shot Multilingual Visual Captioning", ACL, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2308.13218)][[PyTorch (in construction)](https://github.com/yangbang18/MultiCapCLIP)]
    * **Cur-VL**: "Learning from Children: Improving Image-Caption Pretraining via Curriculum", ACL Findings, 2023 (*Columbia*). [[Paper](https://arxiv.org/abs/2305.17540)][[Code (in construction)](https://github.com/hayyubi/cur_vl)]
    * **?**: "Text-Only Training for Visual Storytelling", ACMMM, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2308.08881)]
    * **CgT-GAN**: "CgT-GAN: CLIP-guided Text GAN for Image Captioning", ACMMM, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2308.12045)][[PyTorch](https://github.com/Lihr747/CgtGAN)]
    * **Re-ViLM**: "Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2302.04858)]
    * **Knight**: "From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2304.13273)][[PyTorch](https://github.com/junyangwang0410/Knight)]
    * **VTT**: "Visual Transformation Telling", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2305.01928)]
    * **Caption-Anything**: "Caption Anything: Interactive Image Description with Diverse Multimodal Controls", arXiv, 2023 (*Southern University of Science and Technology*). [[Paper](https://arxiv.org/abs/2305.02677)][[PyTorch](https://github.com/ttengwang/Caption-Anything)]
    * **COLA**: "COLA: How to adapt vision-language models to Compose Objects Localized with Attributes?", arXiv, 2023 (*Boston*). [[Paper](https://arxiv.org/abs/2305.03689)]
    * **?**: "Data Curation for Image Captioning with Text-to-Image Generative Models", arXiv, 2023 (*University of Copenhagen, Denmark*). [[Paper](https://arxiv.org/abs/2305.03610)]
    * **TLC**: "Simple Token-Level Confidence Improves Caption Correctness", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2305.07021)]
    * **VIVID**: "Album Storytelling with Iterative Story-aware Captioning and Large Language Models", arXiv, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2305.12943)]
    * **MCDG**: "Text-Only Image Captioning with Multi-Context Data Generation", arXiv, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2305.18072)]
    * **FuseCap**: "FuseCap: Leveraging Large Language Models to Fuse Visual Data into Enriched Image Captions", arXiv, 2023 (*Israel Institute of Technology*). [[Paper](https://arxiv.org/abs/2305.17718)]
    * **StoryGen**: "Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2306.00973)][[PyTorch (in construction)](https://github.com/haoningwu3639/StoryGen)][[Website](https://haoningwu3639.github.io/StoryGen_Webpage/)]
    * **?**: "Improving Image Captioning Descriptiveness by Ranking and LLM-based Fusion", arXiv, 2023 (*University of Milano-Bicocca, Italy*). [[Paper](https://arxiv.org/abs/2306.11593)]
    * **SITTA**: "SITTA: A Semantic Image-Text Alignment for Image Captioning", arXiv, 2023 (*Johannes Kepler University, Austria*). [[Paper](https://arxiv.org/abs/2307.05591)][[PyTorch](https://github.com/ml-jku/semantic-image-text-alignment)]
    * **MMNS**: "Multimodal Neurons in Pretrained Text-Only Transformers", arXiv, 2023 (*MIT*). [[Paper](https://arxiv.org/abs/2308.01544)]
    * **RegionBLIP**: "RegionBLIP: A Unified Multi-modal Pre-training Framework for Holistic and Regional Comprehension", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2308.02299)][[PyTorch](https://github.com/mightyzau/RegionBLIP)]
    * **?**: "Visually-Aware Context Modeling for News Image Captioning", arXiv, 2023 (*KU Leuven*). [[Paper](https://arxiv.org/abs/2308.08325)]
* Video:
    * **Masked Transformers**: "End-to-End Dense Video Captioning with Masked Transformer", CVPR, 2018 (*UMich + Salesforce*). [[Paper](https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_End-to-End_Dense_Video_CVPR_2018_paper.html)][[PyTorch](https://github.com/salesforce/densecap)]
    * **BMT**: "A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer", BMVC, 2020 (*Tampere University, Finland*). [[Paper](https://arxiv.org/abs/2005.08271)][[PyTorch](https://github.com/v-iashin/bmt)][[Website](https://iashin.ai/bmt)]
    * **?**: "Optimizing Latency for Online Video Captioning Using Audio-Visual Transformers", Interspeech, 2021 (*MERL*). [[Paper](https://arxiv.org/abs/2108.02147)]
    * **PDVC**: "End-to-End Dense Video Captioning with Parallel Decoding", ICCV, 2021 (*HKU + Southern University of Science and Technology*). [[Paper](https://arxiv.org/abs/2108.07781)][[PyTorch](https://github.com/ttengwang/PDVC)]
    * **MV-GPT**: "End-to-end Generative Pretraining for Multimodal Video Captioning", CVPR, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2201.08264)]
    * **VGCL**: "Video-Guided Curriculum Learning for Spoken Video Grounding", ACMMM, 2022 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2209.00277)][[PyTorch](https://github.com/marmot-xy/Spoken-Video-Grounding)]
    * **UVC-VI**: "Aligning Source Visual and Target Language Domains for Unpaired Video Captioning", TPAMI, 2022 (*Peking University*). [[Paper](https://arxiv.org/abs/2211.12148)]
    * **D2**: "Dual-Level Decoupled Transformer for Video Captioning", arXiv, 2022 (*Northwestern Polytechnical University, China*). [[Paper](https://arxiv.org/abs/2205.03039)]
    * **VASTA**: "Diverse Video Captioning by Adaptive Spatio-temporal Attention", arXiv, 2022 (*University of Tubingen, Germany*). [[Paper](https://arxiv.org/abs/2208.09266)]
    * **VCRN**: "Visual Commonsense-aware Representation Network for Video Captioning", arXiv, 2022 (*University of Electronic Science and Technology of China (UESTC)*). [[Paper](https://arxiv.org/abs/2211.09469)][[PyTorch (in construction)](https://github.com/zchoi/VCRN)]
    * **RSFD**: "Refined Semantic Enhancement towards Frequency Diffusion for Video Captioning", arXiv, 2022 (*Wuhan University of Technology*). [[Paper](https://arxiv.org/abs/2211.15076)][[Code (in construction)](https://github.com/lzp870/RSFD)]
    * **VLTinT**: "VLTinT: Visual-Linguistic Transformer-in-Transformer for Coherent Video Paragraph Captioning", AAAI, 2023 (*University of Arkansas*). [[Paper](https://arxiv.org/abs/2211.15103)]
    * **Vid2Seq**: "Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2302.14115)][[Website](https://antoyang.github.io/vid2seq.html)]
    * **TextKG**: "Text with Knowledge Graph Augmented Transformer for Video Captioning", CVPR, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2303.12423)]
    * **G2L**: "G2L: Semantically Aligned and Uniform Video Grounding via Geodesic and Game Theory", ICCV, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2307.14277)]
    * **CoCap**: "Accurate and Fast Compressed Video Captioning", ICCV, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2309.12867)][[PyTorch](https://github.com/acherstyx/CoCap)]
    * **Movie101**: "Movie101: A New Movie Understanding Benchmark", ACL, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2305.12140)][[Code (in construction)](https://github.com/yuezih/Movie101)]
    * **VidChapters-7M**: "VidChapters-7M: Video Chapters at Scale", NeurIPS (Datasets and Benchmarks), 2023 (*INRIA*). [[Paper](https://arxiv.org/abs/2309.13952)][[PyTorch](https://github.com/antoyang/VidChapters)][[Website](https://antoyang.github.io/vidchapters.html)]
    * **?**: "Implicit and Explicit Commonsense for Multi-sentence Video Captioning", arXiv, 2023 (*UBC*). [[Paper](https://arxiv.org/abs/2303.07545)]
    * **Video-Verbalization**: "A Video Is Worth 4096 Tokens: Verbalize Story Videos To Understand Them In Zero Shot", arXiv, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2305.09758)]
    * **Dense-VOC**: "Dense Video Object Captioning from Disjoint Supervision", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2306.11729)]
    * **?**: "Exploring the Role of Audio in Video Captioning", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2306.12559)]
    * **ZeroTA**: "Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment", arXiv, 2023 (*KAIST*). [[Paper](https://arxiv.org/abs/2307.02682)]
* 3D:
    * **Vote2Cap-DETR**: "End-to-End 3D Dense Captioning with Vote2Cap-DETR", CVPR, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2301.02508)][[PyTorch](https://github.com/ch3cook-fdu/Vote2Cap-DETR)]
    * **Cap3D**: "Scalable 3D Captioning with Pretrained Models", arXiv, 2023 (*UMich*). [[Paper](https://arxiv.org/abs/2306.07279)][[Dataset](https://huggingface.co/datasets/tiange/Cap3D)]
    * **Vote2Cap-DETR++**: "Vote2Cap-DETR++: Decoupling Localization and Describing for End-to-End 3D Dense Captioning", arXiv, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2309.02999)][[PyTorch](https://github.com/ch3cook-fdu/Vote2Cap-DETR)]
* Others:
    * **ET-Cap**: "Explore and Tell: Embodied Visual Captioning in 3D Environments", ICCV, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2308.10447)][[Code (in construction)](https://github.com/HAWLYQ/ET-Cap)][[Website](https://aim3-ruc.github.io/ExploreAndTell/)]

[[Back to Overview](#overview)]

### Visual Question Answering
* General:
    * **MCAN**: "Deep Modular Co-Attention Networks for Visual Question Answering", CVPR, 2019 (*Hangzhou Dianzi University*). [[Paper](https://arxiv.org/abs/1906.10770)][[PyTorch](https://github.com/MILVLG/mcan-vqa)]
    * **M4C**: "Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA", CVPR, 2020 (*Facebook*). [[Paper](https://arxiv.org/abs/1911.06258)]
    * **SA-M4C**: "Spatially Aware Multimodal Transformers for TextVQA", ECCV, 2020 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2007.12146)][[PyTorch](https://github.com/yashkant/sam-textvqa)][[Website](https://yashkant.github.io/projects/sam-textvqa.html)]
    * **ConClaT**: "Contrast and Classify: Training Robust VQA Models", ICCV, 2021 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2010.06087)]
    * **TRAR**: "TRAR: Routing the Attention Spans in Transformer for Visual Question Answering", ICCV, 2021 (*Xiamen University*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Zhou_TRAR_Routing_the_Attention_Spans_in_Transformer_for_Visual_Question_ICCV_2021_paper.html)]
    * **UniQer**: "Unified Questioner Transformer for Descriptive Question Generation in Goal-Oriented Visual Dialogue", ICCV, 2021 (*Keio*). [[Paper](https://arxiv.org/abs/2106.15550)]
    * **TxT**: "TxT: Crossmodal End-to-End Learning with Transformers", GCPR, 2021 (*TU Darmstadt*). [[Paper](https://arxiv.org/abs/2109.04422)]
    * **ProTo**: "ProTo: Program-Guided Transformer for Program-Guided Tasks", NeurIPS, 2021 (*Georiga Tech*). [[Paper](https://arxiv.org/abs/2110.00804)]
    * **VisQA**: "VisQA: X-raying Vision and Language Reasoning in Transformers", arXiv, 2021 (*INSA-Lyon*). [[Paper](https://arxiv.org/abs/2104.00926)][[PyTorch](https://github.com/Theo-Jaunet/VisQA)]
    * **Block-Skim**: "Block-Skim: Efficient Question Answering for Transformer", AAAI, 2022 (* Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2112.08560)]
    * **RelViT**: "RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning", ICLR, 2022 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2204.11167)] [[PyTorch](https://github.com/NVlabs/RelViT)]
    * **Hypergraph-Transformer**: "Hypergraph Transformer: Weakly-supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering", ACL, 2022 (*SNU*). [[Paper](https://arxiv.org/abs/2204.10448)][[Code (in construction)](https://github.com/yujungheo/kbvqa-public)]
    * **X-Trans2Cap**: "X-Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning", CVPR, 2022 (*CUHK*). [[Paper](https://arxiv.org/abs/2203.00843)]
    * **UTC**: "UTC: A Unified Transformer with Inter-Task Contrastive Learning for Visual Dialog", CVPR, 2022 (*Fudan*). [[Paper](https://arxiv.org/abs/2205.00423)]
    * **LaTr**: "LaTr: Layout-Aware Transformer for Scene-Text VQA", CVPR, 2022 (*Amazon*). [[Paper](https://arxiv.org/abs/2112.12494)]
    * **QAA**: "Query and Attention Augmentation for Knowledge-Based Explainable Reasoning", CVPR, 2022 (*University of Minnesota*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Query_and_Attention_Augmentation_for_Knowledge-Based_Explainable_Reasoning_CVPR_2022_paper.html)][[PyTorch](https://github.com/SuperJohnZhang/QAA)]
    * **WebQA**: "WebQA: Multihop and Multimodal QA", CVPR, 2022 (*CMU + Microsoft*). [[Paper](https://arxiv.org/abs/2109.00590)][[PyTorch](https://github.com/WebQnA/WebQA_Baseline)][[Website](https://webqna.github.io/)]
    * **GPV-1**: "Towards General Purpose Vision Systems: An End-to-End Task-Agnostic Vision-Language Architecture", CVPR, 2022 (*Allen AI*). [[Paper](https://arxiv.org/abs/2104.00743)][[IEEE](https://ieeexplore.ieee.org/document/9880087)][[Github](https://github.com/allenai/gpv-1)]
    * **?**: "Efficient Adaptive Image-Language Learning for Visual Question Answering", CVPRW, 2022 (*Google*). [[Paper](https://drive.google.com/file/d/1SPeCqJ_Uzs_jk4yxxcSS8OOUKZmXf_Mt/view)]
    * **cViL**: "cViL: Cross-Lingual Training of Vision-Language Models using Knowledge Distillation", ICPR, 2022 (*IIIT, Hyderabad*). [[Paper](https://arxiv.org/abs/2206.03354)]
    * **Distinguishing-VQA**: "Overcoming Language Priors in Visual Question Answering via Distinguishing Superficially Similar Instances", COLING, 2022 (*Nankai University*). [[Paper](https://arxiv.org/abs/2209.08529)][[Code (in construction)](https://github.com/wyk-nku/Distinguishing-VQA)]
    * **?**: "Weakly Supervised Grounding for VQA in Vision-Language Transformers", ECCV, 2022 (*UCF*). [[Paper](https://arxiv.org/abs/2207.02334)][[PyTorch (in construction)](https://github.com/aurooj/WSG-VQA-VLTransformers)]
    * **MUST-VQA**: "MUST-VQA: MUltilingual Scene-text VQA", ECCVW, 2022 (*UAB, Spain*). [[Paper](https://arxiv.org/abs/2209.06730)]
    * **?**: "Training Vision-Language Models with Less Bimodal Supervision", Automated Knowledge Base Construction (AKBC), 2022 (*Tel Aviv*). [[Paper](https://arxiv.org/abs/2211.00262)]
    * **REVIVE**: "REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering", NeurIPS, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2206.01201)]
    * **ScienceQA**: "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering", NeurIPS, 2022 (*AI2*). [[Paper](https://arxiv.org/abs/2209.09513)][[PyTorch](https://github.com/lupantech/ScienceQA)][[Website](https://scienceqa.github.io/)]
    * **FrozenBiLM**: "Zero-Shot Video Question Answering via Frozen Bidirectional Language Models", NeurIPS, 2022 (*INRIA*). [[Paper](https://arxiv.org/abs/2206.08155)][[PyTorch](https://github.com/antoyang/FrozenBiLM)]
    * **MuRAG**: "MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text", EMNLP, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2210.02928)]
    * **MMBS**: "Towards Robust Visual Question Answering: Making the Most of Biased Samples via Contrastive Learning", EMNLP, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2210.04563)][[PyTorch](https://github.com/PhoebusSi/MMBS)]
    * **EnFoRe**: "Entity-Focused Dense Passage Retrieval for Outside-Knowledge Visual Question Answering", EMNLP, 2022 (*UT Austin*). [[Paper](https://arxiv.org/abs/2210.10176)]
    * **CRIPP-VQA**: "CRIPP-VQA: Counterfactual Reasoning about Implicit Physical Properties via Video Question Answering", EMNLP, 2022 (*Arizona State University*). [[Paper](https://arxiv.org/abs/2211.03779)][[PyTorch](https://github.com/Maitreyapatel/CRIPP-VQA/)][[Website](https://maitreyapatel.com/CRIPP-VQA/)]
    * **PnP-VQA**: "Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training", EMNLP Findings, 2022 (*Salesforce*). [[Paper](https://arxiv.org/abs/2210.08773)]
    * **TMN**: "Transformer Module Networks for Systematic Generalization in Visual Question Answering", arXiv, 2022 (*Fujitsu*). [[Paper](https://arxiv.org/abs/2201.11316)]
    * **?**: "On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering", arXiv, 2022 (*Birla Institute of Technology Mesra, India*). [[Paper](https://arxiv.org/abs/2201.03965)]
    * **DST**: "Towards Efficient and Elastic Visual Question Answering with Doubly Slimmable Transformer", arXiv, 2022 (*Hangzhou Dianzi University*). [[Paper](https://arxiv.org/abs/2203.12814)]
    * **PAVCR**: "Attention Mechanism based Cognition-level Scene Understanding", arXiv, 2022 (*Leibniz University of Hannover, Germany*). [[Paper](https://arxiv.org/abs/2204.08027)]
    * **TAG**: "TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation", arXiv, 2022 (*Maryland + Salesforce*). [[Paper](https://arxiv.org/abs/2208.01813)][[PyTorch](https://github.com/HenryJunW/TAG)]
    * **UniCon**: "UniCon: Unidirectional Split Learning with Contrastive Loss for Visual Question Answering", arXiv, 2022 (*University of Tokyo*). [[Paper](https://arxiv.org/abs/2208.11435)]
    * **CLOVE**: "Symbolic Replay: Scene Graph as Prompt for Continual Learning on VQA Task", arXiv, 2022 (*NUS*). [[Paper](https://arxiv.org/abs/2208.12037)][[Code (in construction)](https://github.com/showlab/CLVQA)]
    * **mVQA**: "Towards Multi-Lingual Visual Question Answering", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2209.05401)]
    * **CIB**: "Finetuning Pretrained Vision-Language Models with Correlation Information Bottleneck for Robust Visual Question Answering", arXiv, 2022 (*Xi'an Jiaotong University*). [[Paper](https://arxiv.org/abs/2209.06954)]
    * **?**: "Compressing And Debiasing Vision-Language Pre-Trained Models for Visual Question Answering", arXiv, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2210.14558)]
    * **VLR**: "Visually Grounded VQA by Lattice-based Retrieval", arXiv, 2022 (*University of Bremen, Germany*). [[Paper](https://arxiv.org/abs/2211.08086)]
    * **CMCL**: "Cross-Modal Contrastive Learning for Robust Reasoning in VQA", arxiv, 2022 (*University of Sydney*). [[Paper](https://arxiv.org/abs/2211.11190)][[PyTorch](https://github.com/qizhust/cmcl_vqa_pl)]
    * **CL-CrossVQA**: "CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering", arXiv, 2022 (*LMU Munich*). [[Paper](https://arxiv.org/abs/2211.10567)]
    * **OFA-X**: "Harnessing the Power of Multi-Task Pretraining for Ground-Truth Level Natural Language Explanations", arXiv, 2022 (*University of Hamburg, Germany*). [[Paper](https://arxiv.org/abs/2212.04231)][[Code (in construction)](https://github.com/ofa-x/OFA-X)]
    * **VLC-BERT**: "VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge", WACV, 2023 (*UBC, Canada*). [[Paper](https://arxiv.org/abs/2210.13626)][[PyTorch](https://github.com/aditya10/VLC-BERT)]
    * **LTG**: "Locate Then Generate: Bridging Vision and Language with Bounding Box for Scene-Text VQA", AAAI, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2304.01603)]
    * **SelTDA**: "Q: How to Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!", CVPR, 2023 (*NEC*). [[Paper](https://arxiv.org/abs/2306.03932)][[PyTorch](https://github.com/codezakh/SelTDA)]
    * **Prophet**: "Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering", CVPR, 2023 (*Hangzhou Dianzi University*). [[Paper](https://arxiv.org/abs/2303.01903)][[PyTorch](https://github.com/MILVLG/prophet)]
    * **GenB**: "Generative Bias for Robust Visual Question Answering", CVPR, 2023 (*KAIST*). [[Paper](https://arxiv.org/abs/2208.00690)]
    * **MixPHM**: "MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource Visual Question Answering", CVPR, 2023 (*Xi'an Jiaotong University*). [[Paper](https://arxiv.org/abs/2303.01239)]
    * **POEM**: "Divide and Conquer: Answering Questions with Object Factorization and Compositional Reasoning", CVPR, 2023 (*University of Minnesota (UMN)*). [[Paper](https://arxiv.org/abs/2303.10482)][[PyTorch](https://github.com/szzexpoi/POEM)]
    * **LYP**: "Improving Selective Visual Question Answering by Learning From Your Peers", CVPR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2306.08751)]
    * **VQACL**: "VQACL: A Novel Visual Question Answering Continual Learning Setting", CVPR, 2023 (*CAS*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_VQACL_A_Novel_Visual_Question_Answering_Continual_Learning_Setting_CVPR_2023_paper.html)][[PyTorch](https://github.com/zhangxi1997/VQACL)]
    * **Img2LLM**: "From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models", CVPR, 2023 (*Salesforce*). [[Paper](https://arxiv.org/abs/2212.10846)][[PyTorch](https://github.com/salesforce/LAVIS/tree/main/projects/img2llm-vqa)]
    * **Imp-VQA**: "Logical Implications for Visual Question Answering Consistency", CVPR, 2023 (*University of Bern, Switzerland*). [[Paper](https://arxiv.org/abs/2303.09427)][[PyTorch](https://github.com/sergiotasconmorales/imp_vqa)][[Website](https://sergiotasconmorales.github.io/conferences/cvpr2023.html)]
    * **RMLVQA**: "RMLVQA: A Margin Loss Approach For Visual Question Answering with Language Biases", CVPR, 2023 (*Indian Institute of Science*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Basu_RMLVQA_A_Margin_Loss_Approach_for_Visual_Question_Answering_With_CVPR_2023_paper.html)][[PyTorch](https://github.com/val-iisc/RMLVQA)]
    * **S3C**: "S3C: Semi-Supervised VQA Natural Language Explanation via Self-Critical Learning", CVPR, 2023 (*Northwestern Polytechnical University, China*). [[Paper](https://arxiv.org/abs/2309.02155)]
    * **?**: "Diversifying Joint Vision-Language Tokenization Learning", CVPRW, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2306.03421)]
    * **VQAAnswerTherapy**: "VQA Therapy: Exploring Answer Differences by Visually Grounding Answers", ICCV, 2023 (*UT Austin*). [[Paper](https://arxiv.org/abs/2308.11662)][[Website](https://vizwiz.org/tasks-and-datasets/vqa-answer-therapy/)]
    * **TwO**: "Combo of Thinking and Observing for Outside-Knowledge VQA", ACL, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2305.06407)][[Code (in construction)](https://github.com/PhoebusSi/Thinking-while-Observing)]
    * **Mod-Zero-VQA**: "Modularized Zero-shot VQA with Pre-trained Models", ACL Findings, 2023 (*Singapore Management University*). [[Paper](https://arxiv.org/abs/2305.17369)]
    * **SaL**: "Separate and Locate: Rethink the Text in Text-based Visual Question Answering", ACMMM, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2308.16383)][[Code (in construction)](https://github.com/fangbufang/SaL)]
    * **InfoSeek**: "Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2302.11713)][[Website](https://open-vision-language.github.io/infoseek/)]
    * **CoVGT**: "Contrastive Video Question Answering via Video Graph Transformer", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2302.13668)]
    * **RVQA**: "Toward Unsupervised Realistic Visual Question Answering", arXiv, 2023 (*UCSD*). [[Paper](https://arxiv.org/abs/2303.05068)]
    * **WHOOP**: "Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images", arXiv, 2023 (*Ben Gurion University of the Negev, Israel*). [[Paper](https://arxiv.org/abs/2303.07274)][[Website](https://whoops-benchmark.github.io/)]
    * **IVLT**: "Causality-aware Visual Scene Discovery for Cross-Modal Question Reasoning", arXiv, 2023 (*Sun-Yat-Sen University*). [[Paper](https://arxiv.org/abs/2304.08083)]
    * **MGT**: "Multimodal Graph Transformer for Multimodal Question Answering", arXiv, 2023 (*UC Santa Cruz*). [[Paper](https://arxiv.org/abs/2305.00581)]
    * **VCSR**: "Visual Causal Scene Refinement for Video Question Answering", arXiv, 2023 (*Sun-Yat-Sen University*). [[Paper](https://arxiv.org/abs/2305.04224)]
    * **SeeTRUE**: "What You See is What You Read? Improving Text-Image Alignment Evaluation", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2305.10400)][[PyTorch](https://github.com/yonatanbitton/wysiwyr)][[Website](https://wysiwyr-itm.github.io/)]
    * **JADE**: "Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2305.11769)]
    * **NuScenes-QA**: "NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario", arXiv, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2305.14836)][[Code (in construction)](https://github.com/qiantianwen/NuScenes-QA)]
    * **LAMOC**: "Zero-shot Visual Question Answering with Language Model Feedback", arXiv, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2305.17006)][[PyTorch](https://github.com/RUCAIBox/LAMOC)]
    * **PW-VQA**: "Unveiling Cross Modality Bias in Visual Question Answering: A Causal View with Possible Worlds VQA", arXiv, 2023 (*University of Rochester*). [[Paper](https://arxiv.org/abs/2305.19664)]
    * **Encyclopedic-VQA**: "Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2306.09224)]
    * **?**: "Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering", arXiv, 2023 (*Mila*). [[Paper](https://arxiv.org/abs/2306.09996)]
    * **R2A**: "Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2306.11732)]
    * **WikiTiLo**: "Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times and Location Reasoning", arXiv, 2023 (*LMU Munich*). [[Paper](https://arxiv.org/abs/2307.06166)]
    * **GenVQA**: "Generative Visual Question Answering", arXiv, 2023 (*UW*). [[Paper](https://arxiv.org/abs/2307.10405)]
    * **Context-VQA**: "Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering", arXiv, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2307.15745)]
    * **BLIVA**: "BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions", arXiv, 2023 (*USCD*). [[Paper](https://arxiv.org/abs/2308.09936)]
    * **NExT-GQA**: "Can I Trust Your Answer? Visually Grounded Video Question Answering", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2309.01327)]
    * **CURE**: "Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models", arXiv, 2023 (*SRI*). [[Paper](https://arxiv.org/abs/2309.04461)][[Code (in construction)](https://github.com/Yangyi-Chen/CoTConsistency)]
* Video:
    * **?**: "Mounting Video Metadata on Transformer-based Language Model for Open-ended Video Question Answering", arXiv, 2021 (*Seoul National University*). [[Paper](https://arxiv.org/abs/2108.05158)]
    * **TPT**: "Temporal Pyramid Transformer with Multimodal Interaction for Video Question Answering", arXiv, 2021 (*CAS*). [[Paper](https://arxiv.org/abs/2109.04735)]
    * **SwinBERT**: "SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.13196)][[PyTorch](https://github.com/microsoft/SwinBERT)]
    * **WildQA**: "WildQA: In-the-Wild Video Question Answering", International Conference on Computational Linguistics (COLING), 2022 (*UMich*). [[Paper](https://arxiv.org/abs/2209.06650)][[Website](https://lit.eecs.umich.edu/wildqa/)]
    * **VGT**: "Video Graph Transformer for Video Question Answering", ECCV, 2022 (*Sea AI Lab*). [[Paper](https://arxiv.org/abs/2207.05342)][[PyTorch](https://github.com/sail-sg/VGT)]
    * **?**: "Video Question Answering with Iterative Video-Text Co-Tokenization", ECCV, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2208.00934)][[Website (in construction)](https://sites.google.com/view/videoqa-cotokenization)]
    * **DeST**: "Learning Fine-Grained Visual Understanding for Video Question Answering via Decoupling Spatial-Temporal Modeling", BMVC, 2022 (*NTU*). [[Paper](https://arxiv.org/abs/2210.03941)][[PyTorch](https://github.com/shinying/dest)]
    * **ViteVQA**: "Towards Video Text Visual Question Answering: Benchmark and Baseline", NeurIPS, 2022 (*ByteDance*). [[Paper](https://openreview.net/forum?id=yPZ7w29qSNK)][[GitHub](https://github.com/bytedance/VTVQA)]
    * **WSQG**: "Frame-Subtitle Self-Supervision for Multi-Modal Video Question Answering", arXiv, 2022 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2209.03609)]
    * **LocAns**: "Locate before Answering: Answer Guided Question Localization for Video Question Answering", arXiv, 2022 (*Fudan University*). [[Paper](https://arxiv.org/abs/2210.02081)]
    * **NewsVideoQA**: "Watching the News: Towards VideoQA Models that can Read", arXiv, 2022 (*IIIT Hyderabad, India*). [[Paper](https://arxiv.org/abs/2211.05588)]
    * **SHG-VQA**: "Learning Situation Hyper-Graphs for Video Question Answering", CVPR, 2023 (*UCF*). [[Paper](https://arxiv.org/abs/2304.08682)][[PyTorch](https://github.com/aurooj/SHG-VQA)]
    * **ANetQA**: "ANetQA: A Large-scale Benchmark for Fine-grained Compositional Reasoning over Untrimmed Videos", CVPR, 2023 (*Hangzhou Dianzi University*). [[Paper](https://arxiv.org/abs/2305.02519)][[Website](https://milvlg.github.io/anetqa/)]
    * **MCR**: "Discovering the Real Association: Multimodal Causal Reasoning in Video Question Answering", CVPR, 2023 (*Beijing Institute of Technology*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zang_Discovering_the_Real_Association_Multimodal_Causal_Reasoning_in_Video_Question_CVPR_2023_paper.html)][[Code (in construction)](https://github.com/Chuanqi-Zang/Discovering-the-Real-Association)]
    * **MIST**: "MIST: Multi-modal Iterative Spatial-Temporal Transformer for Long-form Video Question Answering", CVPR, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2212.09522)][[PyTorch](https://github.com/showlab/mist)]
    * **CaKE-LM**: "Language Models are Causal Knowledge Extractors for Zero-shot Video Question Answering", CVPRW, 2023 (*NTU + Columbia*). [[Paper](https://arxiv.org/abs/2304.03754)]
    * **TransSTR**: "Discovering Spatio-Temporal Rationales for Video Question Answering", ICCV, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2307.12058)]
    * **Tem-adapter**: "Tem-adapter: Adapting Image-Text Pretraining for Video Question Answer", ICCV, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2308.08414)][[Code (in construction)](https://github.com/XLiu443/Tem-adapter)]
    * **OVQA**: "Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models", ICCV, 2023 (*Korea University*). [[Paper](https://arxiv.org/abs/2308.09351)]
    * **RaFormer**: "Redundancy-aware Transformer for Video Question Answering", ACMMM, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2308.03267)]
    * **SeViLA**: "Self-Chained Image-Language Model for Video Localization and Question Answering", arXiv, 2023 (*UNC*). [[Paper](https://arxiv.org/abs/2305.06988)][[PyTorch](https://github.com/Yui010206/SeViLA)]
    * **FunQA**: "FunQA: Towards Surprising Video Comprehension", arXiv, 2023 (*Beijing University of Posts and Telecommunication*). [[Paper](https://arxiv.org/abs/2306.14899)][[Code (in construction)](https://github.com/Jingkang50/FunQA)][[Website](https://funqa-benchmark.github.io/)]
* 3D:
    * **3D-VQA**: "CLIP-Guided Vision-Language Pre-training for Question Answering in 3D Scenes", CVPRW, 2023 (*ETHZ*). [[Paper](https://arxiv.org/abs/2304.06061)][[Code (in construction)](https://github.com/AlexDelitzas/3D-VQA)]
    * **Multi-CLIP**: "Multi-CLIP: Contrastive Vision-Language Pre-training for Question Answering tasks in 3D Scenes", arXiv, 2023 (*ETHZ*). [[Paper](https://arxiv.org/abs/2306.02329)]
* Audio-Visual:
    * **PSTP-Net**: "Progressive Spatio-temporal Perception for Audio-Visual Question Answering", ACMMM, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2308.05421)][[PyTorch](https://github.com/GeWu-Lab/PSTP-Net)]

[[Back to Overview](#overview)]

### Visual Grounding
* General:
    * **TransRefer3D**: "TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding", ACMMM, 2021 (*Beihang University*). [[Paper](https://arxiv.org/abs/2108.02388)]
    * **?**: "Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers", EMNLP, 2021 (*University of Trento*). [[Paper](https://arxiv.org/abs/2109.04448)]
    * **MITVG**: "Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation", ACL Findings, 2021 (*Tencent*). [[Paper](https://arxiv.org/abs/2109.08478)]
    * **TransVG**: "TransVG: End-to-End Visual Grounding with Transformers", ICCV, 2021 (*USTC*). [[Paper](https://arxiv.org/abs/2104.08541)]
    * **GSRTR**: "Grounded Situation Recognition with Transformers", BMVC, 2021 (*POSTECH*). [[Paper](https://arxiv.org/abs/2111.10135)][[PyTorch](https://github.com/jhcho99/gsrtr)]
    * **Referring-Transformer**: "Referring Transformer: A One-step Approach to Multi-task Visual Grounding", NeurIPS, 2021 (*UBC*). [[Paper](https://arxiv.org/abs/2106.03089)]
    * **VGTR**: "Visual Grounding with Transformers", arXiv, 2021 (*Beihang University*). [[Paper](https://arxiv.org/abs/2105.04281)]
    * **UNICORN**: "Crossing the Format Boundary of Text and Boxes: Towards Unified Vision-Language Modeling", arXiv, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.12085)]
    * **Word2Pix**: "Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding", arXiv, 2021 (*A\*STAR*). [[Paper](https://arxiv.org/abs/2108.00205)]
    * **CoFormer**: "Collaborative Transformers for Grounded Situation Recognition", CVPR, 2022 (*POSTECH*). [[Paper](https://arxiv.org/abs/2203.16518)][[PyTorch](https://github.com/jhcho99/CoFormer)]
    * **MVT**: "Multi-View Transformer for 3D Visual Grounding", CVPR, 2022 (*CUHK*). [[Paper](https://arxiv.org/abs/2204.02174)][[PyTorch](https://github.com/sega-hsj/MVT-3DVG)]
    * **GLIP**: "Grounded Language-Image Pre-training", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2112.03857)][[PyTorch](https://github.com/microsoft/GLIP)]
    * **M-DGT**: "Multi-Modal Dynamic Graph Transformer for Visual Grounding", CVPR, 2022 (*University of Toronto*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Multi-Modal_Dynamic_Graph_Transformer_for_Visual_Grounding_CVPR_2022_paper.html)][[PyTorch](https://github.com/iQua/M-DGT)]
    * **QRNet**: "Shifting More Attention to Visual Backbone: Query-modulated Refinement Networks for End-to-End Visual Grounding", CVPR, 2022 (*East China Normal University*). [[Paper](https://arxiv.org/abs/2203.15442)][[PyTorch](https://github.com/LukeForeverYoung/QRNet)]
    * **SiRi**: "SiRi: A Simple Selective Retraining Mechanism for Transformer-based Visual Grounding", ECCV, 2022 (*JD*). [[Paper](https://arxiv.org/abs/2207.13325)][[PyTorch](https://github.com/qumengxue/siri-vg)]
    * **UniTAB**: "UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling", ECCV, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.12085)]
    * **TAP**: "Improving Closed and Open-Vocabulary Attribute Prediction Using Transformers", ECCV, 2022 (*Adobe*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5247_ECCV_2022_paper.php)][[GitHub](https://github.com/adobe-research/vaw_dataset)][[Website](https://vkhoi.github.io/TAP/)]
    * **YORO**: "YORO - Lightweight End to End Visual Grounding", ECCVW, 2022 (*Amazon*). [[Paper](https://arxiv.org/abs/2211.07912)]
    * **GLIPv2**: "GLIPv2: Unifying Localization and Vision-Language Understanding", NeurIPS, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2206.05836)][[PyTorch](https://github.com/microsoft/GLIP)]
    * **?**: "Do Vision-and-Language Transformers Learn Grounded Predicate-Noun Dependencies?", EMNLP, 2022 (*Aix-Marseille University, France*). [[Paper](https://arxiv.org/abs/2210.12079)]
    * **SeqTR**: "SeqTR: A Simple yet Universal Network for Visual Grounding", arXiv, 2022 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2203.16265)][[Code (in construction)](https://github.com/sean-zhuh/SeqTR)]
    * **TransVG++**: "TransVG++: End-to-End Visual Grounding with Language Conditioned Vision Transformer", arXiv, 2022 (*USTC*). [[Paper](https://arxiv.org/abs/2206.06619)]
    * **HLGT**: "Hierarchical Local-Global Transformer for Temporal Sentence Grounding", arXiv, 2022 (*Huazhong University of Science and Technology*). [[Paper](https://arxiv.org/abs/2208.14882)]
    * **Dynamic-MDETR**: "Dynamic MDETR: A Dynamic Multimodal Transformer Decoder for Visual Grounding", arXiv, 2022 (*Nanjing University*). [[Paper](https://arxiv.org/abs/2209.13959)]
    * **ClipCrop**: "ClipCrop: Conditioned Cropping Driven by Vision-Language Model", arXiv, 2022 (*The University of Tokyo*). [[Paper](https://arxiv.org/abs/2211.11492)]
    * **VL-MPAG-Net**: "Grounding Scene Graphs on Natural Images via Visio-Lingual Message Passing", WACV, 2023 (*Indian Institute of Science*). [[Paper](https://arxiv.org/abs/2211.01969)][[PyTorch](https://github.com/IISCAditayTripathi/Scene-graph-localization)][[Website](https://iiscaditaytripathi.github.io/sgl/)]
    * **CLEVER**: "Visually Grounded Commonsense Knowledge Acquisition", AAAI, 2023 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2211.12054)][[PyTorch](https://github.com/thunlp/CLEVER)]
    * **LADS**: "Referring Expression Comprehension Using Language Adaptive Inference", AAAI, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2306.04451)]
    * **?**: "Learning to Jointly Share and Prune Weights for Grounding Based Vision and Language Models", ICLR, 2023 (*Samsung*). [[Paper](https://openreview.net/forum?id=UMERaIHMwB3)]
    * **AMC**: "Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations", CVPR, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2206.15462)][[PyTorch](https://github.com/uvavision/AMC-grounding)][[Website](https://vislang.ai/amc)]
    * **CounTEX**: "Grounding Counterfactual Explanation of Image Classifiers to Textual Concept Space", CVPR, 2023 (*Amazon*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Kim_Grounding_Counterfactual_Explanation_of_Image_Classifiers_to_Textual_Concept_Space_CVPR_2023_paper.html)]
    * **SK-VG**: "Advancing Visual Grounding with Scene Knowledge: Benchmark and Method", CVPR, 2023 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2307.11558)][[Code (in construction)](https://github.com/zhjohnchan/SK-VG)]
    * **D-ViTMDETR**: "Dynamic Inference with Grounding Based Vision and Language Models", CVPR, 2023 (*Amazon*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Uzkent_Dynamic_Inference_With_Grounding_Based_Vision_and_Language_Models_CVPR_2023_paper.html)]
    * **?**: "Similarity Maps for Self-Training Weakly-Supervised Phrase Grounding", CVPR, 2023 (*Tel Aviv*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Shaharabany_Similarity_Maps_for_Self-Training_Weakly-Supervised_Phrase_Grounding_CVPR_2023_paper.html)][[Code (in construction)](https://github.com/talshaharabany/Similarity-Maps-for-Self-Training-Weakly-Supervised-Phrase-Grounding)]
    * **RefCLIP**: "RefCLIP: A Universal Teacher for Weakly Supervised Referring Expression Comprehension", CVPR, 2023 (*Xiamen University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Jin_RefCLIP_A_Universal_Teacher_for_Weakly_Supervised_Referring_Expression_Comprehension_CVPR_2023_paper.html)][[PyTorch](https://github.com/kingthreestones/RefCLIP)][[Website](https://refclip.github.io/)]
    * **FROMAGe**: "Grounding Language Models to Images for Multimodal Inputs and Outputs", ICML, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2301.13823)][[PyTorch](https://github.com/kohjingyu/fromage)][[Website](https://jykoh.com/fromage)]
    * **IR-VG**: "Iterative Robust Visual Grounding with Masked Reference based Centerpoint Supervision", ICCV, 2023 (*Beihang*). [[Paper](https://arxiv.org/abs/2307.12392)][[Code (in construction)](https://github.com/cv516Buaa/IR-VG)]
    * **RefEgo**: "RefEgo: Referring Expression Comprehension Dataset from First-Person Perception of Ego4D", ICCV, 2023 (*RIKEN*). [[Paper](https://arxiv.org/abs/2308.12035)]
    * **CLIP-VG**: "CLIP-VG: Self-paced Curriculum Adapting of CLIP via Exploiting Pseudo-Language Labels for Visual Grounding", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2305.08685)][[Code (in construction)](https://github.com/linhuixiao/CLIP-VG)]
    * **TreePrompt**: "TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding", arXiv, 2023 (*HKUST*). [[Paper](https://arxiv.org/abs/2305.11497)]
    * **OctoBERT**: "World-to-Words: Grounded Open Vocabulary Acquisition through Fast Mapping in Vision-Language Models", arXiv, 2023 (*UMich*). [[Paper](https://arxiv.org/abs/2306.08685)]
    * **BuboGPT**: "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2307.08581)][[PyTorch](https://github.com/magic-research/bubogpt)][[Website](https://bubo-gpt.github.io/)]
    * **LG-DVG**: "Language-Guided Diffusion Model for Visual Grounding", arXiv, 2023 (*University of Toronto*). [[Paper](https://arxiv.org/abs/2308.09599)]
    * **VGDiffZero**: "VGDiffZero: Text-to-image Diffusion Models Can Be Zero-shot Visual Grounders", arXiv, 2023 (*Westlake University, China*). [[Paper](https://arxiv.org/abs/2309.01141)]
    * **GREC**: "GREC: Generalized Referring Expression Comprehension", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2308.16182)][[Website](https://henghuiding.github.io/GRES/)]VGDiffZero: Text-to-image Diffusion Models Can Be Zero-shot Visual Grounders
* Video:
    * **Multi-Stage-Transformer**: "Multi-Stage Aggregated Transformer Network for Temporal Language Localization in Videos", CVPR, 2021 (*University of Electronic Science and Technology of China*). [[Paper](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Multi-Stage_Aggregated_Transformer_Network_for_Temporal_Language_Localization_in_Videos_CVPR_2021_paper.html)]
    * **GTR**: "On Pursuit of Designing Multi-modal Transformer for Video Grounding", EMNLP, 2021 (*Peking*). [[Paper](https://arxiv.org/abs/2109.06085)]
    * **STVGBert**: "STVGBert: A Visual-Linguistic Transformer Based Framework for Spatio-Temporal Video Grounding", ICCV, 2021 (*Tencent*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Su_STVGBert_A_Visual-Linguistic_Transformer_Based_Framework_for_Spatio-Temporal_Video_Grounding_ICCV_2021_paper.html)]
    * **DRFT**: "End-to-end Multi-modal Video Temporal Grounding", NeurIPS, 2021 (*UC Merced*). [[Paper](https://arxiv.org/abs/2107.05624)]
    * **TubeDETR**: "TubeDETR: Spatio-Temporal Video Grounding with Transformers", CVPR, 2022 (*INRIA*). [[Paper](https://arxiv.org/abs/2203.16434)][[Website](https://antoyang.github.io/tubedetr.html)]
    * **UMT**: "UMT: Unified Multi-modal Transformers for Joint Video Moment Retrieval and Highlight Detection", CVPR, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2203.12745)][[Code (in constrcution)](https://github.com/TencentARC/UMT)]
    * **STVGFormer**: "STVGFormer: Spatio-Temporal Video Grounding with Static-Dynamic Cross-Modal Understanding", ACMMMW, 2022 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2207.02756)]
    * **STCAT**: "Embracing Consistency: A One-Stage Approach for Spatio-Temporal Video Grounding", NeurIPS, 2022 (*Peking University*). [[Paper](https://arxiv.org/abs/2209.13306)][[PyTorch](https://github.com/jy0205/STCAT)]
    * **VideoWhisperer**: "Grounded Video Situation Recognition", NeurIPS, 2022 (*IIIT Hyderabad, India*). [[Paper](https://arxiv.org/abs/2210.10828)][[Website](https://zeeshank95.github.io/grvidsitu)]
    * **VidGTR**: "Explore and Match: End-to-End Video Grounding with Transformer", arXiv, 2022 (*KAIST*). [[Paper](https://arxiv.org/abs/2201.10168)]
    * **?**: "Language-free Training for Zero-shot Video Grounding", WACV, 2023 (*Yonsei University*). [[Paper](https://arxiv.org/abs/2210.12977)]
    * **VG-LAW**: "Language Adaptive Weight Generation for Multi-task Visual Grounding", CVPR, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2306.04652)]
    * **TCSF**: "You Can Ground Earlier than See: An Effective and Efficient Pipeline for Temporal Sentence Grounding in Compressed Videos", CVPR, 2023 (*Huazhong University of Science and Technology*). [[Paper](https://arxiv.org/abs/2303.07863)]
    * **?**: "Weakly Supervised Temporal Sentence Grounding with Uncertainty-Guided Self-training", CVPR, 2023 (*The University of Tokyo*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Weakly_Supervised_Temporal_Sentence_Grounding_With_Uncertainty-Guided_Self-Training_CVPR_2023_paper.html)]
    * **DeCo**: "DeCo: Decomposition and Reconstruction for Compositional Temporal Grounding via Coarse-To-Fine Contrastive Ranking", CVPR, 2023 (*Toyota*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Yang_DeCo_Decomposition_and_Reconstruction_for_Compositional_Temporal_Grounding_via_Coarse-To-Fine_CVPR_2023_paper.html)]
    * **HSCNet**: "Hierarchical Semantic Correspondence Networks for Video Paragraph Grounding", CVPR, 2023 (*Sun Yat-sen University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Tan_Hierarchical_Semantic_Correspondence_Networks_for_Video_Paragraph_Grounding_CVPR_2023_paper.html)]
    * **WINNER**: "WINNER: Weakly-Supervised hIerarchical decompositioN and aligNment for Spatio-tEmporal Video gRounding", CVPR, 2023 (*Zhejiang University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Li_WINNER_Weakly-Supervised_hIerarchical_decompositioN_and_aligNment_for_Spatio-tEmporal_Video_gRounding_CVPR_2023_paper.html)]
    * **IRON**: "Iterative Proposal Refinement for Weakly-Supervised Video Grounding", CVPR, 2023 (*Microsoft*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Cao_Iterative_Proposal_Refinement_for_Weakly-Supervised_Video_Grounding_CVPR_2023_paper.html)]
    * **?**: "Collaborative Static and Dynamic Vision-Language Streams for Spatio-Temporal Video Grounding", CVPR, 2023 (*Sun Yat-sen University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Collaborative_Static_and_Dynamic_Vision-Language_Streams_for_Spatio-Temporal_Video_Grounding_CVPR_2023_paper.html)]
    * **ProTeGe**: "ProTeGe: Untrimmed Pretraining for Video Temporal Grounding by Video Temporal Grounding", CVPR, 2023 (*Microsoft*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Wang_ProTeGe_Untrimmed_Pretraining_for_Video_Temporal_Grounding_by_Video_Temporal_CVPR_2023_paper.html)]
    * **VidLN**: "Connecting Vision and Language with Video Localized Narratives", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2302.11217)][[Website](https://google.github.io/video-localized-narratives/)]
    * **VDI**: "Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training", CVPR, 2023 (*Queen Mary University of London*). [[Paper](https://arxiv.org/abs/2303.00040)]
    * **UniVTG**: "UniVTG: Towards Unified Video-Language Temporal Grounding", ICCV, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2307.16715)][[PyTorch](https://github.com/showlab/UniVTG)]
    * **EaTR**: "Knowing Where to Focus: Event-aware Transformer for Video Grounding", ICCV, 2023 (*Yonsei*). [[Paper](https://arxiv.org/abs/2308.06947)][[PyTorch](https://github.com/jinhyunj/EaTR)]
    * **TSGSV**: "Temporal Sentence Grounding in Streaming Videos", ACMMM, 2023 (*Shandong University*). [[Paper](https://arxiv.org/abs/2308.07102)]
    * **?**: "Learning Grounded Vision-Language Representation for Versatile Understanding in Untrimmed Videos", arXiv, 2023 (*Southern University of Science and Technology, China*). [[Paper](https://arxiv.org/abs/2303.06378)]
    * **MomentDiff**: "MomentDiff: Generative Video Moment Retrieval from Random to Real", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2307.02869)][[Code (in construction)](https://github.com/IMCCretrieval/MomentDiff)]
    * **BM-DETR**: "Overcoming Weak Visual-Textual Alignment for Video Moment Retrieval", arXiv, 2023 (*Seoul National University (SNU)*). [[Paper](https://arxiv.org/abs/2306.02728)][[PyTorch (in construction)](https://github.com/minjoong507/BM-DETR)]
    * **?**: "Zero-Shot Video Moment Retrieval from Frozen Vision-Language Models", WACV, 2024 (*Queen Mary University of London*). [[Paper](https://arxiv.org/abs/2309.00661)]
* 3D:
    * **ViL3DRel**: "Language Conditioned Spatial Relation Reasoning for 3D Object Grounding", NeurIPS, 2022 (*INRIA*). [[Paper](https://arxiv.org/abs/2211.09646)][[Website](https://cshizhe.github.io/projects/vil3dref.html)]
    * **LAR**: "Look Around and Refer: 2D Synthetic Semantics Knowledge Distillation for 3D Visual Grounding", NeurIPS, 2022 (*KAUST*). [[Paper](https://arxiv.org/abs/2211.14241)][[Website](https://eslambakr.github.io/LAR.github.io/)]
    * **3D-CG**: "3D Concept Grounding on Neural Fields", NeurIPS, 2022 (*MIT*). [[Paper](https://arxiv.org/abs/2207.06403)][[Website](http://3d-cg.csail.mit.edu/)]
    * **NS3D**: "NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations", CVPR, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2303.13483)]
    * **EDA**: "EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding", CVPR, 2023 (*Peking University*). [[Paper](https://arxiv.org/abs/2209.14941)]
    * **?**: "Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding", ICCV, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2307.09267)]
    * **Multi3DRefer**: "Multi3DRefer: Grounding Text Description to Multiple 3D Objects", ICCV, 2023 (*Simon Fraser*). [[Paper](https://arxiv.org/abs/2309.05251)]
    * **UniT3D**: "UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding", ICCV, 2023 (*TUM*). [[Paper](https://arxiv.org/abs/2212.00836)]
    * **3DOGSFormer**: "Dense Object Grounding in 3D Scenes", ACMMM, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2309.02224)]
    * **ViewRefer**: "ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2303.16894)][[Code (in construction)](https://github.com/ZiyuGuo99/ViewRefer3D)]
    * **?**: "What, when, and where? -- Self-Supervised Spatio-Temporal Grounding in Untrimmed Multi-Action Videos from Narrated Instructions", arXiv, 2023 (*Columbia*). [[Paper](https://arxiv.org/abs/2303.16990)]
    * **3DRP-Net**: "3DRP-Net: 3D Relative Position-aware Network for 3D Visual Grounding", arXiv, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2307.13363)]
    * **3DRefTR**: "A Unified Framework for 3D Point Cloud Visual Grounding", arXiv, 2023 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2308.11887)][[PyTorch](https://github.com/Leon1207/3DRefTR)]

[[Back to Overview](#overview)]

### Multi-Modal Representation Learning
* General:
    * **LXMERT**: "LXMERT: Learning Cross-Modality Encoder Representations from Transformers", EMNLP, 2019 (*UNC*). [[Paper](https://arxiv.org/abs/1908.07490)][[PyTorch](https://github.com/airsplay/lxmert)]
    * **ViLBERT**: "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks", NeurIPS, 2019 (*Georgia Tech*). [[Paper](https://papers.nips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html)][[PyTorch](https://github.com/facebookresearch/vilbert-multi-task)]
    * **Unified-VLP**: "Unified Vision-Language Pre-Training for Image Captioning and VQA", AAAI, 2020 (*UMich + Microsoft*). [[Paper](https://arxiv.org/abs/1909.11059)][[PyTorch](https://github.com/LuoweiZhou/VLP)]
    * **UNITER**: "UNITER: UNiversal Image-TExt Representation Learning", ECCV, 2020 (*Microsoft*). [[Paper](https://arxiv.org/abs/1909.11740)][[PyTorch](https://github.com/ChenRocks/UNITER)]
    * **VinVL**: "VinVL: Revisiting Visual Representations in Vision-Language Models", CVPR, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2101.00529)][[Code](https://github.com/pzzhang/VinVL)]
    * **CATT**: "Causal Attention for Vision-Language Tasks", CVPR, 2021 (*NTU Singapore*). [[Paper](https://arxiv.org/abs/2103.03493)][[PyTorch](https://github.com/yangxuntu/lxmertcatt)]
    * **ViLT**: "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision", ICML, 2021 (*Kakao*). [[Paper](https://arxiv.org/abs/2102.03334)][[PyTorch](https://github.com/dandelin/vilt)]
    * **MERLOT**: "MERLOT: Multimodal Neural Script Knowledge Models", NeurIPS, 2021 (*UW + AI2*). [[Paper](https://arxiv.org/abs/2106.02636)][[Tensorflow](https://github.com/rowanz/merlot)][[Website](https://rowanzellers.com/merlot/)]
    * **SVO-Probes**: "Probing Image-Language Transformers for Verb Understanding", arXiv, 2021 (*DeepMind*). [[Paper](https://arxiv.org/abs/2106.09141)]
    * **CLIP-ViL**: "How Much Can CLIP Benefit Vision-and-Language Tasks?", arXiv, 2021 (*Berkeley + UCLA*). [[Paper](https://arxiv.org/abs/2107.06383)][[PyTorch](https://github.com/clip-vil/CLIP-ViL)]
    * **Florence**: "Florence: A New Foundation Model for Computer Vision", arXiv, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.11432)]
    * **UFO**: "UFO: A UniFied TransfOrmer for Vision-Language Representation Learning", arXiv, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.10023)]
    * **SimVLM**: "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision", ICLR, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2108.10904)]
    * **LiT**: "LiT: Zero-Shot Transfer with Locked-image text Tuning", CVPR, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2111.07991)]
    * **UniCL**: "Unified Contrastive Learning in Image-Text-Label Space", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2204.03610)][[PyTorch](https://github.com/microsoft/UniCL)]
    * **FLAVA**: "FLAVA: A Foundational Language And Vision Alignment Model", CVPR, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2112.04482)][[Pretrained Model](https://huggingface.co/facebook/flava-full)][[Code](https://github.com/facebookresearch/multimodal/tree/main/examples/flava)][[Dataset](https://huggingface.co/datasets/facebook/pmd)][[Website](https://flava-model.github.io/)][[Demos](https://huggingface.co/flava)]
    * **LEMON**: "Scaling Up Vision-Language Pre-training for Image Captioning", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.12233)]
    * **METER**: "An Empirical Study of Training End-to-End Vision-and-Language Transformers", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.02387)][[PyTorch](https://github.com/zdou0830/METER)]
    * **Uni-Perceiver**: "Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks", CVPR, 2022 (*SenseTime*). [[Paper](https://arxiv.org/abs/2112.01522)][[PyTorch](https://github.com/fundamentalvision/Uni-Perceiver)]
    * **MERLOT-Reserve**: "MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound", CVPR, 2022 (*UW + AI2*). [[Paper](https://arxiv.org/abs/2201.02639)][[JAX](https://github.com/rowanz/merlot_reserve)][[Website](https://rowanzellers.com/merlotreserve/)]
    * **Omnivore**: "Omnivore: A Single Model for Many Visual Modalities", CVPR, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2201.08377)][[PyTorch](https://github.com/facebookresearch/omnivore)][[Website](https://facebookresearch.github.io/omnivore/)]
    * **CM-mix**: "Pre-training image-language transformers for open-vocabulary tasks", CVPRW, 2022 (*Google*). [[Paper](https://drive.google.com/file/d/1dYM4g42rptj647v1EfNARmnCt3HdPpNR/view)]
    * **VLMixer**: "VLMixer: Unpaired Vision-Language Pre-training via Cross-Modal CutMix", ICML, 2022 (*Southern University of Science and Technology*). [[Paper](https://arxiv.org/abs/2206.08919)][[Code (in construction)](https://github.com/ttengwang/VLMixer)]
    * **VLUE**: "VLUE: A Multi-Task Benchmark for Evaluating Vision-Language Models", ICML, 2022 (*ByteDance*). [[Paper](https://arxiv.org/abs/2205.15237)][[Website](https://vlue-benchmark.github.io/)][[PyTorch](https://github.com/MichaelZhouwang/VLUE)]
    * **X-VLM**: "Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts", ICML, 2022 (*ByteDance*). [[Paper](https://arxiv.org/abs/2111.08276)][[PyTorch](https://github.com/zengyan-97/X-VLM)]
    * **BLIP**: "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", ICML, 2022 (*Salesforce*). [[Paper](https://arxiv.org/abs/2201.12086)][[PyTorch](https://github.com/salesforce/BLIP)]
    * **OFA**: "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework", ICML, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2202.03052)][[PyTorch](https://github.com/OFA-Sys/OFA)]
    * **MS-CLIP**: "Learning Visual Representation from Modality-Shared Contrastive Language-Image Pre-training", ECCV, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2207.12661)][[PyTorch](https://github.com/Hxyou/MSCLIP)]
    * **GRIT-VLP**: "GRIT-VLP: Grouped Mini-batch Sampling for Efficient Vision and Language Pre-training", ECCV, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2208.04060)][[PyTorch](https://github.com/jaeseokbyun/GRIT-VLP)]
    * **SIMLA**: "Single-Stream Multi-Level Alignment for Vision-Language Pretraining", ECCV, 2022 (*Northeastern University*). [[Paper](https://arxiv.org/abs/2203.14395)][[PyTorch](https://github.com/codezakh/SIMLA)][[Website](http://zaidkhan.me/SIMLA/)]
    * **Switch-BERT**: "Switch-BERT: Learning to Model Multimodal Interactions by Switching Attention and Input", ECCV, 2022 (*Ant Group*). [[Paper](https://arxiv.org/abs/2306.14182)]
    * **OmniVL**: "OmniVL: One Foundation Model for Image-Language and Video-Language Tasks", NeurIPS, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2209.07526)]
    * **UniCLIP**: "UniCLIP: Unified Framework for Contrastive Language-Image Pre-training", NeurIPS, 2022 (*LG*). [[Paper](https://arxiv.org/abs/2209.13430)]
    * **Uni-Perceiver-MoE**: "Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs", NeurIPS, 2022 (*SenseTime*). [[Paper](https://arxiv.org/abs/2206.04674)][[PyTorch](https://github.com/fundamentalvision/Uni-Perceiver)]
    * **CLOOB**: "CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP", NeurIPS, 2022 (*Johannes Kepler University, Austria*). [[Paper](https://arxiv.org/abs/2110.11316)][[PyTorch](https://github.com/ml-jku/cloob)]
    * **CyCLIP**: "CyCLIP: Cyclic Contrastive Language-Image Pretraining", NeurIPS, 2022 (*UCLA*). [[Paper](https://arxiv.org/abs/2205.14459)]
    * **?**: "Quality Not Quantity: On the Interaction between Dataset Design and Robustness of CLIP", NeurIPS, 2022 (*UW*). [[Paper](https://openreview.net/forum?id=LTCBavFWp5C)][[Pytorch](https://github.com/mlfoundations/clip_quality_not_quantity)]
    * **PyramidCLIP**: "PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model Pretraining", NeurIPS, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2204.14095)]
    * **?**: "Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning", NeurIPS, 2022 (*Stanford*). [[Paper](https://arxiv.org/abs/2203.02053)][[Website](https://modalitygap.readthedocs.io/)]
    * **LIMoE**: "Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts", NeurIPS, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2206.02770)]
    * **VLMo**: "VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts", NeurIPS, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.02358)][[PyTorch (in construction)](https://github.com/microsoft/unilm/tree/master/vlmo)]
    * **Knowledge-CLIP**: "Contrastive Language-Image Pre-Training with Knowledge Graphs", NeurIPS, 2022 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2210.08901)]
    * **Flamingo**: "Flamingo: a Visual Language Model for Few-Shot Learning", NeurIPS, 2022 (*DeepMind*). [[Paper](https://arxiv.org/abs/2204.14198)]
    * **LOUPE**: "Fine-Grained Semantically Aligned Vision-Language Pre-Training", NeurIPS, 2022 (*Huawei*). [[Paper](https://arxiv.org/abs/2208.02515)][[Code (in construction)](https://github.com/YYJMJC/LOUPE)]
    * **FIBER**: "Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone", NeurIPS, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2206.07643)][[PyTorch](https://github.com/microsoft/FIBER)]
    * **UViM**: "UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes", NeurIPS, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2205.10337)]
    * **LAION-5B**: "LAION-5B: An open large-scale dataset for training next generation image-text models", NeurIPS (Datasets and Benchmarks), 2022 (*LAION*). [[Paper](https://openreview.net/forum?id=M3Y74vmsMcY)][[Website](https://laion.ai/blog/laion-5b/)]
    * **Wukong**: "Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark", NeurIPS (Datasets and Benchmarks), 2022 (*Huawei*). [[Paper](https://arxiv.org/abs/2202.06767)][[Website](https://wukong-dataset.github.io/wukong-dataset/)]
    * **TaiSu**: "TaiSu: A 166M Large-scale High-Quality Dataset for Chinese Vision-Language Pre-training", NeurIPS (Datasets and Benchmarks), 2022 (*CAS*). [[Paper](https://openreview.net/forum?id=iAxH-ikIP0I)][[PyTorch](https://github.com/ksOAn6g5/TaiSu)]
    * **WinoGAViL**: "WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models", NeurIPS (Datasets and Benchmarks), 2022 (*The Hebrew University of Jerusalem, Israel*). [[Paper](https://arxiv.org/abs/2207.12576)][[Website](https://winogavil.github.io/)]
    * **ELEVATER**: "ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models", NeurIPS (Datasets and Benchmarks), 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2204.08790)][[Website](https://computer-vision-in-the-wild.github.io/ELEVATER/)]
    * **?**: "Robustness Analysis of Video-Language Models Against Visual and Language Perturbations", NeurIPS (Datasets and Benchmarks), 2022 (*UCF*). [[Paper](https://arxiv.org/abs/2207.02159)][[Website](https://sites.google.com/view/videolanguagerobustness/home)]
    * **GIT**: "GIT: A Generative Image-to-text Transformer for Vision and Language", TMLR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2205.14100)]
    * **CoCa**: "CoCa: Contrastive Captioners are Image-Text Foundation Models", TMLR, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2205.01917)][[PyTorch (lucidrains)](https://github.com/lucidrains/CoCa-pytorch)]
    * **MultiMAE**: "MultiMAE: Multi-modal Multi-task Masked Autoencoders", arXiv, 2022 (*EPFL*). [[Paper](https://arxiv.org/abs/2204.01678)][[PyTorch](https://github.com/EPFL-VILAB/MultiMAE)][[Website](https://multimae.epfl.ch/)]
    * **VLC**: "Training Vision-Language Transformers from Captions Alone", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2205.09256)][[Code (in construction)](https://github.com/guilk/VLC)]
    * **CCLM**: "Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training", arXiv, 2022 (*ByteDance*). [[Paper](https://arxiv.org/abs/2206.00621)]
    * **VL-BEiT**: "VL-BEiT: Generative Vision-Language Pretraining", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2206.01127)]
    * **MetaLM**: "Language Models are General-Purpose Interfaces", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2206.06336)][[PyTorch](https://github.com/microsoft/unilm)]
    * **Bridge-Tower**: "Bridge-Tower: Building Bridges Between Encoders in Vision-Language Representation Learning", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2206.08657)][[Code (in construction)](https://github.com/microsoft/BridgeTower)]
    * **e-CLIP**: "e-CLIP: Large-Scale Vision-Language Representation Learning in E-commerce", arXiv, 2022 (*NAVER*). [[Paper](https://arxiv.org/abs/2207.00208)]
    * **LW-Transformer**: "Towards Lightweight Transformer via Group-wise Transformation for Vision-and-Language Tasks", arXiv, 2022 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2204.07780)][[PyTorch](https://github.com/luogen1996/LWTransformer)]
    * **UCM**: "Self-Training Vision Language BERTs with a Unified Conditional Model", arXiv, 2022 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2201.02010)]
    * **Prefix-conditioning**: "Prefix Conditioning Unifies Language and Label Supervision", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2206.01125)]
    * **VLMAE**: "VLMAE: Vision-Language Masked Autoencoder", arXiv, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2208.09374)]
    * **ViCHA**: "Efficient Vision-Language Pretraining with Visual Concepts and Hierarchical Alignment", arXiv, 2022 (*Sorbonne University, France*). [[Paper](https://arxiv.org/abs/2208.13628)][[Code (in construction)](https://github.com/mshukor/ViCHA)]
    * **DetailCLIP**: "Injecting Image Details into CLIP's Feature Space", arXiv, 2022 (*Megvii*). [[Paper](https://arxiv.org/abs/2208.14649)]
    * **?**: "Pre-training image-language transformers for open-vocabulary tasks", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2209.04372)]
    * **ERNIE**: "ERNIE-ViL 2.0: Multi-view Contrastive Learning for Image-Text Pre-training", arXiv, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2209.15270)][[Paddle](https://github.com/PaddlePaddle/ERNIE)]
    * **VoLTA**: "VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment", arXiv, 2022 (*JHU*). [[Paper](https://arxiv.org/abs/2210.04135)]
    * **?**: "One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks", arXiv, 2022 (*Technical University of Darmstadt, Germany*). [[Paper](https://arxiv.org/abs/2210.06379)]
    * **MAPL**: "MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting", arXiv, 2022 (*Mila*). [[Paper](https://arxiv.org/abs/2210.07179)]
    * **EfficientVLM**: "EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge Distillation and Modal-adaptive Pruning", arXiv, 2022 (*Bytedance*). [[Paper](https://arxiv.org/abs/2210.07795)][[PyTorch (in construction)](https://github.com/swaggy-TN/EfficientVLM)]
    * **CN-CLIP**: "Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese", arXiv, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2211.01335)]
    * **CLOSE**: "I Can't Believe There's No Images! Learning Visual Tasks Using only Language Data", arXiv, 2022 (*AI2*). [[Paper](https://arxiv.org/abs/2211.09778)]
    * **X<sup>2</sup>-VLM**: "X<sup>2</sup>-VLM: All-In-One Pre-trained Model For Vision-Language Tasks", arXiv, 2022 (*ByteDance*). [[Paper](https://arxiv.org/abs/2211.12402)][[Code (in construction)](https://github.com/zengyan-97/X2-VLM)]
    * **SkillNet**: "One Model, Multiple Modalities: A Sparsely Activated Approach for Text, Sound, Image, Video and Code", arXiv, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2205.06126)]
    * **Compound-Tokens**: "Compound Tokens: Channel Fusion for Vision-Language Representation Learning", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2212.01447)]
    * **WFH**: "Learning by Hallucinating: Vision-Language Pre-training with Weak Supervision", WACV, 2023 (*Aalto University, Finland*). [[Paper](https://arxiv.org/abs/2210.13591)]
    * **Perceiver-VL**: "Perceiver-VL: Efficient Vision-and-Language Modeling with Iterative Latent Attention", WACV, 2023 (*UNC*). [[Paper](https://arxiv.org/abs/2211.11701)][[PyTorch](https://github.com/zinengtang/Perceiver_VL)]
    * **MixGen**: "MixGen: A New Multi-Modal Data Augmentation", WACVW, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2206.08358)]
    * **?**: "Unifying Vision-Language Representation Space with Single-tower Transformer", AAAI, 2023 (*NAVER*). [[Paper](https://arxiv.org/abs/2211.11153)]
    * **PaLI**: "PaLI: A Jointly-Scaled Multilingual Language-Image Model", ICLR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2209.06794)]
    * **LilT**: "Contrastive Alignment of Vision to Language Through Parameter-Efficient Transfer Learning", ICLR, 2023 (*Northeastern University*). [[Paper](https://arxiv.org/abs/2303.11866)][[PyTorch](https://github.com/codezakh/LilT)]
    * **CLIPs**: "Is a Caption Worth a Thousand Images? A Controlled Study for Representation Learning", ICLR, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2207.07635)]
    * **HiCLIP**: "HiCLIP: Contrastive Language-Image Pretraining with Hierarchy-aware Attention", ICLR, 2023 (*Rutgers University*). [[Paper](https://arxiv.org/abs/2303.02995)]
    * **DeCap**: "DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training", ICLR, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2303.03032)][[PyTorch](https://github.com/dhg-wei/DeCap)]
    * **MaskVLM**: "Masked Vision and Language Modeling for Multi-modal Representation Learning", ICLR, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2208.02131)]
    * **DaVinci**: "Write and Paint: Generative Vision-Language Models are Unified Modal Learners", ICLR, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2206.07699)][[Code (in construction)](https://github.com/shizhediao/DaVinci)]
    * **EVA**: "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale", CVPR, 2023 (*Beijing Academy of Artificial Intelligence (BAAI)*). [[Paper](https://arxiv.org/abs/2211.07636)][[PyTorch](https://github.com/baaivision/EVA)]
    * **FLM**: "Accelerating Vision-Language Pretraining with Free Language Modeling", CVPR, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2303.14038)][[PyTorch](https://github.com/TencentARC/FLM)]
    * **FDT**: "Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens", CVPR, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2303.14865)][[Code (in construction)](https://github.com/yuxiaochen1103/FDT)]
    * **VILA**: "VILA: Learning Image Aesthetics from User Comments with Vision-Language Pretraining", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.14302)][[JAX](https://github.com/google-research/google-research/tree/master/vila)]
    * **BEiT-3**: "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2208.10442)][[PyTorch](https://github.com/microsoft/unilm/tree/master/beit)]
    * **ReVeaL**: "REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2212.05221)][[Website](https://reveal-cvpr.github.io/)]
    * **SCL**: "Seeing What You Miss: Vision-Language Pre-training with Semantic Completion Learning", CVPR, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2211.13437)]
    * **EPIC**: "Leveraging per Image-Token Consistency for Vision-Language Pre-training", CVPR, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2211.15398)]
    * **PTP**: "Position-guided Text Prompt for Vision-Language Pre-training", CVPR, 2023 (*Sea AI Lab*). [[Paper](https://arxiv.org/abs/2212.09737)][[PyTorch](https://github.com/sail-sg/ptp)]
    * **PHASE**: "Uncurated Image-Text Datasets: Shedding Light on Demographic Bias", CVPR, 2023 (*Osaka University*). [[Paper](https://arxiv.org/abs/2304.02828)][[GitHub](https://github.com/noagarcia/phase)]
    * **Uni-Perceiver-v2**: "Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks", CVPR, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2211.09808)][[PyTorch](https://github.com/fundamentalvision/Uni-Perceiver)]
    * **?**: "Exploring the Effect of Primitives for Compositional Generalization in Vision-and-Language", CVPR, 2023 (*Beijing Institute of Technology*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Li_Exploring_the_Effect_of_Primitives_for_Compositional_Generalization_in_Vision-and-Language_CVPR_2023_paper.html)]
    * **GIVL**: "GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods", CVPR, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2301.01893)]
    * **FLIP**: "Scaling Language-Image Pre-training via Masking", CVPR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2212.00794)][[PyTorch](https://github.com/facebookresearch/flip)]
    * **MAP**: "MAP: Modality-Agnostic Uncertainty-Aware Vision-Language Pre-training Model", CVPR, 2023 (*Tsinghua + Waseda*). [[Paper](https://arxiv.org/abs/2210.05335)][[PyTorch](https://github.com/IIGROUP/MAP)
    * **DANCE**: "Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2211.16504)][[PyTorch (in construction)](https://github.com/pleaseconnectwifi/DANCE)][[Website](https://shuquanye.com/DANCE_website/)]
    * **xCLIP**: "Non-Contrastive Learning Meets Language-Image Pre-Training", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2210.09304)]
    * **SVLC**: "Teaching Structured Vision & Language Concepts to Vision&Language Models", CVPR, 2023 (*IBM*). [[Paper](https://arxiv.org/abs/2211.11733)]
    * **DeAR**: "DeAR: Debiasing Vision-Language Models with Additive Residuals", CVPR, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2303.10431)][[GitHub](https://github.com/pata-fairness/pata_dataset)]
    * **?**: "Understanding and Constructing Latent Modality Structures in Multi-modal Representation Learning", CVPR, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2303.05952)]
    * **?**: "Joint Adaptive Representations for Image-Language Learning", CVPRW, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2305.19924)]
    * **BLIP-2**: "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", ICML, 2023 (*Salesforce*). [[Paper](https://arxiv.org/abs/2301.12597)][[PyTorch](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)]
    * **RLEG**: "RLEG: Vision-Language Representation Learning with Diffusion-based Embedding Generation", ICML, 2023 (*Alibaba*). [[Paper](https://openreview.net/forum?id=zBShO1Vmf0)]
    * **Mod-X**: "Continual Vision-Language Representation Learning with Off-Diagonal Information", ICML, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2305.07437)]
    * **ILLUME**: "ILLUME: Rationalizing Vision-Language Models through Human Interactions", ICML, 2023 (*German Center for Artificial Intelligence (DFKI)*). [[Paper](https://arxiv.org/abs/2208.08241)][[PyTorch](https://github.com/ml-research/ILLUME)]
    * **Pix2Struct**: "Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding", ICML, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2210.03347)]
    * **MERU**: "Hyperbolic Image-Text Representations", ICML, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2304.09172)]
    * **?**: "Measuring Progress in Fine-grained Vision-and-Language Understanding", ACL, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2305.07558)]
    * **RELIT**: "Weakly Supervised Vision-and-Language Pre-training with Relative Representations", ACL, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2305.15483)]
    * **PuMer**: "PuMer: Pruning and Merging Tokens for Efficient Vision Language Models", ACL, 2023 (*UW*). [[Paper](https://arxiv.org/abs/2305.17530)]
    * **SINC**: "SINC: Self-Supervised In-Context Learning for Vision-Language Tasks", ICCV, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2307.07742)]
    * **ALIP**: "ALIP: Adaptive Language-Image Pre-training with Synthetic Caption", ICCV, 2023 (*DeepGlint, China*). [[Paper](https://arxiv.org/abs/2308.08428)][[PyTorch](https://github.com/deepglint/ALIP)]
    * **SigLiT**: "Sigmoid Loss for Language Image Pre-Training", ICCV, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.15343)]
    * **VL-PET**: "VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control", ICCV, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2308.09804)][[PyTorch](https://github.com/HenryHZY/VL-PET)]
    * **GrowCLIP**: "GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training", ICCV, 2023 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2308.11331)]
    * **ViLLA**: "ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data", ICCV, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2308.11194)][[PyTorch](https://github.com/StanfordMIMI/villa)]
    * **CFM-ViT**: "Contrastive Feature Masking Open-Vocabulary Vision Transformer", ICCV, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2309.00775)]
    * **KOSMOS-1**: "Language Is Not All You Need: Aligning Perception with Language Models", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2302.14045)][[Code](https://github.com/microsoft/unilm)]
    * **Prismer**: "Prismer: A Vision-Language Model with An Ensemble of Experts", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2303.02506)][[PyTorch](https://github.com/NVlabs/prismer)][[Website](https://shikun.io/projects/prismer)]
    * **RVLM**: "Replacement as a Self-supervision for Fine-grained Vision-language Pre-training", arXiv, 2023 (*Harbin Institute of Technology*). [[Paper](https://arxiv.org/abs/2303.05313)]
    * **MuLTI**: "MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler and Multiple Choice Modeling", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2303.05707)]
    * **VL-MoE**: "Scaling Vision-Language Models with Sparse Mixture of Experts", arXiv, 2023 (*Berkeley + Microsoft*). [[Paper](https://arxiv.org/abs/2303.07226)]
    * **EVA-02**: "EVA-02: A Visual Representation for Neon Genesis", arXiv, 2023 (*BAAI*). [[Paper](https://arxiv.org/abs/2303.11331)][[PyTorch](https://github.com/baaivision/EVA/tree/master/EVA-02)]
    * **CoBIT**: "CoBIT: A Contrastive Bi-directional Image-Text Generation Model", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.13455)]
    * **EqSim**: "Equivariant Similarity for Vision-Language Foundation Models", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2303.14465)][[PyTorch](https://github.com/Wangt-CN/EqBen)]
    * **EVA-CLIP**: "EVA-CLIP: Improved Training Techniques for CLIP at Scale", arXiv, 2023 (*BAAI*). [[Paper](https://arxiv.org/abs/2303.15389)][[PyTorch](https://github.com/baaivision/EVA/tree/master/EVA-CLIP)]
    * **Sig**: "Sigmoid Loss for Language Image Pre-Training", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.15343)]
    * **MaMMUT**: "MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.16839)]
    * **CAVL**: "CAVL: Learning Contrastive and Adaptive Representations of Vision and Language", arXiv, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2304.04399)]
    * **MoMo**: "MoMo: A shared encoder Model for text, image and multi-Modal representations", arXiv, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2304.05523)]
    * **REAVL**: "Retrieval-based Knowledge Augmented Vision Language Pre-training", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2304.13923)]
    * **ALBEF-MI**: "Vision Lanauge Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2305.04474)]
    * **Helip**: "Boosting Visual-Language Models by Exploiting Hard Samples", arXiv, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2305.05208)]
    * **IMP**: "Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2305.06324)]
    * **Musketeer**: "Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts", arXiv, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2305.07019)]
    * **GVT**: "What Makes for Good Visual Tokenizers for Large Language Models?", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2305.12223)][[Code (in construction)](https://github.com/TencentARC/GVT)]
    * **S-CLIP**: "S-CLIP: Semi-supervised Vision-Language Pre-training using Few Specialist Captions", arXiv, 2023 (*KAIST*). [[Paper](https://arxiv.org/abs/2305.14095)]
    * **VisorGPT**: "VisorGPT: Learning Visual Prior via Generative Pre-Training", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2305.13777)][[Code (in construction)](https://github.com/Sierkinhane/VisorGPT)][[Website](https://sierkinhane.github.io/visor-gpt/)]
    * **IdealGPT**: "IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models", arXiv, 2023 (*Columbia University*). [[Paper](https://arxiv.org/abs/2305.14985)][[PyTorch](https://github.com/Hxyou/IdealGPT)]
    * **PaLI-X**: "PaLI-X: On Scaling up a Multilingual Vision and Language Model", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2305.18565)]
    * **CrossGET**: "CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2305.17455)][[Code (in construction)](https://github.com/sdc17/CrossGET)]
    * **TL;DR**: "Too Large; Data Reduction for Vision-Language Pre-Training", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2305.20087)][[Code (in construction)](https://github.com/showlab/data-centric.vlp)]
    * **DiffusionITM**: "Are Diffusion Models Vision-And-Language Reasoners?", arXiv, 2023 (*Mila*). [[Paper](https://arxiv.org/abs/2305.16397)]
    * **COSA**: "COSA: Concatenated Sample Pretrained Vision-Language Foundation Model", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2306.09085)][[PyTorch](https://github.com/TXH-mercury/COSA)]
    * **Babel-ImageNet**: "Babel-ImageNet: Massively Multilingual Evaluation of Vision-and-Language Representations", arXiv, 2023 (*University of Würzburg, Germany*). [[Paper](https://arxiv.org/abs/2306.08658)][[PyTorch](https://github.com/gregor-ge/Babel-ImageNet)]
    * **Kosmos-2**: "Kosmos-2: Grounding Multimodal Large Language Models to the World", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2306.14824)][[PyTorch](https://github.com/microsoft/unilm/tree/master/kosmos-2)][[Demo](https://888e9ea5c7b6d250.gradio.app/)]
    * **LENS**: "Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language", arXiv, 2023 (*Contextual AI + Stanford*). [[Paper](https://arxiv.org/abs/2306.16410)][[PyTorch](https://github.com/ContextualAI/lens)][[Demo](https://lens.contextual.ai/)]
    * **OBELISC**: "OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents", arXiv, 2023 (*Hugging Face*). [[Paper](https://arxiv.org/abs/2306.16527)][[GitHub](https://github.com/huggingface/OBELISC)]
    * **Emu**: "Generative Pretraining in Multimodality", arXiv, 2023 (*BAAI*). [[Paper](https://arxiv.org/abs/2307.05222)][[PyTorch](https://github.com/baaivision/Emu)]
    * **mBLIP**: "mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs", arXiv, 2023 (*University of Wurzburg, Germany*). [[Paper](https://arxiv.org/abs/2307.06930)][[PyTorch](https://github.com/gregor-ge/mBLIP)]
    * **P-Former**: "Bootstrapping Vision-Language Learning with Decoupled Language Pre-training", arXiv, 2023 (*Dartmouth College*). [[Paper](https://arxiv.org/abs/2307.07063)]
    * **SEED-OPT**: "Planting a SEED of Vision in Large Language Model", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2307.08041)][[Code (in construction)](https://github.com/AILab-CVC/SEED)]
    * **OpenFlamingo**: "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models", arXiv, 2023 (*UW*). [[Paper](https://arxiv.org/abs/2308.01390)][[PyTorch](https://github.com/mlfoundations/open_flamingo)]
    * **Free-ATM**: "Free-ATM: Exploring Unsupervised Learning on Diffusion-Generated Images with Free Attention Masks", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2308.06739)]
    * **LCL**: "Link-Context Learning for Multimodal LLMs", arXiv, 2023 (*SenseTime*). [[Paper](https://arxiv.org/abs/2308.07891)]
    * **DLIP**: "DLIP: Distilling Language-Image Pre-training", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2308.12956)]
    * **ViLTA**: "ViLTA: Enhancing Vision-Language Pre-training through Textual Augmentation", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2308.16689)]
    * **DAS**: "Parameter and Computation Efficient Transfer Learning for Vision-Language Pre-trained Models", arXiv, 2023 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2309.01479)]
    * **LaVIT**: "Unified Language-Vision Pretraining with Dynamic Discrete Visual Tokenization", arXiv, 2023 (*Kuaishou*). [[Paper](https://arxiv.org/abs/2309.04669)][[Code (in construction)](https://github.com/jy0205/LaVIT)]
    * **MMICL**: "MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning", arXiv, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2309.07915)][[PyTorch](https://github.com/HaozheZhao/MIC)]
* Video:
    * **COOT**: "COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning", NeurIPS, 2020 (*University of Freiburg*). [[Paper](https://arxiv.org/abs/2011.00597)][[PyTorch](https://github.com/gingsi/coot-videotext)]
    * **Parameter-Reduction**: "Parameter Efficient Multimodal Transformers for Video Representation Learning", ICLR, 2021 (*Seoul National University*). [[Paper](https://openreview.net/forum?id=6UdQLhqJyFD)]
    * **ClipBERT**: "Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling", CVPR, 2021 (*UNC + Microsoft*). [[Paper](https://arxiv.org/abs/2102.06183)][[PyTorch](https://github.com/jayleicn/ClipBERT)]
    * **VLM**: "VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding", ACL Findings, 2021 (*Facebook*). [[Paper](https://arxiv.org/abs/2105.09996)][[PyTorch](https://github.com/facebookresearch/fairseq/tree/main/examples/MMPT)]
    * **VideoCLIP**: "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding", EMNLP, 2021 (*Facebook*). [[Paper](https://arxiv.org/abs/2109.14084)][[PyTorch](https://github.com/facebookresearch/fairseq/tree/main/examples/MMPT)]
    * **VALUE**: "VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation", NeurIPS (Datasets and Benchmarks), 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2106.04632)][[Website](https://value-benchmark.github.io/)]
    * **TAN**: "Temporal Alignment Networks for Long-term Video", CVPR, 2022 (*Oxford*). [[Paper](https://arxiv.org/abs/2204.02968)][[Code (in construction)](https://github.com/TengdaHan/TemporalAlignNet)][[Website](https://www.robots.ox.ac.uk/~vgg/research/tan/)]
    * **HD-VILA**: "Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.10337)][[GitHub](https://github.com/microsoft/XPretrain)]
    * **ATP**: "Revisiting the "Video" in Video-Language Understanding", CVPR, 2022 (*Stanford*). [[Paper](https://arxiv.org/abs/2206.01720)][[Website](https://stanfordvl.github.io/atp-revisit-video-lang/)]
    * **ALPRO**: "Align and Prompt: Video-and-Language Pre-training with Entity Prompts", CVPR, 2022 (*Salesforce*). [[Paper](https://arxiv.org/abs/2112.09583)][[PyTorch](https://github.com/salesforce/ALPRO)]
    * **CLOP**: "CLOP: Video-and-Language Pre-Training with Knowledge Regularizations", ACMMM, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2211.03314)]
    * **LocVTP**: "LocVTP: Video-Text Pre-training for Temporal Localization", ECCV, 2022 (*Peking University*). [[Paper](https://arxiv.org/abs/2207.10362)][[PyTorch](https://github.com/mengcaopku/LocVTP)]
    * **FineCo**: "Contrastive Video-Language Learning with Fine-grained Frame Sampling", AACL, 2022 (*ICL, UK*). [[Paper](https://arxiv.org/abs/2210.05039)]
    * **EMCL**: "Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations", NeurIPS, 2022 (*Peking University*). [[Paper](https://arxiv.org/abs/2211.11427)][[PyTorch](https://github.com/jpthu17/EMCL)]
    * **LF-VILA**: "Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning", NeurIPS, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2210.06031)][[GitHub](https://github.com/microsoft/XPretrain)]
    * **VATT-GR-CL**: "Scaling Multimodal Pre-Training via Cross-Modality Gradient Harmonization", NeurIPS, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2211.02077)]
    * **LGDN**: "LGDN: Language-Guided Denoising Network for Video-Language Modeling", NeurIPS, 2022 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2209.11388)]
    * **EgoVLP**: "Egocentric Video-Language Pretraining", NeurIPS, 2022 (*NUS*). [[Paper](https://arxiv.org/abs/2206.01670)][[PyTorch](https://github.com/showlab/EgoVLP)]
    * **LiteVL**: "LiteVL: Efficient Video-Language Learning with Enhanced Spatial-Temporal Modeling", EMNLP, 2022 (*Peking University*). [[Paper](https://arxiv.org/abs/2210.11929)]
    * **Singularity**: "Revealing Single Frame Bias for Video-and-Language Learning", arXiv, 2022 (*UNC*). [[Paper](https://arxiv.org/abs/2206.03428)]
    * **VIOLET**: "VIOLET: End-to-End Video-Language Transformers with Masked Visual-token Modeling", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.12681)][[PyTorch](https://github.com/tsujuifu/pytorch_violet)]
    * **SimVTP**: "SimVTP: Simple Video Text Pre-training with Masked Autoencoders", arXiv, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2212.03490)][[PyTorch (in construction)](https://github.com/mayuelala/SimVTP)]
    * **VideoCoCa**: "Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2212.04979)]
    * **i-Code**: "i-Code: An Integrative and Composable Multimodal Learning Framework", AAAI, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2205.01818)][[Code (in construction)](https://github.com/microsoft/i-Code)]
    * **TempCLR**: "TempCLR: Temporal Alignment Representation with Contrastive Learning", ICLR, 2023 (*Columbia*). [[Paper](https://arxiv.org/abs/2212.13738)]
    * **MELTR**: "MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models", CVPR, 2023 (*Korea University*). [[Paper](https://arxiv.org/abs/2303.13009)][[PyTorch](https://github.com/mlvlab/MELTR)]
    * **VIOLETv2**: "An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2209.01540)][[PyTorch](https://github.com/tsujuifu/pytorch_empirical-mvm)]
    * **LAVENDER**: "LAVENDER: Unifying Video-Language Understanding as Masked Language Modeling", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2206.07160)][[Code (in construction)](https://github.com/microsoft/LAVENDER)]
    * **SViTT**: "SViTT: Temporal Learning of Sparse Video-Text Transformers", CVPR, 2023 (*Intel*). [[Paper](https://arxiv.org/abs/2304.08809)][[Website](http://svcl.ucsd.edu/projects/svitt/)]
    * **TVTS**: "Learning Transferable Spatiotemporal Representations from Natural Script Knowledge", CVPR, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2209.15280)][[PyTorch](https://github.com/TencentARC/TVTS/tree/master/v1)]
    * **HBI**: "Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning", CVPR, 2023 (*Peking University*). [[Paper](https://arxiv.org/abs/2303.14369)][[Code (in construction)](https://github.com/jpthu17/HBI)][[Website](https://jpthu17.github.io/HBI/)]
    * **All-in-One**: "All in One: Exploring Unified Video-Language Pre-training", CVPR, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2203.07303)][[PyTorch](https://github.com/showlab/all-in-one)]
    * **VindLU**: "VindLU: A Recipe for Effective Video-and-Language Pretraining", CVPR, 2023 (*UNC*). [[Paper](https://arxiv.org/abs/2212.05051)][[PyTorch](https://github.com/klauscc/VindLU)]
    * **Clover**: "Clover: Towards A Unified Video-Language Alignment and Fusion Model", CVPR, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2207.07885)][[PyTorch (in construction)](https://github.com/LeeYN-43/Clover)]
    * **mPLUG-2**: "mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video", ICML, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2302.00402)][[Code (in construction)](https://github.com/alibaba/AliceMind)]
    * **BUS**: "BUS: Efficient and Effective Vision-language Pre-training with Bottom-Up Patch Summarization", ICCV, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2307.08504)]
    * **UMT**: "Unmasked Teacher: Towards Training-Efficient Video Foundation Models", ICCV, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2303.16058)][[Code (in construction)](https://github.com/OpenGVLab/unmasked_teacher)]
    * **?**: "Long-range Multimodal Pretraining for Movie Understanding", ICCV, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2308.09775)]
    * **EgoVLPv2**: "EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone", ICCV, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2307.05463)][[Website](https://shramanpramanick.github.io/EgoVLPv2/)]
    * **STOA-VLP**: "STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training", arXiv, 2023 (*Harbin Institute of Technology*). [[Papaer](https://arxiv.org/abs/2302.09736)]
    * **G-ViLM**: "Spatiotemporally Discriminative Video-Language Pre-Training with Text Grounding", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.16341)]
    * **VLAB**: "VLAB: Enhancing Video Language Pre-training by Feature Adapting and Blending", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2305.13167)]
    * **i-Code-V2**: "i-Code V2: An Autoregressive Generation Framework over Vision, Language, and Speech Data", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2305.12311)][[PyTorch (in construction)](https://github.com/microsoft/i-Code/tree/main/i-Code-V2)]
    * **TVTSv2**: "TVTSv2: Learning Out-of-the-box Spatiotemporal Visual Representations at Scale", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2305.14173)][[Code (in construction)](https://github.com/TencentARC/TVTS/tree/master/v2)]
    * **VFC**: "Verbs in Action: Improving verb understanding in video-language models", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2304.06708)]
    * **Youku-mPLUG**: "Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2306.04362)]
    * **VideoGLUE**: "VideoGLUE: Video General Understanding Evaluation of Foundation Models", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2307.03166)]
    * **InternVid**: "InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2307.06942)][[Code (in construction)](https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid)]
    * **EVE**: "EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE", arXiv, 2023 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2308.11971)]
    * **Qwen-VL**: "Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2308.12966)][[PyTorch](https://github.com/QwenLM/Qwen-VL)]
* 3D:
    * **CLIP<sup>2</sup>**: "CLIP<sup>2</sup>: Contrastive Language-Image-Point Pretraining from Real-World Point Cloud Data", CVPR, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2303.12417)]
    * **3D-VLP**: "Context-aware Alignment and Mutual Masking for 3D-Language Pre-training", CVPR, 2023 (*Sichuan University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Jin_Context-Aware_Alignment_and_Mutual_Masking_for_3D-Language_Pre-Training_CVPR_2023_paper.html)][[PyTorch](https://github.com/leolyj/3D-VLP)]
    * **SDFusion**: "SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation", CVPR, 2023 (*Snap*). [[Paper](https://arxiv.org/abs/2212.04493)][[PyTorch](https://github.com/yccyenchicheng/SDFusion)][[Website](https://yccyenchicheng.github.io/SDFusion/)]
    * **3D-VisTA**: "3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment", ICCV, 2023 (*Beijing Institute for General Artificial Intelligence (BIGAI)*). [[Paper](https://arxiv.org/abs/2308.04352)][[PyTorch](https://github.com/3d-vista/3D-VisTA)][[Website](https://3d-vista.github.io/)]
    * **RegionPLC**: "RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding", arXiv, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2304.00962)][[Website](https://jihanyang.github.io/projects/RegionPLC)]
    * **3DVLP**: "Vision-Language Pre-training with Object Contrastive Learning for 3D Scene Understanding", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2305.10714)]
    * **CLIPXPlore**: "CLIPXPlore: Coupled CLIP and Shape Spaces for 3D Shape Exploration", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2306.08226)]
* Vision-Audio-Text:
    * **VATT**: "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text", NeurIPS, 2021 (*Google*). [[Paper](https://arxiv.org/abs/2104.11178)][[Tensorflow](https://github.com/google-research/google-research/tree/master/vatt)]
    * **VideoCC**: "Learning Audio-Video Modalities from Image Captions", ECCV, 2022 (*Google*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4217_ECCV_2022_paper.php)][[Website](https://a-nagrani.github.io/videocc.html)]
    * **MUGEN**: "MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration", ECCV, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2204.08058)][[Website](https://mugen-org.github.io/)]
    * **VATLM**: "VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2211.11275)][[PyTorch](https://github.com/microsoft/SpeechT5/tree/main/VATLM)]
    * **CLIP4VLA**: "Accommodating Audio Modality in CLIP for Multimodal Processing", AAAI, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2303.06591)]
    * **data2vec-2.0**: "Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language", ICML, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2212.07525)][[PyTorch](https://github.com/facebookresearch/fairseq/tree/main/examples/data2vec)]
    * **VALOR**: "VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2304.08345)][[PyTorch](https://github.com/TXH-mercury/VALOR)][[Website](https://casia-iva-group.github.io/projects/VALOR/)]
    * **VAST**: "VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2305.18500)]
* More than 3 modalities:
    * **Meta-Transformer**: "Meta-Transformer: A Unified Framework for Multimodal Learning", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2307.10802)][[Code (in construction)](https://github.com/invictus717/MetaTransformer)][[Website](https://kxgong.github.io/meta_transformer/)]
    * **UnIVAL**: "Unified Model for Image, Video, Audio and Language Tasks", arXiv, 2023 (*Sorbonne University, France*). [[Paper](https://arxiv.org/abs/2307.16184)][[PyTorch](https://github.com/mshukor/UnIVAL)][[Website](https://unival-model.github.io/)]
    * **ViT-Lens**: "ViT-Lens: Towards Omni-modal Representations", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2308.10185)][[PyTorch](https://github.com/TencentARC/ViT-Lens)]

[[Back to Overview](#overview)]

### Multi-Modal Retrieval
* General:
    * **Fast-and-Slow**: "Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers", CVPR, 2021 (*DeepMind*). [[Paper](https://arxiv.org/abs/2103.16553)]
    * **HTR**: "Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning", CVPR, 2021 (*Amazon*). [[Paper](https://arxiv.org/abs/2103.13061)][[PyTorch](https://github.com/amzn/image-to-recipe-transformers)]
    * **TERN**: "Towards Efficient Cross-Modal Visual Textual Retrieval using Transformer-Encoder Deep Features", CBMI, 2021 (*National Research Council, Italy*). [[Paper](https://arxiv.org/abs/2106.00358)]
    * **VisualSparta**: "VisualSparta: Sparse Transformer Fragment-level Matching for Large-scale Text-to-Image Search", arXiv, 2021 (*CMU*). [[Paper](https://arxiv.org/abs/2101.00265)]
    * **CCR-CCS**: "More Than Just Attention: Learning Cross-Modal Attentions with Contrastive Constraints", arXiv, 2021 (*Rutgers + Amazon*). [[Paper](https://arxiv.org/abs/2105.09597)]
    * **MCProp**: "Transformer-Based Multi-modal Proposal and Re-Rank for Wikipedia Image-Caption Matching", ICLRW, 2022 (*National Research Council, Italy*). [[Paper](https://arxiv.org/abs/2206.10436)][[PyTorch](https://github.com/mesnico/Wiki-Image-Caption-Matching)]
    * **TASK-former**: "A Sketch Is Worth a Thousand Words: Image Retrieval with Text and Sketch", ECCV, 2022 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2208.03354)][[Website](https://patsorn.me/projects/tsbir/)]
    * **CODER**: "CODER: Coupled Diversity-Sensitive Momentum Contrastive Learning for Image-Text Retrieval", ECCV, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2208.09843)]
    * **?**: "Most and Least Retrievable Images in Visual-Language Query Systems", ECCV, 2022 (*Old Dominion University, Virginia*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7050_ECCV_2022_paper.php)]
    * **MACK**: "MACK: Multimodal Aligned Conceptual Knowledge for Unpaired Image-text Matching", NeurIPS, 2022 (*CAS*). [[Paper](https://openreview.net/forum?id=7lf58jWnDIS)]
    * **MLA**: "Multi-Lingual Acquisition on Multimodal Pre-training for Cross-modal Retrieval", NeurIPS, 2022 (*Renmin University of China*). [[Paper](https://openreview.net/forum?id=h73nTbImOt9)]
    * **SpeechCLIP**: "SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model", IEEE Workshop on Spoken Language Technology (SLT), 2022 (*NTU*). [[Paper](https://arxiv.org/abs/2210.00705)]
    * **LoopITR**: "LoopITR: Combining Dual and Cross Encoder Architectures for Image-Text Retrieval", arXiv, 2022 (*UNC*). [[Paper](https://arxiv.org/abs/2203.05465)]
    * **TNLBT**: "Transformer-based Cross-Modal Recipe Embeddings with Large Batch Training", arXiv, 2022 (*The University of Electro-Communications, Japan*). [[Paper](https://arxiv.org/abs/2205.04948)]
    * **HiVLP**: "HiVLP: Hierarchical Vision-Language Pre-Training for Fast Image-Text Retrieval", arXiv, 2022 (*Huawei*). [[Paper](https://arxiv.org/abs/2205.12105)]
    * **?**: "Revising Image-Text Retrieval via Multi-Modal Entailment". arXiv, 2022 (*Soochow University, China*). [[Paper](https://arxiv.org/abs/2208.10126)]
    * **TokenFlow**: "TokenFlow: Rethinking Fine-grained Cross-modal Alignment in Vision-Language Retrieval", arXiv, 2022 (*Kuaishou*). [[Paper](https://arxiv.org/abs/2209.13822)]
    * **VLPCook**: "Structured Vision-Language Pretraining for Computational Cooking", arXiv, 2022 (*Sorbonne University, France*). [[Paper](https://arxiv.org/abs/2212.04267)]
    * **UniVL-DR**: "Universal Vision-Language Dense Retrieval: Learning A Unified Representation Space for Multi-Modal Retrieval", ICLR, 2023 (*Northeastern University, China*). [[Paper](https://openreview.net/forum?id=PQOlkgsBsik)]
    * **HREM**: "Learning Semantic Relationship Among Instances for Image-Text Matching", CVPR, 2023 (*USTC*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Fu_Learning_Semantic_Relationship_Among_Instances_for_Image-Text_Matching_CVPR_2023_paper.html)]
    * **CHAN**: "Fine-Grained Image-Text Matching by Cross-Modal Hard Aligning Network", CVPR, 2023 (*Zhejiang University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Pan_Fine-Grained_Image-Text_Matching_by_Cross-Modal_Hard_Aligning_Network_CVPR_2023_paper.html)][[PyTorch](https://github.com/ppanzx/CHAN)]
    * **ViLEM**: "ViLEM: Visual-Language Error Modeling for Image-Text Retrieval", CVPR, 2023 (*CAS*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Chen_ViLEM_Visual-Language_Error_Modeling_for_Image-Text_Retrieval_CVPR_2023_paper.html)]
    * **SoftMask**: "Multi-Modal Representation Learning with Text-Driven Soft Masks", CVPR, 2023 (*SNU*). [[Paper](https://arxiv.org/abs/2304.00719)]
    * **MetaPer**: "Meta-Personalizing Vision-Language Models To Find Named Instances in Video", CVPR, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2306.10169)][[PyTorch](https://github.com/danielchyeh/this-is-my)][[Website](https://danielchyeh.github.io/metaper/)]
    * **DivE**: "Improving Cross-Modal Retrieval with Set of Diverse Embeddings", CVPR, 2023 (*POSTECH*). [[Paper](https://arxiv.org/abs/2211.16761)][[Website](https://cvlab.postech.ac.kr/research/DivE/)]
    * **Pic2Word**: "Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2302.03084)][[PyTorch](https://github.com/google-research/composed_image_retrieval)]
    * **ConaCLIP**: "ConaCLIP: Exploring Distillation of Fully-Connected Knowledge Interaction Graph for Lightweight Text-Image Retrieval", ACL Industry Track, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2305.17652)][[PyTorch](https://github.com/alibaba/EasyNLP)]
    * **FNE**: "Your Negative May not Be True Negative: Boosting Image-Text Matching with False Negative Elimination", ACMMM, 2023 (*University of Electronic Science and Technology of China (UESTC)*). [[Paper](https://arxiv.org/abs/2308.04380)][[PyTorch](https://github.com/LuminosityX/FNE)]
    * **HAT**: "Unifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval", ACMMM, 2023 (*University of Electronic Science and Technology of China (UESTC)*). [[Paper](https://arxiv.org/abs/2308.04343)][[PyTorch](https://github.com/LuminosityX/HAT)]
    * **STAIR**: "STAIR: Learning Sparse Text and Image Representation in Grounded Tokens", arXiv, 2023 (*Apple*). [[Paper](https://arxiv.org/abs/2301.13081)]
    * **ChatIR**: "Chatting Makes Perfect - Chat-based Image Retrieval", arXiv, 2023 (*The Hebrew University of Jerusalem, Israel*). [[Paper](https://arxiv.org/abs/2305.20062)]
    * **TransAgg**: "Zero-shot Composed Text-Image Retrieval", arXiv, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2306.07272)][[PyTorch](https://github.com/Code-kunkun/ZS-CIR)][[Website](https://code-kunkun.github.io/ZS-CIR/)]
* Video:
    * **MMT**: "Multi-modal Transformer for Video Retrieval", ECCV, 2020 (*INRIA + Google*). [[Paper](https://arxiv.org/abs/2007.10639)][[Website](http://thoth.inrialpes.fr/research/MMT/)]
    * **AYCE**: "All You Can Embed: Natural Language based Vehicle Retrieval with Spatio-Temporal Transformers", CVPRW, 2021 (*University of Modena and Reggio Emilia*). [[Paper](https://arxiv.org/abs/2106.10153)][[PyTorch](https://github.com/cscribano/AYCE_2021)]
    * **HiT**: "HiT: Hierarchical Transformer with Momentum Contrast for Video-Text Retrieval", ICCV, 2021 (*Kuaishou*). [[Paper](https://arxiv.org/abs/2103.15049)]
    * **Frozen**: "Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval", ICCV, 2021 (*Oxford*). [[Paper](https://arxiv.org/abs/2104.00650)][[Pytorch](https://github.com/m-bain/frozen-in-time)][[Website](https://www.robots.ox.ac.uk/~vgg/research/frozen-in-time/)][[Dataset](https://m-bain.github.io/webvid-dataset/)]
    * **CLIP4Clip**: "CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval", arXiv, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2104.08860)][[PyTorch](https://github.com/ArrowLuo/CLIP4Clip)]
    * **MMFT**: "Everything at Once - Multi-modal Fusion Transformer for Video Retrieval", CVPR, 2022 (*Goethe University Frankfurt, Germany*). [[Paper](https://arxiv.org/abs/2112.04446)]
    * **X-Pool**: "X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval", CVPR, 2022 (*Layer 6 AI, Toronto*). [[Paper](https://arxiv.org/abs/2203.15086)][[PyTorch](https://github.com/layer6ai-labs/xpool)][[Website](https://layer6ai-labs.github.io/xpool/)]
    * **MVPt**: "It's Time for Artistic Correspondence in Music and Video", CVPR, 2022 (*Adobe*). [[Paper](https://arxiv.org/abs/2206.07148)][[Website](https://musicforvideo.cs.columbia.edu/)]
    * **OA-Trans**: "Object-aware Video-language Pre-training for Retrieval", CVPR, 2022 (*NUS*). [[Paper](https://arxiv.org/abs/2112.00656)][[PyTorch](https://github.com/FingerRec/OA-Transformer)]
    * **BridgeFormer**: "Bridging Video-text Retrieval with Multiple Choice Questions", CVPR, 2022 (*HKU*). [[Paper](https://arxiv.org/abs/2201.04850)][[PyTorch](https://github.com/TencentARC/MCQ)][[Website](https://geyuying.github.io/MCQ.html)]
    * **CenterCLIP**: "CenterCLIP: Token Clustering for Efficient Text-Video Retrieval", SIGIR, 2022 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2205.00823)]
    * **X-CLIP**: "X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval", ACMMM, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2207.07285)]
    * **HiSE**: "Boosting Video-Text Retrieval with Explicit High-Level Semantics", ACMMM, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2208.04215)]
    * **TS2-Net**: "TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval", ECCV, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2207.07852)][[PyTorch](https://github.com/yuqi657/ts2_net)]
    * **LAFF**: "Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video Retrieval", ECCV, 2022 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2112.01832)]
    * **ECLIPSE**: "ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound", ECCV, 2022 (*UNC*). [[Paper](https://arxiv.org/abs/2204.02874)][[PyTorch](https://github.com/GenjiB/ECLIPSE)][[Website](https://yanbo.ml/project_page/eclipse/)]
    * **MILES**: "MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval", ECCV, 2022 (*HKU*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1159_ECCV_2022_paper.php)][[PyTorch](https://github.com/TencentARC/MCQ/blob/main/MILES.md)]
    * **VTC**: "VTC: Improving Video-Text Retrieval with User Comments", ECCV, 2022 (*Unitary, UK*). [[Paper](https://arxiv.org/abs/2210.10820)][[PyTorch](https://github.com/unitaryai/VTC)][[Website](https://unitaryai.github.io/vtc-paper/)]
    * **LINAS**: "Learning Linguistic Association towards Efficient Text-Video Retrieval", ECCV, 2022 (*CAS*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3305_ECCV_2022_paper.php)][[PyTorch](https://github.com/silenceFS/LINAS)]
    * **?**: "A Simple Transformer-Based Model for Ego4D Natural Language Queries Challenge", ECCVW, 2022 (*UW-Madison*). [[Paper](https://arxiv.org/abs/2211.08704)]
    * **?**: "Text-Adaptive Multiple Visual Prototype Matching for Video-Text Retrieval", NeurIPS, 2022 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2209.13307)]
    * **ConTra**: "ConTra: (Con)text (Tra)nsformer for Cross-Modal Video Retrieval", ACCV, 2022 (*University of Bristol, UK*). [[Paper](https://arxiv.org/abs/2210.04341)]
    * **RaP**: "RaP: Redundancy-aware Video-language Pre-training for Text-Video Retrieval", EMNLP, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2210.06881)][[PyTorch](https://github.com/caskcsg/VLP/tree/main/RaP)]
    * **MDMMT-2**: "MDMMT-2: Multidomain Multimodal Transformer for Video Retrieval, One More Step Towards Generalization", arXiv, 2022 (*Huawei*). [[Paper](https://arxiv.org/abs/2203.07086)]
    * **M2HF**: "M2HF: Multi-level Multi-modal Hybrid Fusion for Text-Video Retrieval", arXiv, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2208.07664)]
    * **FIRE**: "Fighting FIRe with FIRE: Assessing the Validity of Text-to-Video Retrieval Benchmarks", arXiv, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2210.05038)][[PyTorch](https://github.com/facebookresearch/mm-retrieval-evaluation)]
    * **Cross-Modal-Adapter**: "Cross-Modal Adapter for Text-Video Retrieval", arXiv, 2022 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2211.09623)][[PyTorch (in construction)](https://github.com/LeapLabTHU/Cross-Modal-Adapter)]
   * **MAC**: "Masked Contrastive Pre-Training for Efficient Video-Text Retrieval", arXiv, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2212.00986)]
   * **CLIP-ViP**: "CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment", ICLR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2209.06430)][[Code (in construction)](https://github.com/microsoft/XPretrain/tree/main/CLIP-ViP)]
    * **HiREST**: "Hierarchical Video-Moment Retrieval and Step-Captioning", CVPR, 2023 (*UNC + Meta*). [[Paper](https://arxiv.org/abs/2303.16406)][[PyTorch](https://github.com/j-min/HiREST)][[Website](https://hirest-cvpr2023.github.io/)]
    * **Cap4Video**: "Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?", CVPR, 2023 (*The University of Sydney*). [[Paper](https://arxiv.org/abs/2301.00184)][[PyTorch](https://github.com/whwu95/Cap4Video)]
    * **CLIPPING**: "CLIPPING: Distilling CLIP-Based Models With a Student Base for Video-Language Retrieval", CVPR, 2023 (*Huawei*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Pei_CLIPPING_Distilling_CLIP-Based_Models_With_a_Student_Base_for_Video-Language_CVPR_2023_paper.html)]
    * **CNVid-3.5M**: "CNVid-3.5M: Build, Filter, and Pre-Train the Large-Scale Public Chinese Video-Text Dataset", CVPR, 2023 (*Ant Group*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Gan_CNVid-3.5M_Build_Filter_and_Pre-Train_the_Large-Scale_Public_Chinese_Video-Text_CVPR_2023_paper.html)][[GitHub (in construction)](https://github.com/CNVid/CNVid-3.5M)]
    * **CelebV-Text**: "CelebV-Text: A Large-Scale Facial Text-Video Dataset", CVPR, 2023 (*University of Sydney*). [[Paper](https://arxiv.org/abs/2303.14717)][[GitHub](https://github.com/CelebV-Text/CelebV-Text)][[Website](https://celebv-text.github.io/)]
    * **ReST**: "Relational Space-Time Query in Long-Form Videos", CVPR, 2023 (*Meta*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Relational_Space-Time_Query_in_Long-Form_Videos_CVPR_2023_paper.html)]
    * **NaQ**: "NaQ: Leveraging Narrations as Queries to Supervise Episodic Memory", CVPR, 2023 (*UT Austin*). [[Paper](https://arxiv.org/abs/2301.00746)][[PyTorch](https://github.com/srama2512/NaQ)][[Website](https://vision.cs.utexas.edu/projects/naq/)]
    * **?**: "Towards Fast Adaptation of Pretrained Contrastive Models for Multi-channel Video-Language Retrieval", CVPR, 2023 (*Columbia*). [[Paper](https://arxiv.org/abs/2206.02082)][[Code (in contruction)](https://github.com/XudongLinthu/upgradable-multimodal-intelligence)]
    * **VoP**: "VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval", CVPR, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2211.12764)][[Code (in construction)](https://github.com/bighuang624/VoP)][[Website](https://kyonhuang.top/publication/text-video-cooperative-prompt-tuning)]
    * **SpotEM**: "SpotEM: Efficient Video Search for Episodic Memory", ICML, 2023 (*UT Austin*). [[Paper](https://arxiv.org/abs/2306.15850)][[Website](https://vision.cs.utexas.edu/projects/spotem/)]
    * **PromptSwitch**: "Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval", ICCV, 2023 (*University of Adelaide*). [[Paper](https://arxiv.org/abs/2308.07648)][[PyTorch (in construction)](https://github.com/bladewaltz1/PromptSwitch)]
    * **?**: "Simple Baselines for Interactive Video Retrieval with Questions and Answers", ICCV, 2023 (*Princeton*). [[Paper](https://arxiv.org/abs/2308.10402)][[Code (in construction)](https://github.com/kevinliang888/IVR-QA-baselines)]
    * **MeVTR**: "Multi-event Video-Text Retrieval", ICCV, 2023 (*LMU Munich*). [[Paper](https://arxiv.org/abs/2308.11551)][[Code (in construction)](https://github.com/gengyuanmax/MeVTR)]
    * **In-Style**: "In-Style: Bridging Text and Uncurated Videos with Style Transfer for Text-Video Retrieval", ICCV, 2023 (*MPI*). [[Paper](https://arxiv.org/abs/2309.08928)][[Code (in construction)](https://github.com/ninatu/in_style)]
    * **ReGaDa**: "Video-adverb retrieval with compositional adverb-action embeddings", BMVC, 2023 (*University of Tübingen, Germany*). [[Paper](https://arxiv.org/abs/2309.15086)][[Code (in construction)](https://github.com/ExplainableML/ReGaDa)][[Website](https://hummelth.github.io/ReGaDa/)]
    * **DiffusionRet**: "DiffusionRet: Generative Text-Video Retrieval with Diffusion Model", arXiv, 2023 (*Peking University*). [[Paper](https://arxiv.org/abs/2303.09867)]
    * **TextVR**: "A Large Cross-Modal Video Retrieval Dataset with Reading Comprehension", arXiv, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2305.03347)][[PyTorch](https://github.com/callsys/TextVR)][[Website](https://sites.google.com/view/loveucvpr23/guest-track)]
    * **MASCOT**: "Mask to reconstruct: Cooperative Semantics Completion for Video-text Retrieval", arXiv, 2023 (*?*). [[Paper](https://arxiv.org/abs/2305.07910)]
    * **CrossTVR**: "Fine-grained Text-Video Retrieval with Frozen Image Encoders", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2307.09972)]
    * **TEFAL**: "Audio-Enhanced Text-to-Video Retrieval using Text-Conditioned Feature Alignment", arXiv, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2307.12964)]
    * **TeachCLIP**: "TeachCLIP: Multi-Grained Teaching for Efficient Text-to-Video Retrieval", arXiv, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2308.01217)]
    * **CoVR**: "CoVR: Learning Composed Video Retrieval from Web Video Captions", arXiv, 2023 (*Ecole des Ponts ParisTech (ENPC), France*). [[Paper](https://arxiv.org/abs/2308.14746)][[PyTorch](https://github.com/lucas-ventura/CoVR/)][[Website](https://imagine.enpc.fr/~ventural/covr/)]
* Vision-Audio-Text:
    * **Multi-SK**: "Preserving Modality Structure Improves Multi-Modal Learning", ICCV, 2023 (*UCF*). [[Paper](https://arxiv.org/abs/2308.13077)][[Code (in construction)](https://github.com/Swetha5/Multi_Sinkhorn_Knopp)]
* Others:
    * **IRRA**: "Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image Person Retrieval", CVPR, 2023 (*Wuhan University*). [[Paper](https://arxiv.org/abs/2303.12501)][[PyTorch](https://github.com/anosorae/IRRA)]
    * **ZS-SBIR**: "CLIP for All Things Zero-Shot Sketch-Based Image Retrieval, Fine-Grained or Not", CVPR, 2023 (*University of Surrey, UK*). [[Paper](https://arxiv.org/abs/2303.13440)][[PyTorch](https://github.com/aneeshan95/Sketch_LVM)]
    * **ViML**: "Language-Guided Music Recommendation for Video via Prompt Analogies", CVPR, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2306.09327)][[Website](https://www.danielbmckee.com/language-guided-music-for-video/)]
    * **Auto-ACD**: "A Large-scale Dataset for Audio-Language Representation Learning", arXiv, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2309.11500)][[Code (in construction)](https://github.com/LoieSun/Auto-ACD)][[Website](https://auto-acd.github.io/)]

[[Back to Overview](#overview)]

### Multi-Modal Generation
* General:
    * **AttnGAN**: "AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks", CVPR, 2018 (*Microsoft*). [[Paper](https://arxiv.org/abs/1711.10485)][[PyTorch](https://github.com/taoxugit/AttnGAN)]
    * **ControlGAN**: "Controllable Text-to-Image Generation", NeurIPS, 2019 (*Oxford*). [[Paper](https://arxiv.org/abs/1909.07083)][[PyTorch](https://github.com/mrlibw/ControlGAN)]
    * **DALL-E**: "Zero-Shot Text-to-Image Generation", ICML, 2021 (*OpenAI*). [[Paper](https://arxiv.org/abs/2102.12092)][[PyTorch](https://github.com/openai/DALL-E)][[PyTorch (lucidrains)](https://github.com/lucidrains/DALLE-pytorch)]
    * **CogView**: "CogView: Mastering Text-to-Image Generation via Transformers", NeurIPS, 2021 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2105.13290)][[PyTorch](https://github.com/THUDM/CogView)][[Website](https://lab.aminer.cn/cogview/index.html)]
    * **Layout-VQGAN**: "Text-to-Image Synthesis Based on Object-Guided Joint-Decoding Transformer", CVPR, 2022 (*CAS*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Wu_Text-to-Image_Synthesis_Based_on_Object-Guided_Joint-Decoding_Transformer_CVPR_2022_paper.html)]
    * **Lafite**: "Towards Language-Free Training for Text-to-Image Generation", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.13792)][[PyTorch](https://github.com/drboog/Lafite)]
    * **LDM**: "High-Resolution Image Synthesis with Latent Diffusion Models", CVPR, 2022 (*LMU Munich*). [[Paper](https://arxiv.org/abs/2112.10752)][[PyTorch](https://github.com/CompVis/latent-diffusion)]
    * **AvatarCLIP**: "AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars", SIGGRAPH, 2022 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2205.08535)][[PyTorch](https://github.com/hongfz16/AvatarCLIP)][[Website](https://hongfz16.github.io/projects/AvatarCLIP.html)]
    * **StoryDALL-E**: "StoryDALL-E: Adapting Pretrained Text-to-Image Transformers for Story Continuation", ECCV, 2022 (*UNC*). [[Paper](https://arxiv.org/abs/2209.06192)][[PyTorch](https://github.com/adymaharana/storydalle)]	
    * **Make-A-Scene**: "Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors", ECCV, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2203.13131)][[Video](https://www.youtube.com/watch?v=QLTyqoJJKTo&ab_channel=OranGafni)]
    * **TCTIG**: "Trace Controlled Text to Image Generation", ECCV, 2022 (*Beihang University*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1894_ECCV_2022_paper.php)]
    * **CogView2**: "CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers", NeurIPS, 2022 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2204.14217)][[PyTorch](https://github.com/THUDM/CogView2)]
    * **CLIPDraw**: "CLIPDraw: Exploring Text-to-Drawing Synthesis through Language-Image Encoders", NeurIPS, 2022 (*Cross Compass, Japan*). [[Paper](https://arxiv.org/abs/2106.14843)][[PyTorch](https://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb)][[Blog](https://kvfrans.com/clipdraw-exploring-text-to-drawing-synthesis/)]
    * **Imagen**: "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding", NeurIPS, 2022 (*Google*). [[Paper](https://gweb-research-imagen.appspot.com/paper.pdf)][[Website](https://gweb-research-imagen.appspot.com/)]
    * **?**: "Human Evaluation of Text-to-Image Models on a Multi-Task Benchmark", NeurIPSW, 2022 (*Boston + MIT + Columbia*). [[Paper](https://arxiv.org/abs/2211.12112)]
    * **DALL-Eval**: "DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers", arXiv, 2022 (*UNC*). [[Paper](https://arxiv.org/abs/2202.04053)][[PyTorch](https://github.com/j-min/DallEval)]
    * **DALL-E-2**: "Hierarchical Text-Conditional Image Generation with CLIP Latents", arXiv, 2022 (*OpenAI*). [[Paper](https://arxiv.org/abs/2204.06125)][[Website](https://openai.com/dall-e-2/)]
    * **?**: "A very preliminary analysis of DALL-E 2", arXiv, 2022 (*NYU*). [[Paper](https://arxiv.org/abs/2204.13807)]
    * **GLIDE**: "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models", arXiv, 2022 (*OpenAI*). [[Paper](https://arxiv.org/abs/2112.10741)][[PyTorch](https://github.com/openai/glide-text2im)]
    * **?**: "Discovering the Hidden Vocabulary of DALLE-2", arXiv, 2022 (*UT Austin*). [[Paper](https://arxiv.org/abs/2206.00169)]
    * **Parti**: "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2206.10789)][[GitHub](https://github.com/google-research/parti)][[Website](https://parti.research.google/)]
    * **Textual-Inversion**: "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion", arXiv, 2022 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2208.01618)][[Website](https://textual-inversion.github.io/)]
    * **VLMGAN**: "Vision-Language Matching for Text-to-Image Synthesis via Generative Adversarial Networks", arXiv, 2022 (*Fudan University*). [[Paper](https://arxiv.org/abs/2208.09596)]
    * **PDM**: "Progressive Denoising Model for Fine-Grained Text-to-Image Generation", arXiv, 2022 (*Meituan*). [[Paper](https://arxiv.org/abs/2210.02291)]
    * **FS-VQG**: "Few-Shot Visual Question Generation: A Novel Task and Benchmark Datasets", arXiv, 2022 (*IIT Kharagpur*). [[Paper](https://arxiv.org/abs/2210.07076)]
    * **Swinv2-Imagen**: "Swinv2-Imagen: Hierarchical Vision Transformer Diffusion Models for Text-to-Image Generation", arXiv, 2022 (*Auckland University of Technology*). [[Paper](https://arxiv.org/abs/2210.09549)]
    * **UniTune**: "UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2210.09477)]
    * **VSD**: "Visual Spatial Description: Controlled Spatial-Oriented Image-to-Text Generation", arXiv, 2022 (*Tianjin University*). [[Paper](https://arxiv.org/abs/2210.11109)][[Code (in construction)](https://github.com/zhaoyucs/VSD)]
    * **Lafite2**: "Lafite2: Few-shot Text-to-Image Generation", arXiv, 2022 (*SUNY, Buffalo*). [[Paper](https://arxiv.org/abs/2210.14124)]
    * **eDiffi**: "eDiffi: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers", arXiv, 2022 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2211.01324)][[Website](https://research.nvidia.com/labs/dir/eDiff-I/)]
    * **SpaText**: "SpaText: Spatio-Textual Representation for Controllable Image Generation", arXiv, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2211.14305)][[Website](https://omriavrahami.com/spatext/)]
    * **Story-LDM**: "Make-A-Story: Visual Memory Conditioned Consistent Story Generation", arXiv, 2022 (*UBC + Snap*). [[Paper](https://arxiv.org/abs/2211.13319)]
    * **Structure-Diffusion**: "Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis", arXiv, 2022 (*UCSB + UC Santa Cruz*). [[Paper](https://arxiv.org/abs/2212.05032)][[PyTorch](https://github.com/weixi-feng/Structured-Diffusion-Guidance)][[Website](https://weixi-feng.github.io/structure-diffusion-guidance/)]
    * **Re-Imagen**: "Re-Imagen: Retrieval-Augmented Text-to-Image Generator", ICLR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2209.14491)]
    * **Prompt-to-Prompt**: "Prompt-to-Prompt Image Editing with Cross Attention Control", ICLR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2208.01626)][[PyTorch](https://github.com/google/prompt-to-prompt/)][[Website](https://prompt-to-prompt.github.io/)]
    * **UniD3**: "Unified Discrete Diffusion for Simultaneous Vision-Language Generation", ICLR, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2211.14842)]
    * **T2P**: "Zero-Shot Text-to-Parameter Translation for Game Character Auto-Creation", CVPR, 2023 (*Fuxi AI Lab*). [[Paper](https://arxiv.org/abs/2303.01311)]
    * **GLIGEN**: "GLIGEN: Open-Set Grounded Text-to-Image Generation", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2301.07093)][[PyTorch](https://github.com/gligen/GLIGEN)][[Website](https://gligen.github.io/)]
    * **MAGVLT**: "MAGVLT: Masked Generative Vision-and-Language Transformer", CVPR, 2023 (*Kakao*). [[Paper](https://arxiv.org/abs/2303.12208)]
    * **ReCo**: "ReCo: Region-Controlled Text-to-Image Generation", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2211.15518)][[PyTorch](https://github.com/microsoft/ReCo)]
    * **GALIP**: "GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis", CVPR, 2023 (*Nanjing University of Posts and Telecommunications*). [[Paper](https://arxiv.org/abs/2301.12959)][[PyTorch](https://github.com/tobran/GALIP)]
    * **DreamBooth**: "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2208.12242)][[GitHub](https://github.com/google/dreambooth)][[Website](https://dreambooth.github.io/)]
    * **RIATIG**: "RIATIG: Reliable and Imperceptible Adversarial Text-to-Image Generation With Natural Prompts", CVPR, 2023 (*Washington University in St. Louis*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Liu_RIATIG_Reliable_and_Imperceptible_Adversarial_Text-to-Image_Generation_With_Natural_Prompts_CVPR_2023_paper.html)]
    * **ERNIE-ViLG-2.0**: "ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts", CVPR, 2023 (*Baidu*). [[Paper](https://arxiv.org/abs/2210.15257)][[Website](https://wenxin.baidu.com/ernie-vilg)]
    * **GigaGAN**: "Scaling up GANs for Text-to-Image Synthesis", CVPR, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2303.05511)][[PyTorch](https://github.com/mingukkang/GigaGAN/tree/main/evaluation)][[Website](https://mingukkang.github.io/GigaGAN/)]
    * **Shifted-Diffusion**: "Shifted Diffusion for Text-to-image Generation", CVPR, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2211.15388)][[PyTorch](https://github.com/drboog/Shifted_Diffusion)]
    * **Specialist-Diffusion**: "Specialist Diffusion: Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models To Learn Any Unseen Style", CVPR, 2023 (*Picsart*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Lu_Specialist_Diffusion_Plug-and-Play_Sample-Efficient_Fine-Tuning_of_Text-to-Image_Diffusion_Models_To_CVPR_2023_paper.html)][[Website](https://specialist-diffusion.github.io/)]
    * **?**: "Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation", CVPR, 2023 (*CyberAgent, Japan*). [[Paper](https://arxiv.org/abs/2304.01816)]
    * **Custom-Diffusion**: "Multi-Concept Customization of Text-to-Image Diffusion", CVPR, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2212.04488)]
    * **UniDiffuser**: "One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale", ICML, 2023 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2303.06555)][[Pytorch](https://github.com/thu-ml/unidiffuser)]
    * **Muse**: "Muse: Text-To-Image Generation via Masked Generative Transformers", ICML, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2301.00704)][[Website](https://muse-model.github.io/)]
    * **RA-CM3**: "Retrieval-Augmented Multimodal Language Modeling", ICML, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2211.12561)]
    * **StyleGAN-T**: "StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis", ICML, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2301.09515)][[PyTorch](https://github.com/autonomousvision/stylegan-t)][[Website](https://sites.google.com/view/stylegan-t/)]
    * **VD**: "Versatile Diffusion: Text, Images and Variations All in One Diffusion Model", ICCV, 2023 (*Oregon*). [[Paper](https://arxiv.org/abs/2211.08332)][[PyTorch](https://github.com/SHI-Labs/Versatile-Diffusion)]
    * **E4T**: "Designing an Encoder for Fast Personalization of Text-to-Image Models", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2302.12228)][[Website](https://tuning-encoder.github.io/)]
    * **?**: "Controlled and Conditional Text to Image Generation with Diffusion Prior", arXiv, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2302.11710)]
    * **Lformer**: "Lformer: Text-to-Image Generation with L-shape Block Parallel Decoding", arXiv, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2303.03800)]
    * **UMM-Diffusion**: "Unified Multi-Modal Latent Diffusion for Joint Subject and Text Conditional Image Generation", arXiv, 2023 (*Peking University*). [[Paper](https://arxiv.org/abs/2303.09319)]
    * **TIFA**: "TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering", arXiv, 2023 (*UW*). [[Paper](https://arxiv.org/abs/2303.11897)][[Code (in construction)](https://github.com/Yushi-Hu/tifa)][[Website](https://tifa-benchmark.github.io/)]
    * **ToMESD**: "Token Merging for Fast Stable Diffusion", arXiv, 2023 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2303.17604)][[PyTorch](https://github.com/dbolya/tomesd)]
    * **layout-guidance**: "Training-Free Layout Control with Cross-Attention Guidance", arXiv, 2023 (*Oxford*). [[Paper](https://arxiv.org/abs/2304.03373)][[PyTorch](https://github.com/silent-chen/layout-guidance)][[Website](https://silent-chen.github.io/layout-guidance/)]
    * **HRS-Bench**: "HRS-Bench: Holistic, Reliable and Scalable Benchmark for Text-to-Image Models", arXiv, 2023 (*KAUST*). [[Paper](https://arxiv.org/abs/2304.05390)][[GitHub](https://github.com/eslambakr/HRS_benchmark)][[Website](https://eslambakr.github.io/hrsbench.github.io/)]
    * **SeedSelect**: "It is all about where you start: Text-to-image generation with seed selection", arXiv, 2023 (*Bar-Ilan University, Israel*). [[Paper](https://arxiv.org/abs/2304.14530)]
    * **DisenBooth**: "DisenBooth: Disentangled Parameter-Efficient Tuning for Subject-Driven Text-to-Image Generation", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2305.03374)]
    * **VideoOFA**: "VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/pdf/2305.03204.pdf)]
    * **FastComposer**: "FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention", arXiv, 2023 (*MIT*). [[Paper](https://arxiv.org/abs/2305.10431)][[PyTorch](https://github.com/mit-han-lab/fastcomposer)][[Website](https://fastcomposer.mit.edu/)]
    * **LLMScore**: "LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation", arXiv, 2023 (*UCSB*). [[Paper](https://arxiv.org/abs/2305.11116)][[PyTorch](https://github.com/YujieLu10/LLMScore)]
    * **CoDi**: "Any-to-Any Generation via Composable Diffusion", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2305.11846)][[PyTorch](https://github.com/microsoft/i-Code/tree/main/i-Code-V3)][[Website](https://codi-gen.github.io/)]
    * **?**: "The CLIP Model is Secretly an Image-to-Prompt Converter", arXiv, 2023 (*Xidian University*). [[Paper](https://arxiv.org/abs/2305.12716)]
    * **PoS-subspaces**: "Parts of Speech-Grounded Subspaces in Vision-Language Models", arXiv, 2023 (*Queen Mary University of London*). [[Paper](https://arxiv.org/abs/2305.14053)][[PyTorch (in construction)](https://github.com/james-oldfield/PoS-subspaces)][[Website](http://eecs.qmul.ac.uk/~jo001/PoS-subspaces/)]
    * **VPGen**: "Visual Programming for Text-to-Image Generation and Evaluation", arXiv, 2023 (*UNC*). [[Paper](https://arxiv.org/abs/2305.15328)][[PyTorch](https://github.com/j-min/VPGen)][[Website](https://vp-t2i.github.io/)]
    * **BLIP-Diffusion**: "BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing", arXiv, 2023 (*Salesforce*). [[Paper](https://arxiv.org/abs/2305.14720)][[Code (in construction)](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion)][[Website](https://dxli94.github.io/BLIP-Diffusion-website/)]
    * **SeeCoder**: "Prompt-Free Diffusion: Taking "Text" out of Text-to-Image Diffusion Models", arXiv, 2023 (*Picsart*). [[Paper](https://arxiv.org/abs/2305.16223)][[PyTorch](https://github.com/SHI-Labs/Prompt-Free-Diffusion)]
    * **GILL**: "Generating Images with Multimodal Language Models", arXiv, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2305.17216)][[Code (in construction)](https://github.com/kohjingyu/gill)][[Website](https://jykoh.com/gill)]
    * **CAC**: "Localized Text-to-Image Generation for Free via Cross Attention Control", arXiv, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2306.14636)]
    * **CLIPAG**: "CLIPAG: Towards Generator-Free Text-to-Image Generation", arXiv, 2023 (*Technion, Israel*). [[Paper](https://arxiv.org/abs/2306.16805)]
    * **PACGen**: "Generate Anything Anywhere in Any Scene", arXiv, 2023 (*UW Madison*). [[Paper](https://arxiv.org/abs/2306.17154)][[Code (in construction)](https://github.com/Yuheng-Li/PACGen)][[Website](https://yuheng-li.github.io/PACGen/)]
    * **SPAE**: "SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2306.17842)]
    * **DA-Score**: "Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback", arXiv, 2023 (*ANU*). [[Paper](https://arxiv.org/abs/2307.04749)][[Code (in construction)](https://github.com/1jsingh/Divide-Evaluate-and-Refine)][[Website](https://1jsingh.github.io/divide-evaluate-and-refine)]
    * **HyperDreamBooth**: "HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2307.06949)][[Website](https://hyperdreambooth.github.io/)]
    * **?**: "Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2307.06925)][[Website](https://datencoder.github.io/)]
    * **GORS**: "T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation", arXiv, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2307.06350)][[Website](https://karine-h.github.io/T2I-CompBench/)][[PyTorch](https://github.com/Karine-Huang/T2I-CompBench)]
    * **IP-Adapter**: "IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2308.06721)][[Website](https://ip-adapter.github.io/)]
    * **ORES**: "ORES: Open-vocabulary Responsible Visual Synthesis", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2308.13785)]
    * **CM3Leon**: "Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2309.02591)]
    * **DreamLLM**: "DreamLLM: Synergistic Multimodal Comprehension and Creation", arXiv, 2023 (*Megvii*). [[Paper](https://arxiv.org/abs/2309.11499)][[Code (in construction)](https://github.com/RunpeiDong/DreamLLM)][[Website](https://dreamllm.github.io/)]
    * **FreeU**: "FreeU: Free Lunch in Diffusion U-Net", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2309.11497)][[Website](https://chenyangsi.top/FreeU/)][[Code (in construction)](https://github.com/ChenyangSi/FreeU)]
* Video:
    * **Imagen-Video**: "Imagen Video: High Definition Video Generation with Diffusion Models", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2210.02303)][[Website](https://imagen.research.google/video/)]
    * **Phenaki**: "Phenaki: Variable Length Video Generation From Open Domain Textual Description", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2210.02399)][[PyTorch (LAION-AI, in construction)](https://github.com/LAION-AI/phenaki)][[Website](https://phenaki.video/)]
    * **?**: "Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization", arXiv, 2022 (*CMU*). [[Paper](https://arxiv.org/abs/2210.12826)][[PyTorch](https://github.com/pschaldenbrand/Text2Video)][[Website](https://pschaldenbrand.github.io/text2video/)]
    * **MagicVideo**: "MagicVideo: Efficient Video Generation With Latent Diffusion Models", arXiv, 2022 (*ByteDance*). [[Paper](https://arxiv.org/abs/2211.11018)][[Website](https://magicvideo.github.io/)]
    * **CogVideo**: "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers", ICLR, 2023 (*Tsinghua University*) [[Paper](https://arxiv.org/abs/2205.15868)][[GitHub (in construction)](https://github.com/THUDM/CogVideo)]
    * **Make-A-Video**: "Make-A-Video: Text-to-Video Generation without Text-Video Data", ICLR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2209.14792)]
    * **VideoLDM**: "Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models", CVPR, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2304.08818)][[Website](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)]
    * **MMVG**: "Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation", CVPR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2211.12824)]
    * **MM-Diffusion**: "MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2212.09478)][[PyTorch](https://github.com/researchmm/MM-Diffusion)]
    * **PYoCo**: "Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models", ICCV, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2305.10474)][[Website](https://research.nvidia.com/labs/dir/pyoco/)]
    * **Text2Video-Zero**: "Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators", ICCV, 2023 (*Picsart*). [[Paper](https://arxiv.org/abs/2303.13439)][[Code (in construction)](https://github.com/Picsart-AI-Research/Text2Video-Zero)]
    * **Text2Performer**: "Text2Performer: Text-Driven Human Video Generation", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2304.08483)][[Code (in construction)](https://github.com/yumingj/Text2Performer)][[Website](https://yumingj.github.io/projects/Text2Performer.html)]
    * **VideoFactory**: "VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2305.10874)]
    * **Video-Adapter**: "Probabilistic Adaptation of Text-to-Video Models", arXiv, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2306.01872)][[Website](https://video-adapter.github.io/video-adapter/)]
    * **SimDA**: "SimDA: Simple Diffusion Adapter for Efficient Video Generation", arXiv, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2308.09710)][[Website](https://chenhsing.github.io/SimDA/)]
* 3D:
    * **Magic3D**: "Magic3D: High-Resolution Text-to-3D Content Creation", CVPR, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2211.10440)][[Website](https://research.nvidia.com/labs/dir/magic3d/)]
    * **CLIP-Sculptor**: "CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Natural Language", CVPR, 2023 (*Autodesk*). [[Paper](https://arxiv.org/abs/2211.01427)][[Website](https://ivl.cs.brown.edu/#/projects/clip-sculptor)]
    * **Diffusion-SDF**: "Diffusion-SDF: Text-to-Shape via Voxelized Diffusion", CVPR, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2212.03293)][[PyTorch](https://github.com/ttlmh/Diffusion-SDF)][[Website](https://ttlmh.github.io/DiffusionSDF/)]
    * **TAPS3D**: "TAPS3D: Text-Guided 3D Textured Shape Generation from Pseudo Supervision", CVPR, 2023 (*Bytedance*). [[Paper](https://arxiv.org/abs/2303.13273)][[PyTorch](https://github.com/plusmultiply/TAPS3D)]
    * **Dream3D**: "Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models", CVPR, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2212.14704)][[Website](https://bluestyle97.github.io/dream3d/)]
    * **ATT3D**: "ATT3D: Amortized Text-To-3D Object Synthesis", arXiv, 2023 (*NVIDIA*). [[Paper](https://research.nvidia.com/labs/toronto-ai/ATT3D/images/papers/att3d.pdf)][[Website](https://research.nvidia.com/labs/toronto-ai/ATT3D/)]
    * **InstructP2P**: "InstructP2P: Learning to Edit 3D Point Clouds with Text Instructions", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2306.07154)]
    * **ATT3D**: "ATT3D: Amortized Text-to-3D Object Synthesis", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2306.07349)][[Website](https://research.nvidia.com/labs/toronto-ai/ATT3D/)]
    * **SDS-Complete**: "Point-Cloud Completion with Pretrained Text-to-image Diffusion Models", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2306.10533)][[Website](https://sds-complete.github.io/)]
    * **Michelangelo**: "Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2306.17115)][[Code (in construction)(https://github.com/NeuralCarver/michelangelo)]][[Website](https://neuralcarver.github.io/michelangelo/)]
    * **DiffTF**: "Large-Vocabulary 3D Diffusion Model with Transformer", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2309.07920)][[Code (in construction)](https://github.com/ziangcao0312/DiffTF)][[Website](https://ziangcao0312.github.io/difftf_pages/)]
* Others:
    * **DiffGesture**: "Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation", CVPR, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2303.09119)][[PyTorch](https://github.com/Advocate99/DiffGesture)]
    * **CondFoleyGen**: "Conditional Generation of Audio from Video via Foley Analogies", CVPR, 2023 (*UMich*). [[Paper](https://arxiv.org/abs/2304.08490)][[PyTorch (in construction)](https://github.com/XYPB/CondFoleyGen)][[Website](https://xypb.github.io/CondFoleyGen/)]
    * **Physics-Diffusion**: "Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos", CVPR, 2023 (*IBM*). [[Paper](https://arxiv.org/abs/2303.16897)][[PyTorch](https://github.com/sukun1045/video-physics-sound-diffusion)][[Website](https://sukun1045.github.io/video-physics-sound-diffusion/)]
    * **RACER**: "Co-Speech Gesture Synthesis by Reinforcement Learning With Contrastive Pre-Trained Rewards", CVPR, 2023 (*Dalian University of Technology*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Sun_Co-Speech_Gesture_Synthesis_by_Reinforcement_Learning_With_Contrastive_Pre-Trained_Rewards_CVPR_2023_paper.html)][[Code (in construction)](https://github.com/RLracer/RACER)]
    * **ReVISE**: "ReVISE: Self-Supervised Speech Resynthesis with Visual Input for Universal and Generalized Speech Regeneration", CVPR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2212.11377)][[PyTorch](https://github.com/facebookresearch/av_hubert)][[Website](https://wnhsu.github.io/ReVISE/)]
    * **MAV3D**: "Text-To-4D Dynamic Scene Generation", ICML, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2301.11280)][[Website](https://make-a-video3d.github.io/)]
    * **LORIS**: "Long-Term Rhythmic Video Soundtracker", ICML, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2305.01319)][[PyTorch](https://github.com/OpenGVLab/LORIS)]
    * **NExT-GPT**: "NExT-GPT: Any-to-Any Multimodal LLM", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2309.05519)][[Code (in construction)](https://github.com/NExT-GPT/NExT-GPT)][[Website](https://next-gpt.github.io/)]

[[Back to Overview](#overview)]

### Prompt Learning/Tuning:
* **CLIP-Adapter**: "CLIP-Adapter: Better Vision-Language Models with Feature Adapters", arXiv, 2021 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2110.04544)][[PyTorch](https://github.com/gaopengcuhk/CLIP-Adapter)]
* **CoCoOp**: "Conditional Prompt Learning for Vision-Language Models", CVPR, 2022 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2203.05557)][[PyTorch](https://github.com/KaiyangZhou/CoOp)]
* **ProDA**: "Prompt Distribution Learning", CVPR, 2022 (*Huawei*). [[Paper](https://arxiv.org/abs/2205.03340)]
* **VPT**: "Visual Prompt Tuning", ECCV, 2022 (*Cornell*). [[Paper](https://arxiv.org/abs/2203.12119)][[PyTorch](https://github.com/kmnp/vpt)]
* **PerVL**: "This is my unicorn, Fluffy": Personalizing frozen vision-language representations", ECCV, 2022 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2204.01694)][[PyTorch](https://github.com/NVlabs/PALAVRA)]
* **OrdinalCLIP**: "OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression", NeurIPS, 2022 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2206.02338)][[PyTorch](https://github.com/xk-huang/OrdinalCLIP)]
* **BeamCLIP**: "Transferring Pre-trained Multimodal Representations with Cross-modal Similarity Matching", NeurIPS, 2022 (*LG*). [[Paper](https://openreview.net/forum?id=j2Vtg_jhKZ)]
* **TPT**: "Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models", NeurIPS, 2022 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2209.07511)][[PyTorch](https://github.com/azshue/TPT)][[Website](https://azshue.github.io/TPT/)]
* **CoOp**: "Learning to Prompt for Vision-Language Models", IJCV, 2022 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2109.01134)][[PyTorch](https://github.com/KaiyangZhou/CoOp)]
* **LASP**: "Language-Aware Soft Prompting for Vision & Language Foundation Models", CVPR, 2023 (*Samsung*). [[Paper](https://arxiv.org/abs/2210.01115)][[Website](https://www.adrianbulat.com/lasp)]
* **VPT**: "Variational prompt tuning improves generalization of vision-language models", arXiv, 2022 (*Samsung*). [[Paper](https://arxiv.org/abs/2210.02390)]
* **CAVPT**: "Class-Aware Visual Prompt Tuning for Vision-Language Pre-Trained Model", arXiv, 2022 (*Northwestern Polytechnical University, China*). [[Paper](https://arxiv.org/abs/2208.08340)]
* **Visual-Prompting**: "Exploring Visual Prompts for Adapting Large-Scale Models", arXiv, 2022 (*MIT*). [[Paper](https://arxiv.org/abs/2203.17274)][[PyTorch](https://github.com/hjbahng/visual_prompting)][[Website](https://hjbahng.github.io/visual_prompting/)]
* **PGN**: "Prompt Generation Networks for Efficient Adaptation of Frozen Vision Transformers", arXiv, 2022 (*University of Amsterdam*). [[Paper](https://arxiv.org/abs/2210.06466)][[PyTorch](https://github.com/jochemloedeman/PGN)]
* **UPT**: "Unified Vision and Language Prompt Learning", arXiv, 2022 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2210.07225)][[Code (in construction)](https://github.com/yuhangzang/UPT)]
* **CPL**: "CPL: Counterfactual Prompt Learning for Vision and Language Models", arXiv, 2022 (*UC Santa Cruz*). [[Paper](https://arxiv.org/abs/2210.10362)]
* **PTP**: "Prompting through Prototype: A Prototype-based Prompt Learning on Pretrained Vision-Language Models", arXiv, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2210.10841)]
* **MVLPT**: "Multitask Vision-Language Prompt Tuning", arXiv, 2022 (*Berkeley*). [[Paper](https://arxiv.org/abs/2211.11720)][[PyTorch](https://github.com/sIncerass/MVLPT)]
* **?**: "Task Bias in Vision-Language Models", arXiv, 2022 (*Columbia*). [[Paper](https://arxiv.org/abs/2212.04412)]
* **UPL**: "Unsupervised Prompt Learning for Vision-Language Models", arXiv, 2022 (*Peking*). [[Paper](https://arxiv.org/abs/2204.03649)][[PyTorch](https://github.com/tonyhuang2022/UPL)]
* **DeFo**: "Learning to Decompose Visual Features with Latent Textual Prompts", ICLR, 2023 (*UIUC*). [[Paper](https://arxiv.org/abs/2210.04287)]
* **PLOT**: "Prompt Learning with Optimal Transport for Vision-Language Models", ICLR, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2210.01253)]
* **?**: "Visual Classification via Description from Large Language Models", ICLR, 2023 (*Columbia*). [[Paper](https://arxiv.org/abs/2210.07183)]
* **CSP**: "Learning to Compose Soft Prompts for Compositional Zero-Shot Learning", ICLR, 2023 (*Brown University*). [[Paper](https://arxiv.org/abs/2204.03574)][[PyTorch](https://github.com/BatsResearch/csp)]
* **CaFo**: "Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners", CVPR, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2303.02151)][[PyTorch](https://github.com/ZrrSkywalker/CaFo)]
* **?**: "Multimodal Prompting with Missing Modalities for Visual Recognition", CVPR, 2023 (*NYCU*). [[Paper](https://arxiv.org/abs/2303.03369)][[PyTorch](https://github.com/YiLunLee/Missing_aware_prompts)][[Website](https://yilunlee.github.io/missing_aware_prompts/)]
* **DAM-VP**: "Diversity-Aware Meta Visual Prompting", CVPR, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2303.08138)][[PyTorch](https://github.com/shikiw/DAM-VP)]
* **ILM-VP**: "Understanding and Improving Visual Prompting: A Label-Mapping Perspective", CVPR, 2023 (*Michigan State*). [[Paper](https://arxiv.org/abs/2211.11635)][[PyTorch](https://github.com/OPTML-Group/ILM-VP)]
* **KgCoOp**: "Visual-Language Prompt Tuning with Knowledge-guided Context Optimization", CVPR, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2303.13283)][[PyTorch](https://github.com/htyao89/KgCoOp)]
* **BlackVIP**: "BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning", CVPR, 2023 (*University of Seoul*). [[Paper](https://arxiv.org/abs/2303.14773)][[PyTorch](https://github.com/changdaeoh/BlackVIP)]
* **EXPRES**: "Learning Expressive Prompting With Residuals for Vision Transformers", CVPR, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2303.15591)]
* **?**: "Learning to Name Classes for Vision and Language Models", CVPR, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2304.01830)]
* **PMF**: "Efficient Multimodal Fusion via Interactive Prompting", CVPR, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2304.06306)]
* **MaPLe**: "MaPLe: Multi-modal Prompt Learning", CVPR, 2023 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2210.03117)][[PyTorch](https://github.com/muzairkhattak/multimodal-prompt-learning)]
* **HiPro**: "Hierarchical Prompt Learning for Multi-Task Learning", CVPR, 2023 (*JD*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Hierarchical_Prompt_Learning_for_Multi-Task_Learning_CVPR_2023_paper.html)]
* **DFSP**: "Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning", CVPR, 2023 (*The Hong Kong Polytechnic University*). [[Paper](https://arxiv.org/abs/2211.10681)][[PyTorch](https://github.com/Forest-art/DFSP)]
* **TaI-DP**: "Texts as Images in Prompt Tuning for Multi-Label Image Recognition", CVPR, 2023 (*Tomorrow Advancing Life (TAL)*). [[Paper](https://arxiv.org/abs/2211.12739)][[PyTorch](https://github.com/guozix/TaI-DPT)]
* **ESPER**: "Fusing Pre-Trained Language Models With Multimodal Prompts Through Reinforcement Learning", CVPR, 2023 (*Yonsei*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Fusing_Pre-Trained_Language_Models_With_Multimodal_Prompts_Through_Reinforcement_Learning_CVPR_2023_paper.html)][[PyTorch](https://github.com/JiwanChung/esper)]
* **APT**: "A-La-Carte Prompt Tuning (APT): Combining Distinct Data via Composable Prompting", CVPR, 2023 (*Amazon*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Bowman_A-La-Carte_Prompt_Tuning_APT_Combining_Distinct_Data_via_Composable_Prompting_CVPR_2023_paper.html)]
* **VQT**: "Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning", CVPR, 2023 (*The Ohio State University (OSU)*). [[Paper](https://arxiv.org/abs/2212.03220)]
* **LaBo**: "Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification", CVPR, 2023 (*University of Pennsylvania*). [[Paper](https://arxiv.org/abs/2211.11158)][[PyTorch](https://github.com/YueYANG1996/LaBo)]
* **TaskRes**: "Task Residual for Tuning Vision-Language Models", CVPR, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2211.10277)][[PyTorch](https://github.com/geekyutao/TaskRes)]
* **POUF**: "POUF: Prompt-oriented unsupervised fine-tuning for large pre-trained models", ICML, 2023 (*UT Austin*). [[Paper](https://arxiv.org/abs/2305.00350)][[PyTorch](https://github.com/korawat-tanwisuth/POUF)]
* **?**: "Improving Visual Prompt Tuning for Self-supervised Vision Transformers", ICML, 2023 (*SNU*). [[Paper](https://arxiv.org/abs/2306.05067)][[PyTorch](https://github.com/ryongithub/GatedPromptTuning)]
* **ZPE**: "A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models", ICML, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2302.06235)]
* **CMPA**: "Deeply Coupled Cross-Modal Prompt Learning", ACL Findings, 2023 (*SenseTime*). [[Paper](https://arxiv.org/abs/2305.17903)]
* **PromptSRC**: "Self-regulating Prompts: Foundational Model Adaptation without Forgetting", ICCV, 2023 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2307.06948)][[PyTorch](https://github.com/muzairkhattak/PromptSRC)][[Website](https://muzairkhattak.github.io/PromptSRC/)]
* **SHIP**: "Improving Zero-Shot Generalization for CLIP with Synthesized Prompts", ICCV, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2307.07397)]
* **PTNL**: "Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?", ICCV, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2307.11978)]
* **E<sup>2</sup>VPT**: "E<sup>2</sup>VPT: An Effective and Efficient Approach for Visual Prompt Tuning", ICCV, 2023 (*Rochester Institute of Technology, NY*). [[Paper](https://arxiv.org/abs/2307.13770)][[PyTorch](https://github.com/ChengHan111/E2VPT)]
* **R-AMT**: "Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models", ICCV, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2307.15049)][[Code (in construction)](https://github.com/wuw2019/RMT)][[Website](https://wuw2019.github.io/RMT/)]
* **DiffTPT**: "Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning", ICCV, 2023 (*A\*STAR*). [[Paper](https://arxiv.org/abs/2308.06038)][[PyTorch (in construction)](https://github.com/chunmeifeng/DiffTPT)]
* **KAPT**: "Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models", ICCV, 2023 (*Southern University of Science and Technology (SUSTech)*). [[Paper](https://arxiv.org/abs/2308.11186)]
* **RPO**: "Read-only Prompt Optimization for Vision-Language Few-shot Learning", ICCV, 2023 (*Korea University*). [[Paper](https://arxiv.org/abs/2308.14960)][[PyTorch](https://github.com/mlvlab/RPO)]
* **LoGoPrompt**: "LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models", ICCV, 2023 (*ShanghaiTech*). [[Paper](https://arxiv.org/abs/2309.01155)][[Website](https://chengshiest.github.io/logo/)]
* **DAPT**: "Distribution-Aware Prompt Tuning for Vision-Language Models", ICCV, 2023 (*Korea University*). [[Paper](https://arxiv.org/abs/2309.03406)][[Code (in construction)](https://github.com/mlvlab/DAPT)]
* **GOPro**: "GOPro: Generate and Optimize Prompts in CLIP using Self-Supervised Learning", BMVC, 2023 (*IIT Bombay*). [[Paper](https://arxiv.org/abs/2308.11605)][[Code (in construction)](https://github.com/mainaksingha01/GOPro)]
* **ALIGN**: "Tuning Multi-mode Token-level Prompt Alignment across Modalities", NeurIPS, 2023 (*Xidian University*). [[Paper](https://arxiv.org/abs/2309.13847)][[Code (in construction)](https://github.com/wds2014/ALIGN)]
* **GraphAdapter**: "GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph", NeurIPS, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2309.13625)][[Code (in construction)](https://github.com/lixinustc/GraphAdapter)]
* **SeMap**: "From Visual Prompt Learning to Zero-Shot Transfer: Mapping Is All You Need", arXiv, 2023 (*CISPA, Germany*). [[Paper](https://arxiv.org/abs/2303.05266)]
* **R-Tuning**: "R-Tuning: Regularized Prompt Tuning in Open-Set Scenarios", arXiv, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2303.05122)]
* **VPTM**: "Rethinking Visual Prompt Learning as Masked Visual Token Modeling", arXiv, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2303.04998)]
* **GRAM**: "Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models", arXiv, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2303.06571)]
* **PBPrompt**: "Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models", arXiv, 2023 (*Xidian University*). [[Paper](https://arxiv.org/abs/2303.09100)]
* **CTP-TFT**: "Task-Oriented Multi-Modal Mutual Leaning for Vision-Language Models", arXiv, 2023 (*Baidu*). [[Paper](https://arxiv.org/abs/2303.17169)]
* **POMP**: "Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition", arXiv, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2304.04704)][[PyTorch](https://github.com/amazon-science/prompt-pretraining)]
* **?**: "What does CLIP know about a red circle? Visual prompt engineering for VLMs", arXiv, 2023 (*Oxford*). [[Paper](https://arxiv.org/abs/2304.06712)]
* **Robust-ProL**: "Towards Robust Prompts on Vision-Language Models", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2304.08479)]
* **ProVP**: "Progressive Visual Prompt Learning with Contrastive Feature Re-formation", arXiv, 2023 (*vivo, China*). [[Paper](https://arxiv.org/abs/2304.08386)]
* **?**: "Chain of Thought Prompt Tuning in Vision Language Models", arXiv, 2023 (*Peking University*). [[Paper](https://arxiv.org/abs/2304.07919)]
* **Instruction-ViT**: "Instruction-ViT: Multi-Modal Prompts for Instruction Learning in ViT", arXiv, 2023 (*University of Electronic Science and Technology of China*). [[Paper](https://arxiv.org/abs/2305.00201)]
* **VPGTrans**: "Transfer Visual Prompt Generator across LLMs", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2305.01278)][[PyTorch](https://github.com/VPGTrans/VPGTrans)][[Website](https://vpgtrans.github.io/)]
* **DRPT**: "DRPT: Disentangled and Recurrent Prompt Tuning for Compositional Zero-Shot Learning", arXiv, 2023 (*Hong Kong Polytechnic University*). [[Paper](https://arxiv.org/abs/2305.01239)][[Code (in construction)](https://github.com/Forest-art/DRPT-torch)]
* **VCoT**: "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings", arXiv, 2023 (*UCSB*). [[Paper](https://arxiv.org/abs/2305.02317)]
* **PMPO**: "Multi-Prompt with Depth Partitioned Cross-Modal Learning", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2305.06221)]
* **Aurora**: "Mode Approximation Makes Good Vision-Language Prompts", arXiv, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2305.08381)][[PyTorch](https://github.com/WillDreamer/Aurora)]
* **DSD**: "Discriminative Diffusion Models as Few-shot Vision and Language Learners", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2305.10722)]
* **PLID**: "Prompting Language-Informed Distribution for Compositional Zero-Shot Learning", arXiv, 2023 (*Michigan State*). [[Paper](https://arxiv.org/abs/2305.14428)]
* **ConES**: "ConES: Concept Embedding Search for Parameter Efficient Tuning Large Vision Language Models", arXiv, 2023 (*Sichuan University*). [[Paper](https://arxiv.org/abs/2305.18993)]
* **LaFTer**: "LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unlabeled Image Collections", arXiv, 2023 (*TU Graz, Austria*). [[Paper](https://arxiv.org/abs/2305.18287)]
* **?**: "Enhancing CLIP with CLIP: Exploring Pseudolabeling for Limited-Label Prompt Tuning", arXiv, 2023 (*Brown*). [[Paper](https://arxiv.org/abs/2306.01669)][[PyTorch](https://github.com/BatsResearch/menghini-enhanceCLIPwithCLIP-code)]
* **CoPrompt**: "Consistency-guided Prompt Learning for Vision-Language Models", arXiv, 2023 (*Queen’s University, Canada*). [[Paper](https://arxiv.org/abs/2306.01195)]
* **ProTeCt**: "ProTeCt: Prompt Tuning for Hierarchical Consistency", arXiv, 2023 (*UCSD*). [[Paper](https://arxiv.org/abs/2306.02240)]
* **FGVP**: "Fine-Grained Visual Prompting", arXiv, 2023 (*BAAI*). [[Paper](https://arxiv.org/abs/2306.04356)]
* **POP**: "POP: Prompt Of Prompts for Continual Learning", arXiv, 2023 (*Qualcomm*). [[Paper](https://arxiv.org/abs/2306.08200)]
* **GAVIE**: "Aligning Large Multi-Modal Model with Robust Instruction Tuning", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2306.14565)][[PyTorch](https://github.com/FuxiaoLiu/LRV-Instruction)][[Website](https://fuxiaoliu.github.io/LRV/)]
* **NPT**: "Bridging the Gap: Neural Collapse Inspired Prompt Tuning for Generalization under Class Imbalance", arXiv, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2306.15955)]
* **APT**: "Approximated Prompt Tuning for Vision-Language Pre-trained Models", arXiv, 2023 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2306.15706)]
* **CoPL**: "Contextual Prompt Learning for Vision-Language Understanding", arXiv, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2307.00910)]
* **CiP**: "Image Captions are Natural Prompts for Text-to-Image Models", arXiv, 2023 (*The University of Sydney*). [[Paper](https://arxiv.org/abs/2307.08526)]
* **UP-DP**: "UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models", arXiv, 2023 (*Bosch*). [[Paper](https://arxiv.org/abs/2307.11227)]
* **DPL**: "DPL: Decoupled Prompt Learning for Vision-Language Models", arXiv, 2023 (*vivo*). [[Paper](https://arxiv.org/abs/2308.10061)]
* **DuAl-PT**: "Context-Aware Prompt Tuning for Vision-Language Model with Dual-Alignment", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2309.04158)]
* **DePT**: "DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning", arXiv, 2023 (*UCL*). [[Paper](https://arxiv.org/abs/2309.05173)][[PyTorch](https://github.com/ZhengxiangShi/DePT)]
* **Prompting4Debugging**: "Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts", arXiv, 2023 (*NYCU*). [[Paper](https://arxiv.org/abs/2309.06135)]
* **?**: "Language Models as Black-Box Optimizers for Vision-Language Models", arXiv, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2309.05950)]
* **DePT**: "DePT: Decoupled Prompt Tuning", arXiv, 2023 (*University of Electronic Science and Technology of China*). [[Paper](https://arxiv.org/abs/2309.07439)][[PyTorch](https://github.com/Koorye/DePT)]

[[Back to Overview](#overview)]

### Visual Document Understanding
* **LayoutLMv2**: "LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding", ACL, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2012.14740)][[PyTorch](https://github.com/microsoft/unilm/tree/master/layoutlmv2)]
* **DocFormer**: "DocFormer: End-to-End Transformer for Document Understanding", ICCV, 2021 (*Amazon*). [[Paper](https://arxiv.org/abs/2106.11539)]
* **StrucTexT**: "StrucTexT: Structured Text Understanding with Multi-Modal Transformers", ACMMM, 2021 (*Baidu*). [[Paper](https://arxiv.org/abs/2108.02923)][[Paddle](https://github.com/PaddlePaddle/VIMER/tree/main/StrucTexT/v1)]
* **LayoutXLM**: "LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding", arXiv, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2104.08836)][[PyTorch](https://github.com/microsoft/unilm/tree/master/layoutxlm)]
* **TableFormer**: "TableFormer: Table Structure Understanding with Transformers", CVPR, 2022 (*IBM*). [[Paper](https://arxiv.org/abs/2203.01017)]
* **TSRFormer**: "TSRFormer: Table Structure Recognition with Transformers", ACMMM, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2208.04921)]
* **ERNIE-mmLayout**: "ERNIE-mmLayout: Multi-grained MultiModal Transformer for Document Understanding", ACMMM, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2209.08569)]
* **Donut**: "Donut: Document Understanding Transformer without OCR", ECCV, 2022 (*NAVER*). [[Paper](https://arxiv.org/abs/2111.15664)][[PyTorch](https://github.com/clovaai/donut)]
* **I2DFormer**: "I2DFormer: Learning Image to Document Attention for Zero-Shot Image Classification", NeurIPS, 2022 (*ETHZ*). [[Paper](https://arxiv.org/abs/2209.10304)]
* **MGDoc**: "MGDoc: Pre-training with Multi-granular Hierarchy for Document Image Understanding", EMNLP, 2022 (*Adobe*). [[Paper](https://arxiv.org/abs/2211.14958)]
* **DocEnTr**: "DocEnTr: An End-to-End Document Image Enhancement Transformer", arXiv, 2022 (*UAB, Spain*). [[Paper](https://arxiv.org/abs/2201.10252)][[PyTorch](https://github.com/dali92002/DocEnTR)]
* **DocSegTr**: "DocSegTr: An Instance-Level End-to-End Document Image Segmentation Transformer", arXiv, 2022 (*UAB, Spain*). [[Paper](https://arxiv.org/abs/2201.11438)]
* **DiT**: "DiT: Self-supervised Pre-training for Document Image Transformer", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2203.02378)][[Code (in construction)](https://github.com/microsoft/unilm/tree/master/dit)]
* **LayoutLMv3**: "LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2204.08387)][[PyTorch](https://github.com/microsoft/unilm/tree/master/layoutlmv3)]
* **MATrIX**: "MATrIX - Modality-Aware Transformer for Information eXtraction", arXiv, 2022 (*Amazon*). [[Paper](https://arxiv.org/abs/2205.08094)]
* **VLCDoC**: "VLCDoC: Vision-Language Contrastive Pre-Training Model for Cross-Modal Document Classification", arXiv, 2022 (*La Rochelle University, France*). [[Paper](https://arxiv.org/abs/2205.12029)]
* **Bi-VLDoc**: "Bi-VLDoc: Bidirectional Vision-Language Modeling for Visually-Rich Document Understanding", arXiv, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2206.13155)]
* **TRUST**: "TRUST: An Accurate and End-to-End Table structure Recognizer Using Splitting-based Transformers", arXiv, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2208.14687)]
* **Hi-VT5**: "Hierarchical multimodal transformers for Multi-Page DocVQA", arXiv, 2022 (*UAB, Spain*). [[Paper](https://arxiv.org/abs/2212.05935)]
* **OCR-VQGAN**: "OCR-VQGAN: Taming Text-within-Image Generation", WACV, 2023 (*UAB, Spain*). [[Paper](https://arxiv.org/abs/2210.11248)]
* **PIXEL**: "Language Modelling with Pixels", ICLR, 2023 (*University of Copenhagen, Denmark*). [[Paper](https://openreview.net/forum?id=FkSp8VW8RjH)]
* **Spotlight**: "Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus", ICLR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2209.14927)]
* **MaskDoc**: "Masked Visual-Textual Prediction for Document Image Representation Pretraining", ICLR, 2023 (*Baidu*). [[Paper](https://openreview.net/forum?id=HE_75XY5Ljh)]
* **StrucTexTv2**: "StrucTexTv2: Masked Visual-Textual Prediction for Document Image Pre-training", ICLR, 2023 (*Baidu*). [[Paper](https://arxiv.org/abs/2303.00289)][[Paddle](https://github.com/PaddlePaddle/VIMER/tree/main/StrucTexT/v2)]
* **FlexDM**: "Towards Flexible Multi-modal Document Models", CVPR, 2023 (*CyberAgent, Japan*). [[Paper](https://arxiv.org/abs/2303.18248)][[Tensorflow](https://github.com/CyberAgentAILab/flex-dm)][[Website](https://cyberagentailab.github.io/flex-dm/)]
* **MUI**: "Mobile User Interface Element Detection Via Adaptively Prompt Tuning", CVPR, 2023 (*Ant Group*). [[Paper](https://arxiv.org/abs/2305.09699)][[GitHub (in construction)](https://github.com/antmachineintelligence/MUI-zh)]
* **UDOP**: "Unifying Vision, Text, and Layout for Universal Document Processing", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2212.02623)][[PyTorch](https://github.com/microsoft/i-Code/tree/main/i-Code-Doc)]
* **M<sup>6</sup>Doc**: "M<sup>6</sup>Doc: A Large-Scale Multi-Format, Multi-Type, Multi-Layout, Multi-Language, Multi-Annotation Category Dataset for Modern Document Layout Analysis", CVPR, 2023 (*South China University of Technology*). [[Paper](https://arxiv.org/abs/2305.08719)][[GitHub](https://github.com/HCIILAB/M6Doc)]
* **VGT**: "Vision Grid Transformer for Document Layout Analysis", ICCV, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2308.14978)][[PyTorch](https://github.com/AlibabaResearch/AdvancedLiterateMachinery)]
* **SeRum**: "Attention Where It Matters: Rethinking Visual Document Understanding with Selective Region Concentration", ICCV, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2309.01131)]
* **FormNetV2**: "FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction", ACL, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2305.02549)]
* **mmc4**: "Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text", arXiv, 2023 (*AI2*). [[Paper](https://arxiv.org/abs/2304.06939)][[GitHub (in construction)](https://github.com/allenai/mmc4)]
* **DUBLIN**: "DUBLIN - Document Understanding By Language-Image Network", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2305.14218)]
* **DocFormerv2**: "DocFormerv2: Local Features for Document Understanding", arXiv, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2306.01733)]
* **DocumentCLIP**: "DocumentCLIP: Linking Figures and Main Body Text in Reflowed Documents", arXiv, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2306.06306)][[PyTorch](https://github.com/FuxiaoLiu/DocumentCLIP)]
* **DocTr**: "DocTr: Document Transformer for Structured Information Extraction in Documents", arXiv, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2307.07929)]
* **Kosmos-2.5**: "Kosmos-2.5: A Multimodal Literate Model", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2309.11419)]

[[Back to Overview](#overview)]

### Other Multi-Modal Tasks
* Transfer Learning/Adaptation/Distillation:
    * **FLYP**: "Finetune like you pretrain: Improved finetuning of zero-shot vision models", CVPR, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2212.00638)][[PyTorch](https://github.com/locuslab/FLYP)]
    * **Pi-Tuning**: "Pi-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation", ICML, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2304.14381)][[Code (in construction)](https://github.com/TencentARC/pi-Tuning)]
    * **OCRA**: "Cross-Modal Fine-Tuning: Align then Refine", ICML, 2023 (*CMU + HP*). [[Paper](https://arxiv.org/abs/2302.05738)][[PyTorch](https://github.com/sjunhongshen/ORCA)]
    * **TeS**: "Improved Visual Fine-tuning with Natural Language Supervision", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2304.01489)]
    * **Paxion**: "Paxion: Patching Action Knowledge in Video-Language Foundation Models", arXiv, 2023 (*UIUC*). [[Paper](https://arxiv.org/abs/2305.10683)][[PyTorch](https://github.com/MikeWangWZHL/Paxion)]
    * **RLCF**: "Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models", arXiv, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2305.18010)][[Code (in construction)](https://github.com/mzhaoshuai/RLCF)]
    * **LMAT**: "Can Large Pre-trained Models Help Vision Models on Perception Tasks?", arXiv, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2306.00693)][[Website (in construction)](https://dingning97.github.io/imagenet-descriptions/)]
    * **TaCA**: "TaCA: Upgrading Your Visual Foundation Model with Task-agnostic Compatible Adapter", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2306.12642)][[Code (in construction)](https://github.com/TencentARC/TaCA)]
    * **ProbVLM**: "ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models", arXiv, 2023 (*University of Tubingen, Germany*). [[Paper](https://arxiv.org/abs/2307.00398)]
    * **CLIP-KD**: "CLIP-KD: An Empirical Study of Distilling CLIP Models", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2307.12732)][[Code (in construction)](https://github.com/winycg/CLIP-KD)]
* Zero-Shot:
    * **CuPL**: "What does a platypus look like? Generating customized prompts for zero-shot image classification", arXiv, 2022 (*UW*). [[Paper](https://arxiv.org/abs/2209.03320)][[PyTorch](https://github.com/sarahpratt/CuPL)]
    * **SMs**: "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language", ICLR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2204.00598)][[GitHub](https://github.com/google-research/google-research/tree/master/socraticmodels)][[Website](https://socraticmodels.github.io/)]
    * **iCLIP**: "iCLIP: Bridging Image Classification and Contrastive Language-Image Pre-Training for Visual Recognition", CVPR, 2023 (*Microsoft*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Wei_iCLIP_Bridging_Image_Classification_and_Contrastive_Language-Image_Pre-Training_for_Visual_CVPR_2023_paper.html)]
    * **DiffDis**: "DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability", ICCV, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2308.09306)]
    * **V-GLOSS**: "Visually-Grounded Descriptions Improve Zero-Shot Image Classification", arXiv, 2023 (*University of Alberta, Canada*). [[Paper](https://arxiv.org/abs/2306.06077)]
    * **?**: "Challenges of Zero-Shot Recognition with Vision-Language Models: Granularity and Correctness", arXiv, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2306.16048)]
    * **UniFine**: "UniFine: A Unified and Fine-grained Approach for Zero-shot Vision-Language Understanding", arXiv, 2023 (*Columbia*). [[Paper](https://arxiv.org/abs/2307.00862)][[Code (in construction)](https://github.com/ThreeSR/UniFine)]
    * **Cheetah**: "Empowering Vision-Language Models to Follow Interleaved Vision-Language Instructions", arXiv, 2023 (*Zhejiang*). [[Paper](https://arxiv.org/abs/2308.04152)]
    * **PerceptionCLIP**: "More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes", arXiv, 2023 (*University of Maryland*). [[Paper](https://arxiv.org/abs/2308.01313)][[Github](https://github.com/umd-huang-lab/perceptionCLIP)]
* X-Shot:
    * **Tip-Adapter**: "Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification", ECCV, 2022 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2207.09519)][[PyTorch](https://github.com/gaopengcuhk/Tip-Adapter)]
    * **VidIL**: "Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners", NeurIPS, 2022 (*UIUC*). [[Paper](https://arxiv.org/abs/2205.10747)][[PyTorch](https://github.com/MikeWangWZHL/VidIL)]
    * **ComCLIP**: "ComCLIP: Training-Free Compositional Image and Text Matching", arXiv, 2022 (*UC Santa Cruz*). [[Paper](https://arxiv.org/abs/2211.13854)]
    * **TCT**: "Efficient Zero-shot Visual Search via Target and Context-aware Transformer", arXiv, 2022 (*Baylor College of Medicine, TX*). [[Paper](https://arxiv.org/abs/2211.13470)]
    * **?**: "Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning", ICLR, 2023 (*University of Amsterdam*). [[Paper](https://arxiv.org/abs/2302.14794)]
    * **?**: "Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models", CVPR, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2301.06267)]
    * **SADA**: "Few-Shot Learning with Visual Distribution Calibration and Cross-Modal Distribution Alignment", CVPR, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2305.11439)][[PyTorch](https://github.com/bhrqw/SADA)]
    * **APE**: "Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2304.01195)][[PyTorch](https://github.com/yangyangyang127/APE)]
    * **LFA**: "Black Box Few-Shot Adaptation for Vision-Language models", arXiv, 2023 (*Samsung*). [[Paper](https://arxiv.org/abs/2304.01752)]
    * **?**: "Making the Most of What You Have: Adapting Pre-trained Visual Language Models in the Low-data Regime", arXiv, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2305.02297)]
    * **Proto-CLIP**: "Proto-CLIP: Vision-Language Prototypical Network for Few-Shot Learning", arXiv, 2023 (*UT Dallas*). [[Paper](https://arxiv.org/abs/2307.03073)]
    * **NtUA**: "Noise-Tolerant Unsupervised Adapter for Vision-Language Models", arXiv, 2023 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2309.14928)]
* Referring Image Segmentation:
    * **VLT**: "Vision-Language Transformer and Query Generation for Referring Segmentation", ICCV, 2021 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2108.05565)][[Tensorflow](https://github.com/henghuiding/Vision-Language-Transformer)]
    * **CRIS**: "CRIS: CLIP-Driven Referring Image Segmentation", CVPR, 2022 (*University of Sydney*). [[Paper](https://arxiv.org/abs/2111.15174)]
    * **LAVT**: "LAVT: Language-Aware Vision Transformer for Referring Image Segmentation", CVPR, 2022 (*Oxford*). [[Paper](https://arxiv.org/abs/2112.02244)]
    * **ReSTR**: "ReSTR: Convolution-free Referring Image Segmentation Using Transformers", CVPR, 2022 (*POSTECH*). [[Paper](https://arxiv.org/abs/2203.16768)][[Website](http://cvlab.postech.ac.kr/research/restr/)]
    * **ReCLIP**: "ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension", ACL, 2022 (*AI2*). [[Paper](https://arxiv.org/abs/2204.05991)]
    * **TSEG**: "Weakly-supervised segmentation of referring expressions", arXiv, 2022 (*INRIA*). [[Paper](https://arxiv.org/abs/2205.04725)]
    * **ZS-RIS**: "Zero-shot Referring Image Segmentation with Global-Local Context Features", CVPR, 2023 (*Gwangju Institute of Science and Technology (GIST)*). [[Paper](https://arxiv.org/abs/2303.17811)][[PyTorch](https://github.com/Seonghoon-Yu/Zero-shot-RIS)]
    * **PolyFormer**: "PolyFormer: Referring Image Segmentation as Sequential Polygon Generation", CVPR, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2302.07387)][[Website](https://polyformer.github.io/)]
    * **MCRES**: "Meta Compositional Referring Expression Segmentation", CVPR, 2023 (*Singapore University of Technology and Design*). [[Paper](https://arxiv.org/abs/2304.04415)]
    * **ReLA**: "GRES: Generalized Referring Expression Segmentation", CVPR, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2306.00968)][[PyTorch](https://github.com/henghuiding/ReLA)][[Website](https://henghuiding.github.io/GRES/)]
    * **CGFormer**: "Contrastive Grouping With Transformer for Referring Image Segmentation", CVPR, 2023 (*ShanghaiTech*). [[Paper](https://arxiv.org/abs/2309.01017)][[PyTorch](https://github.com/Toneyaya/CGFormer)]
    * **CCTF**: "Learning To Segment Every Referring Object Point by Point", CVPR, 2023 (*JD*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Qu_Learning_To_Segment_Every_Referring_Object_Point_by_Point_CVPR_2023_paper.html)][[Code (in construction)](https://github.com/qumengxue/Partial-RES)]
    * **ETRIS**: "Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation", ICCV, 2023 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2307.11545)][[PyTorch](https://github.com/kkakkkka/ETRIS)]
    * **DMMI**: "Beyond One-to-One: Rethinking the Referring Image Segmentation", ICCV, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2308.13853)][[Code (in construction)](https://github.com/toggle1995/RIS-DMMI)]
    * **TRIS**: "Referring Image Segmentation Using Text Supervision", ICCV, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2308.14575)][[Code (in construction)](https://github.com/fawnliu/TRIS)]
    * **SnG**: "Shatter and Gather: Learning Referring Image Segmentation with Text Supervision", ICCV, 2023 (*POSTECH*). [[Paper](https://arxiv.org/abs/2308.15512)]
    * **VLT**: "VLT: Vision-Language Transformer and Query Generation for Referring Segmentation", TPAMI, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2210.15871)]
    * **IREG**: "Whether you can locate or not? Interactive Referring Expression Generation", arXiv, 2023 (*Beijing University of Posts and Telecommunications*). [[Paper](https://arxiv.org/abs/2308.09977)][[Code (in construction)](https://github.com/superhero-7/IREG)]
    * **R-RIS**: "Towards Robust Referring Image Segmentation", arXiv, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2209.09554)][[Code (in construction)](https://github.com/jzwu48033552/robust-ref-seg)][[Website](https://lxtgh.github.io/project/robust_ref_seg/)]
    * **PVD**: "Parallel Vertex Diffusion for Unified Visual Grounding", arXiv, 2023 (*Peking University*). [[Paper](https://arxiv.org/abs/2303.07216)]
    * **MMNet**: "MMNet: Multi-Mask Network for Referring Image Segmentation", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2305.14969)]
    * **LGFormer**: "Linguistic Query-Guided Mask Generation for Referring Image Segmentation", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2301.06429)]
    * **RISCLIP**: "RISCLIP: Referring Image Segmentation Framework using CLIP", arXiv, 2023 (*POSTECH*). [[Paper](https://arxiv.org/abs/2306.08498)]
    * **EAVL**: "EAVL: Explicitly Align Vision and Language for Referring Image Segmentation", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2308.09779)]
    * **Ref-Diff**: "Ref-Diff: Zero-shot Referring Image Segmentation with Generative Models", arXiv, 2023 (*Harbin Institute of Technology*). [[Paper](https://arxiv.org/abs/2308.16777)][[Code (in construction)](https://github.com/kodenii/Ref-Diff)]
* Referring Video Segmentation:
    * **ReferFormer**: "Language as Queries for Referring Video Object Segmentation", CVPR, 2022 (*HKU*). [[Paper](https://arxiv.org/abs/2201.00487)][[PyTorch](https://github.com/wjn922/ReferFormer)]
    * **MTTR**: "End-to-End Referring Video Object Segmentation with Multimodal Transformers", CVPR, 2022 (*Technion - Israel Institute of Technology*). [[Paper](https://arxiv.org/abs/2111.14821)][[PyTorch](https://github.com/mttr2021/MTTR)]
    * **MANet**: "Multi-Attention Network for Compressed Video Referring Object Segmentation", ACMMM, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2207.12622)][[PyTorch](https://github.com/DexiangHong/MANet)]
    * **R<sup>2</sup>VOS**: "R<sup>2</sup>VOS: Robust Referring Video Object Segmentation via Relational Multimodal Cycle Consistency", arXiv, 2022 (*CMU*). [[Paper](https://arxiv.org/abs/2207.01203)][[PyTorch](https://github.com/lxa9867/R2VOS)][[Website](https://lxa9867.github.io/works/rrvos/)]
    * **OnlineRefer**: "OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation", ICCV, 2023 (*Megvii*). [[Paper](https://arxiv.org/abs/2307.09356)][[PyTorch](https://github.com/wudongming97/OnlineRefer)]
    * **SgMg**: "Spectrum-guided Multi-granularity Referring Video Object Segmentation", ICCV, 2023 (*The University of Western Australia*). [[Paper](https://arxiv.org/abs/2307.13537)][[PyTorch](https://github.com/bo-miao/SgMg)]
    * **MeViS**: "MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions", ICCV, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2308.08544)][[PyTorch](https://github.com/henghuiding/MeViS)][[Website](https://henghuiding.github.io/MeViS/)]
    * **CMA**: "Learning Cross-Modal Affinity for Referring Video Object Segmentation Targeting Limited Samples", ICCV, 2023 (*SUSTech*). [[Paper](https://arxiv.org/abs/2309.02041)][[PyTorch](https://github.com/hengliusky/Few_shot_RVOS)]
    * **TempCD**: "Temporal Collection and Distribution for Referring Video Object Segmentation", ICCV, 2023 (*ShanghaiTech*). [[Paper](https://arxiv.org/abs/2309.03473)][[Website](https://toneyaya.github.io/tempcd/)]
    * **Locater**: "Local-Global Context Aware Transformer for Language-Guided Video Segmentation", TPAMI, 2023 (*Zhejiang*). [[Paper](https://arxiv.org/abs/2203.09773)][[PyTorch](https://github.com/leonnnop/Locater)]
    * **LoSh**: "LoSh: Long-Short Text Joint Prediction Network for Referring Video Object Segmentation", arXiv, 2023 (*King’s College London*). [[Paper](https://arxiv.org/abs/2306.08736)]
    * **SOC**: "SOC: Semantic-Assisted Object Cluster for Referring Video Object Segmentation", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2305.17011)]
    * **RefSAM**: "RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation", arXiv, 2023 (*National University of Defense Technology, China*). [[Paper](https://arxiv.org/abs/2307.00997)][[Code (in construction)](https://github.com/LancasterLi/RefSAM)]
    * **IFIRVOS**: "Referring Video Object Segmentation with Inter-Frame Interaction and Cross-Modal Correlation", arXiv, 2023 (*Wuhan University*). [[Paper](https://arxiv.org/abs/2307.00536)]
    * **LGCFS**: "Learning Referring Video Object Segmentation from Weak Annotation", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2308.02162)]
    * **EPCFormer**: "EPCFormer: Expression Prompt Collaboration Transformer for Universal Referring Video Object Segmentation", arXiv, 2023 (*Hunan University*). [[Paper](https://arxiv.org/abs/2308.04162)][[Code (in construction)](https://github.com/lab206/EPCFormer)]
    * **FTEA**: "Fully Transformer-Equipped Architecture for End-to-End Referring Video Object Segmentation", arXiv, 2023 (*Hangzhou Dianzi University*). [[Paper](https://arxiv.org/abs/2309.11933)]
* Referring 3D Segmentation:
    * **3D-STMN**: "3D-STMN: Dependency-Driven Superpoint-Text Matching Network for End-to-End 3D Referring Expression Segmentation", arXiv, 2023 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2308.16632)][[PyTorch](https://github.com/sosppxo/3D-STMN)]
* Tracking:
    * **ModaMixer**: "Divert More Attention to Vision-Language Tracking", NeurIPS, 2022 (*Beijing Jiaotong University*). [[Paper](https://arxiv.org/abs/2207.01076)][[PyTorch](https://github.com/JudasDie/SOTS)]
    * **TransRMOT**: "Referring Multi-Object Tracking", CVPR, 2023 (*Megvii*). [[Paper](https://arxiv.org/abs/2303.03366)][[PyTorch](https://github.com/wudongming97/RMOT)][[Website](https://referringmot.github.io/)]
    * **ModaMixer**: "Divert More Attention to Vision-Language Object Tracking", arXiv, 2023 (*Beijing Jiaotong University*). [[Paper](https://arxiv.org/abs/2307.10046)][[PyTorch](https://github.com/JudasDie/SOTS)]
* Analysis:
    * **MM-Explainability**: "Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers", ICCV, 2021 (*Tel Aviv*). [[Paper](https://arxiv.org/abs/2103.15679)][[PyTorch](https://github.com/hila-chefer/Transformer-MM-Explainability)]
    * **?**: "Are Multimodal Transformers Robust to Missing Modality?", CVPR, 2022 (*University of Delaware*). [[Paper](https://arxiv.org/abs/2204.05454)]
    * **VL-InterpreT**: "VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers", CVPR (demo), 2022 (*Intel*). [[Paper](https://arxiv.org/abs/2203.17247)][[Website](http://vlinterpretenv4env-env.eba-vmhhefup.us-east-2.elasticbeanstalk.com/)][[Video](https://www.youtube.com/watch?v=2HZ2IjzG5_4&ab_channel=MengDu)]
    * **?**: "Understanding Attention for Vision-and-Language Tasks", International Conference on Computational Linguistics (COLING), 2022 (*The University of Sydney*). [[Paper](https://arxiv.org/abs/2208.08104)]
    * **VL-CheckList**: "VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations", arXiv, 2022 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2207.00221)][[Code (in construction)](https://github.com/om-ai-lab/VL-CheckList)]
    * **?**: "Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding", CVPR, 2023 (*Tel Aviv*). [[Paper](https://arxiv.org/abs/2303.12513)][[PyTorch](https://github.com/TAU-VAILab/isbertblind)][[Website](https://tau-vailab.github.io/isbertblind/)]
    * **Why-Prompt**: "Doubly Right Object Recognition: A Why Prompt for Visual Rationales", CVPR, 2023 (*Columbia*). [[Paper](https://arxiv.org/abs/2212.06202)]
    * **CREPE**: "CREPE: Can Vision-Language Foundation Models Reason Compositionally?", CVPR, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2212.07796)]
    * **ZOOM**: "Zero-shot Model Diagnosis", CVPR, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2303.15441)]
    * **?**: "On the Generalization of Multi-modal Contrastive Learning", ICML, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2306.04272)][[PyTorch](https://github.com/PKU-ML/CLIP-Help-SimCLR)]
    * **?**: "Learning Concise and Descriptive Attributes for Visual Recognition", ICCV, 2023 (*UCSD*). [[Paper](https://arxiv.org/abs/2308.03685)]
* Speaker Localization:
    * **?**: "The Right to Talk: An Audio-Visual Transformer Approach", ICCV, 2021 (*University of Arkansas*). [[Paper](https://arxiv.org/abs/2108.03256)]
* Multi-task:
    * **UniT**: "Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer", ICCV, 2021 (*Facebook*). [[Paper](https://arxiv.org/abs/2102.10772)][[PyTorch](https://github.com/facebookresearch/mmf)][[Website](https://mmf.sh/)]
    * **Pix2Seq**: "A Unified Sequence Interface for Vision Tasks", NeurIPS, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2206.07669)]
    * **LAVIS**: "LAVIS: A Library for Language-Vision Intelligence", arXiv, 2022 (*Salesforce*). [[Paper](https://arxiv.org/abs/2209.09019)][[PyTorch](https://github.com/salesforce/LAVIS)]
    * **Unified-IO**: "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks", ICLR, 2023 (*AI2*). [[Paper](https://arxiv.org/abs/2206.08916)][[JAX](https://github.com/allenai/unified-io-inference)][[Website](https://unified-io.allenai.org/)]
    * **ImageBind**: "ImageBind: One Embedding Space To Bind Them All", CVPR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2305.05665)][[PyTorch](https://github.com/facebookresearch/ImageBind)][[Website](https://imagebind.metademolab.com/)]
    * **EgoT2**: "Egocentric Video Task Translation", CVPR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2212.06301)][[Website](https://vision.cs.utexas.edu/projects/egot2/)]
    * **VTAGML**: "Vision Transformer Adapters for Generalizable Multitask Learning", ICCV, 2023 (*EPFL*). [[Paper](https://arxiv.org/abs/2308.12372)][[Website](https://ivrl.github.io/VTAGML/)]
    * **CoCoCon**: "Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models", arXiv, 2023 (*AI2*). [[Paper](https://arxiv.org/abs/2303.16133)][[PyTorch](https://github.com/adymaharana/cococon)][[Website](https://adymaharana.github.io/cococon/)]
    * **VisionLLM**: "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2305.11175)][[Code (in construction)](https://github.com/OpenGVLab/VisionLLM)]
    * **ONE-PEACE**: "ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2305.11172)][[PyTorch (in construction)](https://github.com/OFA-Sys/ONE-PEACE)]
    * **VideoLLM**: "VideoLLM: Modeling Video Sequence with Large Language Models", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2305.13292)][[Code (in construction)](https://github.com/cg1177/VideoLLM)]
    * **i-Code-Studio**: "i-Code Studio: A Configurable and Composable Framework for Integrative AI", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2305.13738)][[Code (in construction)](https://github.com/microsoft/i-Code/tree/main/i-Code-Studio)][[Website](https://i-code-studio.github.io/)]
    * **Tag2Text**: "Tag2Text: Guiding Vision-Language Model via Image Tagging", arXiv, 2023 (*OPPO*). [[Paper](https://arxiv.org/abs/2303.05657)][[PyTorch](https://github.com/xinyu1205/Tag2Text)][[Website](https://tag2text.github.io/)]
    * **RAM**: "Recognize Anything: A Strong Image Tagging Model", arXiv, 2023 (*OPPO*). [[Paper](https://arxiv.org/abs/2306.03514)][[PyTorch](https://github.com/xinyu1205/Tag2Text-Recognize_Anything)][[Website](https://recognize-anything.github.io/)]
    * **InstructDiffusion**: "InstructDiffusion: A Generalist Modeling Interface for Vision Tasks", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2309.03895)][[PyTorch](https://github.com/cientgu/InstructDiffusion)][[Website](https://gengzigang.github.io/instructdiffusion.github.io/)]
* Language-based Video Editing:
    * **M<sup>3</sup>L**: "Language-based Video Editing via Multi-Modal Multi-Level Transformer", CVPR, 2022 (*UCSB*). [[Paper](https://arxiv.org/abs/2104.01122)]
    * **Video-P2P**: "Video-P2P: Video Editing with Cross-attention Control", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2303.04761)][[Website](https://video-p2p.github.io/)]
    * **FateZero**: "FateZero: Fusing Attentions for Zero-shot Text-based Video Editing", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2303.09535)][[PyTorch](https://github.com/ChenyangQiQi/FateZero)][[Website](https://fate-zero-edit.github.io/)]
    * **Make-A-Protagonist**: "Make-A-Protagonist: Generic Video Editing with An Ensemble of Experts", arXiv, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2305.08850)][[PyTorch](https://github.com/Make-A-Protagonist/Make-A-Protagonist)][[Website](https://make-a-protagonist.github.io/)]
* Video Summarization:
    * **GPT2MVS**: "GPT2MVS: Generative Pre-trained Transformer-2 for Multi-modal Video Summarization", ICMR, 2021 (*BBC*). [[Paper](https://arxiv.org/abs/2104.12465)]
    * **QVHighlights**: "QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries", NeurIPS, 2021 (*UNC*). [[Paper](https://arxiv.org/abs/2107.09609)][[PyTorch](https://github.com/jayleicn/moment_detr)]
    * **HMT**: "Hierarchical Multimodal Transformer to Summarize Videos", arXiv, 2021 (*Xidian University*). [[Paper](https://arxiv.org/abs/2109.10559)]
    * **?**: "Show Me What I Like: Detecting User-Specific Video Highlights Using Content-Based Multi-Head Attention", ACMMM, 2022 (*Adobe*). [[Paper](https://arxiv.org/abs/2207.08352)]
    * **IV-Sum**: "TL;DW? Summarizing Instructional Videos with Task Relevance & Cross-Modal Saliency", ECCV, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2208.06773)][[Website](https://medhini.github.io/ivsum/)]
    * **A2Summ**: "Align and Attend: Multimodal Summarization with Dual Contrastive Losses", CVPR, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2303.07284)][[Code (in construction)](https://github.com/boheumd/A2Summ)][[Website](https://boheumd.github.io/A2Summ/)]
    * **QD-DETR**: "Query-Dependent Video Representation for Moment Retrieval and Highlight Detection", CVPR, 2023 (*Sungkyunkwan University, Korea*). [[Paper](https://arxiv.org/abs/2303.13874)][[PyTorch](https://github.com/wjun0830/QD-DETR)]
    * **A2Summ**: "Align and Attend: Multimodal Summarization with Dual Contrastive Losses", CVPR, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2303.07284)][[PyTorch](https://github.com/boheumd/A2Summ)][[Website](https://boheumd.github.io/A2Summ/)]
    * **CLC**: "Collaborative Noisy Label Cleaner: Learning Scene-aware Trailers for Multi-modal Highlight Detection in Movies", CVPR, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2303.14768)][[Code (in construction)](https://github.com/TencentYoutuResearch/HighlightDetection-CLC)]
    * **VideoXum**: "VideoXum: Cross-modal Visual and Textural Summarization of Videos", arXiv, 2023 (*OPPO*). [[Paper](https://arxiv.org/abs/2303.12060)][[Website](https://videoxum.github.io/)]
    * **MH-DETR**: "MH-DETR: Video Moment and Highlight Detection with Cross-modal Transformer", arXiv, 2023 (*Nanjing University*). [[Paper](https://arxiv.org/abs/2305.00355)]
    * **VisionaryVid**: "Joint Moment Retrieval and Highlight Detection Via Natural Language Queries", arXiv, 2023 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2305.04961)][[PyTorch](https://github.com/Skyline-9/Visionary-Vids)]
* Robotics:
    * **CRT**: "Case Relation Transformer: A Crossmodal Language Generation Model for Fetching Instructions", IROS, 2021 (*Keio University*). [[Paper](https://arxiv.org/abs/2107.00789)]
    * **TraSeTR**: "TraSeTR: Track-to-Segment Transformer with Contrastive Query for Instance-level Instrument Segmentation in Robotic Surgery", ICRA, 2022 (*CUHK*). [[Paper](https://arxiv.org/abs/2202.08453)]
    * **VLMbench**: "VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation", NeurIPS (Datasets and Benchmarks), 2022 (*UC Santa Cruz*). [[Paper](https://arxiv.org/abs/2206.08522)][[Pytorch](https://github.com/eric-ai-lab/vlmbench)][[Website](https://sites.google.com/ucsc.edu/vlmbench/home)]
    * **Surgical-VQLA**: "Surgical-VQLA: Transformer with Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery", ICRA, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2305.11692)][[PyTorch](https://github.com/longbai1006/Surgical-VQLA)]
    * **?**: "Distilling Internet-Scale Vision-Language Models into Embodied Agents", ICML, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2301.12507)]
    * **LIV**: "LIV: Language-Image Representations and Rewards for Robotic Control", ICML, 2023 (*UPenn*). [[Paper](https://arxiv.org/abs/2306.00958)][[PyTorch](https://github.com/penn-pal-lab/LIV)][[Website](https://penn-pal-lab.github.io/LIV/)]
    * **PaLM-E**: "PaLM-E: An Embodied Multimodal Language Model", ICML, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.03378)][[Website](https://palm-e.github.io/)]
    * **VIMA**: "VIMA: General Robot Manipulation with Multimodal Prompts", ICML, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2210.03094)][[PyTorch](https://github.com/vimalabs/VIMA)][[Website](https://vimalabs.github.io/)]
    * **GVCCI**: "GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic Manipulation", IROS, 2023 (*SNU, Korea*). [[Paper](https://arxiv.org/abs/2307.05963)]
    * **LACO**: "Language-Conditioned Path Planning", CoRL, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2308.16893)][[Code (in construction)](https://github.com/amberxie88/lapp)][[Website](https://amberxie88.github.io/lapp/)]
    * **Grounded-Decoding**: "Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.00855)][[Website](https://grounded-decoding.github.io/)]
    * **MOO**: "Open-World Object Manipulation using Pre-trained Vision-Language Models", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.00905)][[Website](https://robot-moo.github.io/)]
    * **?**: "Vision-Language Models as Success Detectors", arXiv, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2303.07280)]
    * **VC-1**: "Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2303.18240)][[Website](https://eai-vc.github.io/)]
    * **HomeRobot**: "HomeRobot: Open-Vocabulary Mobile Manipulation", arXiv, 2023 (*Georgia Tech + Meta*). [[Paper](https://arxiv.org/abs/2306.11565)][[PyTorch](https://github.com/facebookresearch/home-robot)][[Website](https://ovmm.github.io/)]
    * **TaPA**: "Embodied Task Planning with Large Language Models", arXiv, 2023 (*Beijing University of Posts and Telecommunications*). [[Paper](https://arxiv.org/abs/2307.01848)][[PyTorch](https://github.com/Gary3410/TaPA)][[Website](https://gary3410.github.io/TaPA/)]
    * **VoxPoser**: "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models", arXiv, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2307.05973)][[Website](https://voxposer.github.io/)]
    * **RT-2**: "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control", arXiv, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2307.15818)][[Website](https://robotics-transformer2.github.io/)]
* Multi-modal Fusion:
    * **MICA**: "Attention Is Not Enough: Mitigating the Distribution Discrepancy in Asynchronous Multimodal Sequence Fusion", ICCV, 2021 (*Southwest Jiaotong University*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Liang_Attention_Is_Not_Enough_Mitigating_the_Distribution_Discrepancy_in_Asynchronous_ICCV_2021_paper.html)]
    * **IFT**: "Image Fusion Transformer", arXiv, 2021 (*Johns Hopkins*). [[Paper](https://arxiv.org/abs/2107.09011)][[PyTorch](https://github.com/Vibashan/Image-Fusion-Transformer)]
    * **PPT**: "PPT Fusion: Pyramid Patch Transformer for a Case Study in Image Fusion", arXiv, 2021 (*?*). [[Paper](https://arxiv.org/abs/2107.13967)]
    * **TransFuse**: "TransFuse: A Unified Transformer-based Image Fusion Framework using Self-supervised Learning", arXiv, 2022 (*Fudan University*). [[Paper](https://arxiv.org/abs/2201.07451)]
    * **SwinFuse**: "SwinFuse: A Residual Swin Transformer Fusion Network for Infrared and Visible Images", arXiv, 2022 (*Taiyuan University of Science and Technology*). [[Paper](https://arxiv.org/abs/2204.11436)]
    * **?**: "Array Camera Image Fusion using Physics-Aware Transformers", arXiv, 2022 (*University of Arizona*). [[Paper](https://arxiv.org/abs/2207.02250)]
    * **CDDFuse**: "CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion", CVPR, 2023 (*ETHZ*). [[Paper](https://arxiv.org/abs/2211.14461)][[PyTorch](https://github.com/Zhaozixiang1228/MMIF-CDDFuse)]
* Human Interaction:
    * **Dyadformer**: "Dyadformer: A Multi-modal Transformer for Long-Range Modeling of Dyadic Interactions", ICCVW, 2021 (*Universitat de Barcelona*). [[Paper](https://arxiv.org/abs/2109.09487)]
* 3D:
    * **3DRefTransformer**: "3DRefTransformer: Fine-Grained Object Identification in Real-World Scenes Using Natural Language", WACV, 2022 (*KAUST*). [[Paper](https://openaccess.thecvf.com/content/WACV2022/html/Abdelreheem_3DRefTransformer_Fine-Grained_Object_Identification_in_Real-World_Scenes_Using_Natural_Language_WACV_2022_paper.html)][[Website](https://vision-cair.github.io/3dreftransformer/)]
    * **EDA**: "EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual and Language Learning", arXiv, 2022 (*Peking University*). [[Paper](https://arxiv.org/abs/2209.14941)]
    * **PLA**: "Language-driven Open-Vocabulary 3D Scene Understanding", CVPR, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2211.16312)][[PyTorch](https://github.com/CVMI-Lab/PLA)][[Website](https://dingry.github.io/projects/PLA)]
    * **VL-SAT**: "VL-SAT: Visual-Linguistic Semantics Assisted Training for 3D Semantic Scene Graph Prediction in Point Cloud", CVPR, 2023 (*Beihang University*). [[Paper](https://arxiv.org/abs/2303.14408)][[PyTorch](https://github.com/wz7in/CVPR2023-VLSAT)]
    * **LERF**: "LERF: Language Embedded Radiance Fields", ICCV, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2303.09553)][[Website](https://www.lerf.io/)]
    * **ConceptFusion**: "ConceptFusion: Open-set Multimodal 3D Mapping", arXiv, 2023 (*MIT*). [[Paper](https://arxiv.org/abs/2302.07241)][[Website](https://concept-fusion.github.io/)]
    * **CG3D**: "CLIP goes 3D: Leveraging Prompt Tuning for Language Grounded 3D Recognition", arXiv, 2023 (*JHU*). [[Paper](https://arxiv.org/abs/2303.11313)][[PyTorch](https://github.com/deeptibhegde/CLIP-goes-3D)][[Website](https://jeya-maria-jose.github.io/cg3d-web/)]
    * **DiffCLIP**: "DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification", arXiv, 2023 (*Beijing Institute of Technology*). [[Paper](https://arxiv.org/abs/2305.15957)]
    * **LLM-Grounder**: "LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent", arXiv, 2023 (*UMich*). [[Paper](https://arxiv.org/abs/2309.12311)][[PyTorch](https://github.com/sled-group/chat-with-nerf)][[Website](https://chat-with-nerf.github.io/)]
* 3D Segmentation:
    * **OpenScene**: "OpenScene: 3D Scene Understanding with Open Vocabularies", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2211.15654)][[PyTorch](https://github.com/pengsongyou/openscene)][[Website](https://pengsongyou.github.io/openscene)]
    * **PartSLIP**: "PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models", CVPR, 2023 (*Qualcomm*). [[Paper](https://arxiv.org/abs/2212.01558)]
    * **CLIP2Scene**: "CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP", CVPR, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2301.04926)][[PyTorch](https://github.com/runnanchen/CLIP2Scene)]
    * **PLA**: "Language-driven Open-Vocabulary 3D Scene Understanding", CVPR, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2211.16312)][[PyTorch](https://github.com/CVMI-Lab/PLA)][[Website](https://dingry.github.io/projects/PLA)]
    * **3D-Highlighter**: "3D Highlighter: Localizing Regions on 3D Shapes via Text Descriptions", CVPR, 2023 (*University of Chicago*). [[Paper](https://arxiv.org/abs/2212.11263)][[PyTorch](https://github.com/threedle/3DHighlighter)][[Website](https://threedle.github.io/3DHighlighter/)]
    * **CLIP-FO3D**: "CLIP-FO3D: Learning Free Open-world 3D Scene Representations from 2D Dense CLIP", arXiv, 2023 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2303.04748)]
    * **3D-OVS**: "3D Open-vocabulary Segmentation with Foundation Models", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2305.14093)][[Code (in construction)](https://github.com/Kunhao-Liu/3D-OVS)]
    * **OVO**: "OVO: Open-Vocabulary Occupancy", arXiv, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2305.16133)]
    * **SAM3D**: "SAM3D: Segment Anything in 3D Scenes", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2306.03908)][[PyTorch](https://github.com/Pointcept/SegmentAnything3D)]
    * **Seal**: "Segment Any Point Cloud Sequences by Distilling Vision Foundation Models", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2306.09347)][[PyTorch (in construction)](https://github.com/youquanl/Segment-Any-Point-Cloud)]
    * **OpenMask3D**: "OpenMask3D: Open-Vocabulary 3D Instance Segmentation", arXiv, 2023 (*ETHZ*). [[Paper](https://arxiv.org/abs/2306.13631)][[Website (in construction)](https://openmask3d.github.io/)]
    * **Lowis3D**: "Lowis3D: Language-Driven Open-World Instance-Level 3D Scene Understanding", arXiv, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2308.00353)]
    * **OpenIns3D**: "OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation", arXiv, 2023 (*Cambridge*). [[Paper](https://arxiv.org/abs/2309.00616)][[Website](https://zheninghuang.github.io/OpenIns3D/)]
* Speech Recognition:
    * **AV-HuBERT**: "Robust Self-Supervised Audio-Visual Speech Recognition", arXiv, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2201.01763)][[PyTorch](https://github.com/facebookresearch/av_hubert)]
    * **?**: "Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2201.10439)]
    * **AVFormer**: "AVFormer: Injecting Vision into Frozen Speech Models for Zero-Shot AV-ASR", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.16501)]
    * **AV-RelScore**: "Watch or Listen: Robust Audio-Visual Speech Recognition with Visual Corruption Modeling and Reliability Scoring", CVPR, 2023 (*KAIST*). [[Paper](https://arxiv.org/abs/2303.08536)][[PyTorch](https://github.com/joannahong/AV-RelScore)]
    * **SynthVSR**: "SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision", CVPR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2303.17200)]
* Emotion Recognition:
    * **?**: "A Pre-trained Audio-Visual Transformer for Emotion Recognition", ICASSP, 2022 (*USC*). [[Paper](https://arxiv.org/abs/2201.09165)]
    * **MDAN**: "MDAN: Multi-level Dependent Attention Network for Visual Emotion Analysis", CVPR, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2203.13443)]
    * **DMD**: "Decoupled Multimodal Distilling for Emotion Recognition", CVPR, 2023 (*Nanjing University of Science and Technology*). [[Paper](https://arxiv.org/abs/2303.13802)][[PyTorch](https://github.com/mdswyz/DMD)]
* Sound Separation:
    * **VoViT**: "VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer", ECCV, 2022 (*Universitat Pompeu Fabra, Spain*). [[Paper](https://arxiv.org/abs/2203.04099)][[PyTorch](https://github.com/JuanFMontesinos/VoViT)][[Website](https://ipcv.github.io/VoViT/)]
    * **iQuery**: "iQuery: Instruments as Queries for Audio-Visual Sound Separation", CVPR, 2023 (*UCSD*). [[Paper](https://arxiv.org/abs/2212.03814)][[Code (in construction)](https://github.com/JiabenChen/iQuery)]
    * **VAST**: "Language-Guided Audio-Visual Source Separation via Trimodal Consistency", CVPR, 2023 (*Boston University*). [[Paper](https://arxiv.org/abs/2303.16342)][[Website](https://cs-people.bu.edu/rxtan/projects/VAST/)]
    * **AVIN**: "Induction Network: Audio-Visual Modality Gap-Bridging for Self-Supervised Sound Source Localization", ACMMM, 2023 (*Northwestern Polytechnical University*). [[Paper](https://arxiv.org/abs/2308.04767)][[Code (in construction)](https://github.com/Tahy1/AVIN)]
    * **GAVS**: "Prompting Segmentation with Sound is Generalizable Audio-Visual Source Localizer", arXiv, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2309.07929)]
* Audio-Visual:
    * **AV-HuBERT**: "Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction", ICLR, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2201.02184)][[PyTorch](https://github.com/facebookresearch/av_hubert)]
    * **AVCA**: "Audio-visual Generalised Zero-shot Learning with Cross-modal Attention and Language", CVPR, 2022 (*University of Tubingen, Germany*). [[Paper](https://arxiv.org/abs/2203.03598)][[PyTorch](https://github.com/ExplainableML/AVCA-GZSL)]
    * **TCaF**: "Temporal and cross-modal attention for audio-visual zero-shot learning", ECCV, 2022 (*University of Tubingen, Germany*). [[Paper](https://arxiv.org/abs/2207.09966)][[PyTorch](https://github.com/ExplainableML/TCAF-GZSL)]
    * **AVA-Memory**: "Audio-Visual Mismatch-Aware Video Retrieval via Association and Adjustment", ECCV, 2022 (*KAIST*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5034_ECCV_2022_paper.php)]
    * **TVLT**: "TVLT: Textless Vision-Language Transformer", NeurIPS, 2022 (*UNC*). [[Paper](https://arxiv.org/abs/2209.14156)][[PyTorch](https://github.com/zinengtang/TVLT)]
    * **ANGIE**: "Audio-Driven Co-Speech Gesture Video Generation", NeurIPS, 2022 (*CUHK*). [[Paper](https://openreview.net/forum?id=VhgC3SMTiy)][[Website](https://alvinliu0.github.io/projects/ANGIE)]
    * **MGN**: "Multi-modal Grouping Network for Weakly-Supervised Audio-Visual Video Parsing", NeurIPS, 2022 (*CMU + UT Austin*). [[Paper](https://openreview.net/forum?id=zfo2LqFEVY)][[PyTorch](https://github.com/stoneMo/MGN)]
    * **FS-RIR**: "Few-Shot Audio-Visual Learning of Environment Acoustics", NeurIPS, 2022 (*UT Austin*). [[Paper](https://arxiv.org/abs/2206.04006)][[Website](https://vision.cs.utexas.edu/projects/fs_rir/)]
    * **u-HuBERT**: "u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer to Unlabeled Modality", NeurIPS, 2022 (*Meta*). [[Paper](https://openreview.net/forum?id=zrAUoI2JA2)]
    * **PC-VAE**: "Multimodal Transformer for Parallel Concatenated Variational Autoencoders", NeurIPSW, 2022 (*USC*). [[Paper](https://arxiv.org/abs/2210.16174)]
    * **AV-CAT**: "Masked Lip-Sync Prediction by Audio-Visual Contextual Exploitation in Transformers", SIGGRAPH Asia, 2022 (*Tokyo Institute of Technology + Baidu*). [[Paper](https://arxiv.org/abs/2212.04970)][[Website](https://hangz-nju-cuhk.github.io/projects/AV-CAT)]
    * **Audiovisual-MAE**: "Audiovisual Masked Autoencoders", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2212.05922)]
    * **MTD**: "Multimodal Transformer Distillation for Audio-Visual Synchronization", arXiv, 2022 (*NTU*). [[Paper](https://arxiv.org/abs/2210.15563)]
    * **AVE-CLIP**: "AVE-CLIP: AudioCLIP-based Multi-window Temporal Transformer for Audio Visual Event Localization", WACV, 2023 (*UT Austin*). [[Paper](https://arxiv.org/abs/2210.05060)]
    * **CLIPSep**: "CLIPSep: Learning Text-queried Sound Separation with Noisy Unlabeled Videos", ICLR, 2023 (*Sony*). [[Paper](https://arxiv.org/abs/2212.07065)]
    * **CAV-MAE**: "Contrastive Audio-Visual Masked Autoencoder", ICLR, 2023 (*MIT + IBM*). [[Paper](https://arxiv.org/abs/2210.07839)]
    * **UnAV**: "Dense-Localizing Audio-Visual Events in Untrimmed Videos: A Large-Scale Benchmark and Baseline", CVPR, 2023 (*Southern University of Science and Technology*). [[Paper](https://arxiv.org/abs/2303.12930)][[PyTorch](https://github.com/ttgeng233/UnAV)][[Website](https://unav100.github.io/)]
    * **LAVISH**: "Vision Transformers are Parameter-Efficient Audio-Visual Learners", CVPR, 2023 (*UNC*). [[Paper](https://arxiv.org/abs/2212.07983)][[Pytorch](https://github.com/GenjiB/LAVISH)][[Website](https://yanbo.ml/project_page/LAVISH/)]
    * **OneAVM**: "A Unified Audio-Visual Learning Framework for Localization, Separation, and Recognition", ICML, 2023 (*CMU + UW Madison*). [[Paper](https://arxiv.org/abs/2305.19458)][[Code (in construction)](https://github.com/stoneMo/OneAVM)]
    * **AdVerb**: "AdVerb: Visually Guided Audio Dereverberation", ICCV, 2023 (*Maryland*). [[Paper](https://arxiv.org/abs/2308.12370)][[Website](https://gamma.umd.edu/researchdirections/speech/adverb)]
    * **CIGN**: "Class-Incremental Grouping Network for Continual Audio-Visual Learning", ICCV, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2309.05281)][[PyTorch](https://github.com/stoneMo/CIGN)]
    * **GestureDiffuCLIP**: "GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents", arXiv, 2023 (*Peking University*). [[Paper](https://arxiv.org/abs/2303.14613)]
    * **MMViT**: "MMViT: Multiscale Multiview Vision Transformers", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2305.00104)]
    * **?**: "Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos" arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2307.04760)]
* Audio-Visual Localization/Segmentation:
    * **AVSBench**: "Audio-Visual Segmentation", ECCV, 2022 (*SenseTime*). [[Paper](https://arxiv.org/abs/2207.05042)][[PyTorch](https://github.com/OpenNLPLab/AVSBench)][[Website](https://opennlplab.github.io/AVSBench/)]
    * **AV-SAM**: "AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation", arXiv, 2023 (*CMU + UT Dallas*). [[Paper](https://arxiv.org/abs/2305.01836)]
    * **AUSS**: "Hear to Segment: Unmixing the Audio to Guide the Semantic Segmentation", arXiv, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2305.07223)]
    * **AuTR**: "Annotation-free Audio-Visual Segmentation", arXiv, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2305.11019)]
    * **AVSegFormer**: "AVSegFormer: Audio-Visual Segmentation with Transformer", arXiv, 2023 (*Nanjing University*). [[Paper](https://arxiv.org/abs/2307.01146)][[PyTorch](https://github.com/vvvb-github/AVSegFormer)]
* Audio Description:
    * **AutoAD**: "AutoAD: Movie Description in Context", CVPR, 2023 (*Oxford*). [[Paper](https://arxiv.org/abs/2303.16899)][[Code (in construction)](https://github.com/TengdaHan/AutoAD)][[Website](https://www.robots.ox.ac.uk/~vgg/research/autoad/)]
* Sound Localization:
    * **TURN**: "Towards Effective Multi-Modal Interchanges in Zero-Resource Sounding Object Localization", NeurIPS, 2022 (*Zhejiang University*). [[Paper](https://openreview.net/forum?id=rQAJmrLmGC6)][[PyTorch (in construction)](https://github.com/AwalkZY/TURN)]
    * **AVGN**: "Audio-Visual Grouping Network for Sound Localization from Mixtures", CVPR, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2303.17056)][[PyTorch](https://github.com/stoneMo/AVGN)]
* Sentiment Analysis:
    * **CubeMLP**: "CubeMLP: A MLP-based Model for Multimodal Sentiment Analysis and Depression Estimation", ACMMM, 2022 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2207.14087)]
    * **MCMulT**: "Multi-scale Cooperative Multimodal Transformers for Multimodal Sentiment Analysis in Videos", arXiv, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2206.07981)]
* Name Entity Recognition:
    * **FMIT**: "Flat Multi-modal Interaction Transformer for Named Entity Recognition", International Conference on Computational Linguistics (COLING), 2022 (*South China University of Technology*). [[Paper](https://arxiv.org/abs/2208.11039)]
* Localization via Embodied Dialog:
    * **LED-Bert**: "Transformer-based Localization from Embodied Dialog with Large-scale Pre-training", arXiv, 2022 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2210.04864)]
* Object Captioning:
    * **GRiT**: "GRiT: A Generative Region-to-text Transformer for Object Understanding", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2212.00280)][[PyTorch](https://github.com/JialianW/GRiT)]
* Conversation:
    * **VisProg**: "Visual Programming: Compositional visual reasoning without training", CVPR, 2023 (*AI2*). [[Paper](https://arxiv.org/abs/2211.11559)][[PyTorch](https://github.com/allenai/visprog)][[Website](https://prior.allenai.org/projects/visprog)]
    * **LaVIN**: "Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models", NeurIPS, 2023 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2305.15023)][[PyTorch](https://github.com/luogen1996/LaVIN)][[Website](https://luogen1996.github.io/lavin/)]
    * **Visual-ChatGPT**: "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2303.04671)]
    * **MM-REACT**: "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2303.11381)][[Code](https://github.com/microsoft/MM-REACT)][[Website](https://multimodal-react.github.io/)]
    * **Video-ChatCaptioner**: "Video ChatCaptioner: Towards the Enriched Spatiotemporal Descriptions", arXiv, 2023 (*KAUST*). [[Paper](https://arxiv.org/abs/2304.04227)][[PyTorch](https://github.com/Vision-CAIR/ChatCaptioner)]
    * **Chameleon**: "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models", arXiv, 2023 (*UCLA + Microsoft*). [[Paper](https://arxiv.org/abs/2304.09842)][[PyTorch](https://github.com/lupantech/chameleon-llm)][[Website](https://chameleon-llm.github.io/)]
    * **MiniGPT-4**: "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models", arXiv, 2023 (*KAUST*). [[Paper](https://arxiv.org/abs/2304.10592)][[PyTorch](https://github.com/Vision-CAIR/MiniGPT-4)][[Website](https://minigpt-4.github.io/)]
    * **ChatVideo**: "ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System", arXiv, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2304.14407)][[Website](https://www.wangjunke.info/ChatVideo/)]
    * **LLaMA-Adapter**: "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2303.16199)][[PyTorch](https://github.com/ZrrSkywalker/LLaMA-Adapter)]
    * **LLaMA-Adapter-V2**: "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2304.15010)][[PyTorch](https://github.com/ZrrSkywalker/LLaMA-Adapter)]
    * **Otter**: "Otter: A Multi-Modal Model with In-Context Instruction Tuning", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2305.03726)][[PyTorch](https://github.com/Luodian/Otter)]
    * **LMEye**: "LMEye: An Interactive Perception Network for Large Language Models", arXiv, 2023 (*Meituan*). [[Paper](https://arxiv.org/abs/2305.03701)]
    * **MultiModal-GPT**: "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2305.04790)][[PyTorch](https://github.com/open-mmlab/Multimodal-GPT)]
    * **InternChat**: "InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2305.05662)][[PyTorch](https://github.com/OpenGVLab/InternGPT)]
    * **VideoChat**: "VideoChat: Chat-Centric Video Understanding", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2305.06355)][[PyTorch](https://github.com/OpenGVLab/Ask-Anything)]
    * **InstructBLIP**: "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning", arXiv, 2023 (*Salesforce*). [[Paper](https://arxiv.org/abs/2305.06500)][[PyTorch](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)]
    * **ArtGPT-4**: "ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4", arXiv, 2023 (*Anhui Polytechnic University*). [[Paper](https://arxiv.org/abs/2305.07490)][[PyTorch](https://github.com/DLYuanGod/ArtGPT-4)]
    * **EmbodiedGPT**: "EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought", arXiv, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2305.15021)][[PyTorch (in construction)](https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch)][[Website](https://embodiedgpt.github.io/)]
    * **PandaGPT**: "PandaGPT: One Model To Instruction-Follow Them All", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2305.16355)][[PyTorch](https://github.com/yxuansu/PandaGPT)][[Website](https://panda-gpt.github.io/)]
    * **Video-LLaMA**: "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2306.02858)][[PyTorch](https://github.com/DAMO-NLP-SG/Video-LLaMA)]
    * **MIMIC-IT**: "MIMIC-IT: Multi-Modal In-Context Instruction Tuning", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2306.05425)][[PyTorch](https://github.com/Luodian/otter)][[Website](https://otter-ntu.github.io/)]
    * **Video-ChatGPT**: "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models", arXiv, 2023 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2306.05424)][[PyTorch](https://github.com/mbzuai-oryx/Video-ChatGPT)]
    * **LAMM**: "LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2306.06687)]
    * **?**: "Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models", arXiv, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2306.08641)]
    * **AssistGPT**: "AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2306.08640)][[Code (in construction)](https://github.com/showlab/assistgpt)][[Website](https://showlab.github.io/assistgpt/)]
    * **Macaw-LLM**: "Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2306.09093)][[PyTorch](https://github.com/lyuchenyang/Macaw-LLM)]
    * **Shikra**: "Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic", arXiv, 2023 (*SenseTime*). [[Paper](https://arxiv.org/abs/2306.15195)][[Code (in construction)](https://github.com/shikras/shikra)]
    * **LLaVAR**: "LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding", arXiv, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2306.17107)][[PyTorch](https://github.com/SALT-NLP/LLaVAR)][[Website](https://llavar.github.io/)]
    * **Polite-Flamingo**: "Visual Instruction Tuning with Polite Flamingo", arXiv, 2023 (*Xiaobing.AI*). [[Paper](https://arxiv.org/abs/2307.01003)]
    * **Lynx**: "What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2307.02469)][[Website](https://lynx-llm.github.io/)]
    * **GPT4RoI**: "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2307.03601)][[PyTorch](https://github.com/jshilong/GPT4RoI)]
    * **SVIT**: "SVIT: Scaling up Visual Instruction Tuning", arXiv, 2023 (*BAAI*). [[Paper](https://arxiv.org/abs/2307.04087)]
    * **AmadeusGPT**: "AmadeusGPT: a natural language interface for interactive animal behavioral analysis", arXiv, 2023 (*EPFL*). [[Paper](https://arxiv.org/abs/2307.04858)][[Code (in construction)](https://github.com/AdaptiveMotorControlLab/AmadeusGPT)]
    * **ChatSpot**: "ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning", arXiv, 2023 (*Megvii*). [[Paper](https://arxiv.org/abs/2307.09474)][[Demo](https://chatspot.streamlit.app/)]
    * **3D-LLM**: "3D-LLM: Injecting the 3D World into Large Language Models", arXiv, 2023 (*UCLA*). [[Paper](https://arxiv.org/abs/2307.12981)][[PyTorch (in construction)](https://github.com/UMass-Foundation-Model/3D-LLM)][[Website](https://vis-www.cs.umass.edu/3dllm/)]
    * **?**: "How Good is Google Bard's Visual Understanding? An Empirical Study on Open Challenges", arXiv, 2023 (*ETHZ*). [[Paper](https://arxiv.org/abs/2307.15016)][[GitHub (in construction)](https://github.com/htqin/GoogleBard-VisUnderstand)]
    * **MovieChat**: "MovieChat: From Dense Token to Sparse Memory for Long Video Understanding", arXiv, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2307.16449)][[PyTorch](https://github.com/rese1f/MovieChat)][[Website](https://rese1f.github.io/MovieChat/)]
    * **AntGPT**: "AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?", arXiv, 2023 (*Brown*). [[Paper](https://arxiv.org/abs/2307.16368)][[Website](https://brown-palm.github.io/AntGPT/)]
    * **?**: "Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2308.00675)]
    * **MM-Vet**: "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2308.02490)][[Code](https://github.com/yuweihao/MM-Vet)]
    * **Chat-3D**: "Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes", arXiv, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2308.08769)][[PyTorch](https://github.com/Chat-3D/Chat-3D)][[Website](https://chat-3d.github.io/)]
    * **LLaVA**: "Visual Instruction Tuning", arXiv, 2023 (*UW-Madison*). [[Paper](https://arxiv.org/abs/2304.08485)][[PyTorch](https://github.com/haotian-liu/LLaVA)][[Website](https://llava-vl.github.io/)]
    * **StableLLaVA**: "StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2308.10253)][[Code (in construction)](https://github.com/icoz69/StableLLAVA)]
    * **PVIT**: "Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2308.13437)]
    * **PointLLM**: "PointLLM: Empowering Large Language Models to Understand Point Clouds", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2308.16911)][[Code (in construction)](https://github.com/OpenRobotLab/PointLLM)][[Website](https://runsenxu.com/projects/PointLLM/)]
    * **Point-Bind**: "Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2309.00615)][[PyTorch](https://github.com/ZiyuGuo99/Point-Bind_Point-LLM)]
    * **ImageBind-LLM**: "ImageBind-LLM: Multi-modality Instruction Tuning", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2309.03905)]
    * **?**: "An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2309.09958)][[GitHub](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md)]
    * **InternLM-XComposer**: "InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2309.15112)][[PyTorch](https://github.com/InternLM/InternLM-XComposer)]
    * **LLaVA-RLHF**: "Aligning Large Multimodal Models with Factually Augmented RLHF", arXiv, 2023 (*Berkeley + CMU + UIUC*). [[Paper](https://arxiv.org/abs/2309.14525)][[Code (in construction)](https://github.com/llava-rlhf/LLaVA-RLHF)][[Website](https://llava-rlhf.github.io/)]
* Visual Reasoning:
    * **BDC-Adapter**: "BDC-Adapter: Brownian Distance Covariance for Better Vision-Language Reasoning", BMVC, 2023 (*SUSTech*). [[Paper](https://arxiv.org/abs/2309.01256)]
    * **RPT**: "Fine-Grained Regional Prompt Tuning for Visual Abductive Reasoning", arXiv, 2023 (*A\*STAR*). [[Paper](https://arxiv.org/abs/2303.10428)]
    * **LRR**: "Look, Remember and Reason: Visual Reasoning with Grounded Rationales", arXiv, 2023 (*Qualcomm*). [[Paper](https://arxiv.org/abs/2306.17778)]
    * **SDS-CLIP**: "Augmenting CLIP with Improved Visio-Linguistic Reasoning", arXiv, 2023 (*Maryland*). [[Paper](https://arxiv.org/abs/2307.09233)]
    * **?**: "Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models", arXiv, 2023 (*George Mason University*). [[Paper](https://arxiv.org/abs/2308.09778)]
* Tracking:
    * **JointNLT**: "Joint Visual Grounding and Tracking with Natural Language Specification", CVPR, 2023 (*Harbin Institute of Technology*). [[Paper](https://arxiv.org/abs/2303.12027)][[PyTorch](https://github.com/lizhou-cs/JointNLT)]
    * **MMTrack**: "Towards Unified Token Learning for Vision-Language Tracking", arXiv, 2023 (*Guangxi Normal University*). [[Paper](https://arxiv.org/pdf/2308.14103.pdf)]
* Scene Graph:
    * **CaCao**: "Visually-Prompted Language Model for Fine-Grained Scene Graph Generation in an Open World", arXiv, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2303.13233)]
* Egocentric Video:
    * **MMG-Ego4D**: "MMG-Ego4D: Multi-Modal Generalization in Egocentric Action Recognition", CVPR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2305.07214)]
    * **EgoTV**: "EgoTV: Egocentric Task Verification from Natural Language Task Descriptions", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2303.16975)]
* Dance Generation:
    * **TM2D**: "TM2D: Bimodality Driven 3D Dance Generation via Music-Text Integration", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2304.02419)][[Code (in construction)](https://github.com/Garfield-kh/TM2D)][[Website](https://garfield-kh.github.io/TM2D/)]
* Conceptual Understanding:
    * **?**: "Text-To-Concept (and Back) via Cross-Model Alignment", ICML, 2023 (*Maryland*). [[Paper](https://arxiv.org/abs/2305.06386)]
    * **?**: "Probing Conceptual Understanding of Large Visual-Language Models", arXiv, 2023 (*UCF + SRI*). [[Paper](https://arxiv.org/abs/2304.03659)]
    * **EAC**: "Explain Any Concept: Segment Anything Meets Concept-Based Explanation", arXiv, 2023 (*HKUST*). [[Paper](https://arxiv.org/abs/2305.10289)]
* Model Merging:
    * **VL-merging**: "An Empirical Study of Multimodal Model Merging", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2304.14933)][[PyTorch](https://github.com/ylsung/vl-merging)]
* Visual Word Sense Disambiguation (VWSD):
    * **CADG**: "Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information", ACL, 2023 (*UMass*). [[Paper](https://arxiv.org/abs/2305.01788)]
* Object Hallucination:
    * **POPE**: "Evaluating Object Hallucination in Large Vision-Language Models", arXiv, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2305.10355)][[Code (in construction)](https://github.com/RUCAIBox/POPE)]
* Social Interaction:
    * **HIINT**: "HIINT: Historical, Intra- and Inter- personal Dynamics Modeling with Cross-person Memory Transformer", arXiv, 2023 (*MIT*). [[Paper](https://arxiv.org/abs/2305.12369)]
* Evaluation:
    * **Perception-Test**: "Perception Test: A Diagnostic Benchmark for Multimodal Video Models", arXiv, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2305.13786)][[GitHub](https://github.com/deepmind/perception_test)]
    * **VLM-Probing**: "Scalable Performance Analysis for Vision-Language Models", Joint Conference on Lexical and Computational Semantics (\*SEM), 2023 (*UMich*). [[Paper](https://arxiv.org/abs/2305.18786)][[PyTorch](https://github.com/MichiganNLP/Scalable-VLM-Probing)]
    * **VisualGPTScore**: "VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores", arXiv, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2306.01879)][[Code (in construction)](https://github.com/linzhiqiu/visual_gpt_score/)][[Website](https://linzhiqiu.github.io/papers/visual_gpt_score/)]
    * **LVLM-eHub**: "LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2306.09265)][[PyTorch (in construction)](https://github.com/OpenGVLab/Multi-Modality-Arena)]
    * **VisoGender**: "VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution", arXiv, 2023 (*Oxford*). [[Paper](https://arxiv.org/abs/2306.12424)][[PyTorch](https://github.com/oxai/visogender)]
    * **MME**: "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2306.13394)][[Code (in construction)](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)]
    * **MMBench**: "MMBench: Is Your Multi-modal Model an All-around Player?", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2307.06281)][[Website](https://opencompass.org.cn/mmbench)]
    * **Tiny-LVLM-eHub**: "Tiny LVLM-eHub: Early Multimodal Experiments with Bard", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2308.03729)][[PyTorch](https://github.com/OpenGVLab/Multi-Modality-Arena)][[Website](http://lvlm-ehub.opengvlab.com/)]
    * **VisIT-Bench**: "VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use", arXiv, 2023 (*UW*). [[Paper](https://arxiv.org/abs/2308.06595)][[Website](https://visit-bench.github.io/)]
    * **MODE**: "An Examination of the Compositionality of Large Generative Vision-Language Models", arXiv, 2023 (*HKUST*). [[Paper](https://arxiv.org/abs/2308.10509)]
    * **TouchStone**: "TouchStone: Evaluating Vision-Language Models by Language Models", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2308.16890)]
    * **Q-Bench**: "Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2309.14181)]
* Robustness:
    * **Hierarchy-CLIP**: "Improving Zero-shot Generalization and Robustness of Multi-modal Models", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2212.01758)][[JAX](https://github.com/gyhandy/Hierarchy-CLIP)][[Website](https://sites.google.com/usc.edu/hierarchy-clip/)]
    * **?**: "Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning", ICML, 2023 (*UCLA*). [[Paper](https://arxiv.org/abs/2304.03916)]
    * **SGA**: "Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models", ICCV, 2023 (*Southern University of Science and Technology*). [[Paper](https://arxiv.org/abs/2307.14061)]
    * **AttackVLM**: "On Evaluating Adversarial Robustness of Large Vision-Language Models", arXiv, 2023 (*Singapore University of Technology and Design (SUTD)*). [[Paper](https://arxiv.org/abs/2305.16934)][[PyTorch (in construction)](https://github.com/yunqing-me/AttackVLM)]
* Compositional Reasoning:
    * **DAC**: "Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models", arXiv, 2023 (*IBM*). [[Paper](https://arxiv.org/abs/2305.19595)]
    * **SugarCrepe**: "SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality", arXiv, 2023 (*AI2*). [[Paper](https://arxiv.org/abs/2306.14610)][[PyTorch](https://github.com/RAIVNLab/sugar-crepe)]
* Vocabulary-free Image Classification (VIC):
    * **CaSED**: "Vocabulary-free Image Classification", arXiv, 2023 (*University of Trento, Italy*). [[Paper](https://arxiv.org/abs/2306.00917)][[PyTorch](https://github.com/altndrr/vic)]
* Retrieval Augmentated Methods:
    * **?**: "Improving Image Recognition by Retrieving from Web-Scale Image-Text Data", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2304.05173)]
* NeRF:
    * **NeRDi**: "NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors", CVPR, 2023 (*Waymo*). [[Paper](https://arxiv.org/abs/2212.03267)]
* Model Selection:
    * **LOVM**: "LOVM: Language-Only Vision Model Selection", arXiv, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2306.08893)] 
* Multimodal Interaction:
    * **?**: "Learning Unseen Modality Interaction", arXiv, 2023 (*University of Amsterdam*). [[Paper](https://arxiv.org/abs/2306.12795)]
* Multimodal Translation:
    * **CLIPTrans**: "CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation", ICCV, 2023 (*Boston College*). [[Paper](https://arxiv.org/abs/2308.15226)][[PyTorch](https://github.com/devaansh100/CLIPTrans)]

[[Back to Overview](#overview)]
