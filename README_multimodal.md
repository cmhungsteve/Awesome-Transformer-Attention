(back to [README.md](README.md) and [README_2.md](README_2.md) for other categories)
## Overview

- [Citation](#citation)
- [Multi-Modality](#multi-modality)
    - [Visual Captioning](#visual-captioning)
    - [Visual Question Answering](#visual-question-answering)
    - [Visual Grounding](#visual-grounding)
    - [Multi-Modal Representation Learning](#multi-modal-representation-learning)
    - [Multi-Modal Retrieval](#multi-modal-retrieval)
    - [Multi-Modal Generation](#multi-modal-generation)
    - [Prompt Learning/Tuning](#prompt-learningtuning)
    - [Visual Document Understanding](#visual-document-understanding)
    - [Other Multi-Modal Tasks](#other-multi-modal-tasks)

---

## Citation
If you find this repository useful, please consider citing this list:
```
@misc{chen2022transformerpaperlist,
    title = {Ultimate awesome paper list: transformer and attention},
    author = {Chen, Min-Hung},
    journal = {GitHub repository},
    url = {https://github.com/cmhungsteve/Awesome-Transformer-Attention},
    year = {2022},
}
```

---

## Multi-Modality
### Visual Captioning
* General:
    * **SAT**: "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", ICML, 2015. [[paper](https://arxiv.org/abs/1502.03044)] 
    * **ETA-Transformer**: "Entangled Transformer for Image Captioning", ICCV, 2019 (*UTS*). [[Paper](https://openaccess.thecvf.com/content_ICCV_2019/html/Li_Entangled_Transformer_for_Image_Captioning_ICCV_2019_paper.html)]
    * **M2-Transformer**: "Meshed-Memory Transformer for Image Captioning", CVPR, 2020 (*UniMoRE*). [[Paper](https://arxiv.org/abs/1912.08226)][[PyTorch](https://github.com/aimagelab/meshed-memory-transformer)] 
    * **MCCFormers**: "Describing and Localizing Multiple Changes with Transformers", ICCV, 2021 (*AIST*). [[Paper](https://arxiv.org/abs/2103.14146)][[Website](https://cvpaperchallenge.github.io/Describing-and-Localizing-Multiple-Change-with-Transformers/)]
    * **SATIC**: "Semi-Autoregressive Transformer for Image Captioning", ICCVW, 2021 (*Hefei University of Technology*). [[Paper](https://arxiv.org/abs/2106.09436)][[PyTorch](https://github.com/YuanEZhou/satic)]
    * **DGCN**: "Dual Graph Convolutional Networks with Transformer and Curriculum Learning for Image Captioning", ACMMM, 2021 (*Wuhan University*). [[Paper](https://arxiv.org/abs/2108.02366)]
    * **CPTR**: "CPTR: Full Transformer Network for Image Captioning", arXiv, 2021 (*CAS*). [[Paper](https://arxiv.org/abs/2101.10804)] 
    * **ReFormer**: "ReFormer: The Relational Transformer for Image Captioning", arXiv, 2021 (*Stony Brook University*). [[Paper](https://arxiv.org/abs/2107.14178)]
    * **LAViTeR**: "LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation", arXiv, 2021 (*University at Buffalo*). [[Paper](https://arxiv.org/abs/2109.04993)]
    * **LATGeO**: "Label-Attention Transformer with Geometrically Coherent Objects for Image Captioning", arXiv, 2021 (*Gwangju Institute of Science and Technology*). [[Paper](https://arxiv.org/abs/2109.07799)]
    * **GEVST**: "Geometry-Entangled Visual Semantic Transformer for Image Captioning", arXiv, 2021 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2109.14137)]
    * **GAT**: "Geometry Attention Transformer with Position-aware LSTMs for Image Captioning", arXiv, 2021 (*University of Electronic Science and Technology of China*). [[Paper](https://arxiv.org/abs/2110.00335)]
    * **PureT**: "End-to-End Transformer Based Model for Image Captioning", AAAI, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2203.15350)]
    * **VisualGPT**: "VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning", CVPR, 2022 (*KAUST*). [[Paper](https://arxiv.org/abs/2102.10407)][[PyTorch](https://github.com/Vision-CAIR/VisualGPT)]
    * **ViTCAP**: "Injecting Semantic Concepts into End-to-End Image Captioning", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2112.05230)]
    * **CLIP-Event**: "CLIP-Event: Connecting Text and Images with Event Structures", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2201.05078)][[PyTorch](https://github.com/limanling/clip-event)]
    * **?**: "Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning", CVPR, 2022 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2205.04363)][[PyTorch](https://github.com/GT-RIPL/Xmodal-Ctx)]
    * **CLIP4IDC**: "CLIP4IDC: CLIP for Image Difference Captioning", CVPRW, 2022 (*Aalto University, Finland*). [[Paper](https://arxiv.org/abs/2206.00629)][[Code (in construction)](https://github.com/sushizixin/CLIP4IDC)]
    * **?**: "A Dual-Attentive Approach to Style-Based Image Captioning Using a CNN-Transformer Model", CVPRW, 2022 (*The University of the West Indies, Jamaica*). [[Paper](https://drive.google.com/file/d/1QYq69dBFMBKHYDUolZqPaermiFz67k77/view)]
    * **SpaCap3D**: "Spatiality-guided Transformer for 3D Dense Captioning on Point Clouds", IJCAI, 2022 (*University of Sydney*). [[Paper](https://arxiv.org/abs/2204.10688)][[Code (in construction)](https://github.com/heng-hw/SpaCap3D)][[Website](https://spacap3d.github.io/)]
    * **RA-Transformer**: "Retrieval-Augmented Transformer for Image Captioning", International Conference on Content-based Multimedia Indexing (CMBI), 2022 (*University of Modena and Reggio Emilia, Italy*). [[Paper](https://arxiv.org/abs/2207.13162)]
    * **GRIT**: "GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features", ECCV, 2022 (*Tohoku University + RIKEN AIP*). [[Paper](https://arxiv.org/abs/2207.09666)][[PyTorch](https://github.com/davidnvq/grit)]
    * **?**: "Object-Centric Unsupervised Image Captioning", ECCV, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2112.00969)][[PyTorch](https://github.com/zihangm/obj-centric-unsup-caption)]
    * **UEDVC**: "Unifying Event Detection and Captioning as Sequence Generation via Pre-Training", ECCV, 2022 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2207.08625)][[PyTorch](https://github.com/QiQAng/UEDVC)]
    * **TIger**: "Explicit Image Caption Editing", ECCV, 2022 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2207.09625)][[Code](https://github.com/baaaad/ECE)]
    * **DML**: "Learning Distinct and Representative Modes for Image Captioning", NeurIPS, 2022 (*University of Adelaide, Australia*). [[Paper](https://arxiv.org/abs/2209.08231)]
    * **P2C**: "Paraphrasing Is All You Need for Novel Object Captioning", NeurIPS, 2022 (*NTU + CMU*). [[Paper](https://arxiv.org/abs/2209.12343)]
    * **BEST**: "Visual Clues: Bridging Vision and Language Foundations for Image Paragraph Captioning", NeurIPS, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2206.01843)]
    * **CapDec**: "Text-Only Training for Image Captioning using Noise-Injected CLIP", EMNLP, 2022 (*Tel Aviv*). [[Paper](https://arxiv.org/abs/2211.00575)][[Pytorch](https://github.com/DavidHuji/CapDec)]
    * **?**: "Focus! Relevant and Sufficient Context Selection for News Image Captioning", EMNLP Findings, 2022 (*UC Davis*). [[Paper](https://arxiv.org/abs/2212.00843)]
    * **CVLNM**: "Learning to Collocate Visual-Linguistic Neural Modules for Image Captioning", IJCV, 2022 (*Southeast University, China*). [[Paper](https://arxiv.org/abs/2210.01338)][[PyTorch](https://github.com/GCYZSL/CVLMN)]
    * **ViNTER**: "ViNTER: Image Narrative Generation with Emotion-Arc-Aware Transformer", arXiv, 2022 (*The University of Tokyo*). [[Paper](https://arxiv.org/abs/2202.07305)]
    * **VaT**: "Variational Transformer: A Framework Beyond the Trade-off between Accuracy and Diversity for Image Captioning", arXiv, 2022 (*Tongji University*). [[Paper](https://arxiv.org/abs/2205.14458)]
    * **SCST-GEG**: "Distincive Image Captioning via CLIP Guided Group Optimization", arXiv, 2022 (*McGill University*). [[Paper](https://arxiv.org/abs/2208.04254)]
    * **?**: "Vision Transformer Based Model for Describing a Set of Images as a Story", arXiv, 2022 (*The University of Western Australia*). [[Paper](https://arxiv.org/abs/2210.02762)]
    * **CLM**: "Zero-shot Image Captioning by Anchor-augmented Vision-Language Space Alignment", arXiv, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2211.07275)]
    * **PTSN**: "Progressive Tree-Structured Prototype Network for End-to-End Image Captioning", arXiv, 2022 (*University of Electronic Science and Technology of China (UESTC)*). [[Paper](https://arxiv.org/abs/2211.09460)][[PyTorch (in construction)](https://github.com/NovaMind-Z/PTSN)]
    * **DDCap**: "Exploring Discrete Diffusion Models for Image Captioning", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2211.11694)][[PyTorch](https://github.com/buxiangzhiren/DDCap)]
    * **ARIC**: "Aesthetically Relevant Image Captioning", AAAI, 2023 (*Shenzhen University*). [[Paper](https://arxiv.org/abs/2211.15378)][[Code (in construction)](https://github.com/PengZai/ARIC)]
    * **UAIC**: "Uncertainty-Aware Image Captioning", AAAI, 2023 (*Meituan*). [[Paper](https://arxiv.org/abs/2211.16769)]
    * **LiMBeR**: "Linearly Mapping from Image to Text Space", ICLR, 2023 (*Brown University*). [[Paper](https://arxiv.org/abs/2209.15162)]
    * **DiscriTune**: "Cross-Domain Image Captioning with Discriminative Finetuning", CVPR, 2023 (*Universitat Pompeu Fabra (UPF), Spain*). [[Paper](https://arxiv.org/abs/2304.01662)]
    * **LIBRA**: "Model-Agnostic Gender Debiased Image Captioning", CVPR, 2023 (*Osaka University*). [[Paper](https://arxiv.org/abs/2304.03693)]
    * **A-CAP**: "A-CAP: Anticipation Captioning with Commonsense Knowledge", CVPR, 2023 (*The University of Tokyo*). [[Paper](https://arxiv.org/abs/2304.06602)]
    * **HAAV**: "HAAV: Hierarchical Aggregation of Augmented Views for Image Captioning", CVPR, 2023 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2305.16295)][[Website](https://sites.google.com/view/chiawen-kuo/home/haav)]
    * **?**: "Cross-Domain Image Captioning with Discriminative Finetuning", CVPR, 2023 (*Universitat Pompeu Fabra (UPF), Spain*). [[Paper](https://arxiv.org/abs/2304.01662)]
    * **PAC-S**: "Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation", CVPR, 2023 (*UniMoRE, Italy*). [[Paper](https://arxiv.org/abs/2303.12112)][[PyTorch](https://github.com/aimagelab/pacscore)]
    * **SCD-Net**: "Semantic-Conditional Diffusion Networks for Image Captioning", CVPR, 2023 (*JD*). [[Paper](https://arxiv.org/abs/2212.03099)][[PyTorch](https://github.com/jianjieluo/SCD-Net)]
    * **ConZIC**: "ConZIC: Controllable Zero-shot Image Captioning by Sampling-Based Polishing", CVPR, 2023 (*Xidian University*). [[Paper](https://arxiv.org/abs/2303.02437)][[PyTorch](https://github.com/joeyz0z/ConZIC)]
    * **SmallCap**: "SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation", CVPR, 2023 (*University of Lisbon, Portugal*). [[Paper](https://arxiv.org/abs/2209.15323)][[PyTorch](https://github.com/RitaRamo/smallcap)]
    * **LSML**: "Crossing the Gap: Domain Generalization for Image Captioning", CVPR, 2023 (*USTC*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Ren_Crossing_the_Gap_Domain_Generalization_for_Image_Captioning_CVPR_2023_paper.html)]
    * **MuE**: "You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model", CVPR, 2023 (*NC State*). [[Paper](https://arxiv.org/abs/2211.11152)]
    * **OxfordTVG-HIC**: "OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?", ICCV, 2023 (*Oxford*). [[Paper](https://arxiv.org/abs/2307.11636)][[Website](https://torrvision.com/tvghic/)]
    * **?**: "Guiding Image Captioning Models Toward More Specific Captions", ICCV, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2307.16686)]
    * **ViECap**: "Transferable Decoding with Visual Entities for Zero-Shot Image Captioning", ICCV, 2023 (*Southern University of Science and Technology*). [[Paper](https://arxiv.org/abs/2307.16525)][[PyTorch](https://github.com/FeiElysia/ViECap)]
    * **PMA-Net**: "With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning", ICCV, 2023 (*University of Modena and Reggio Emilia (UniMoRE), Italy*). [[Paper](https://arxiv.org/abs/2308.12383)][[Code (in construction)](https://github.com/aimagelab/PMA-Net)]
    * **SCORER**: "Self-supervised Cross-view Representation Reconstruction for Change Captioning", ICCV, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2309.16283)][[Code (in construction)](https://github.com/tuyunbin/SCORER)]
    * **PromptCap**: "PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3", ICCV, 2023 (*UW*). [[Paper](https://arxiv.org/abs/2211.09699)][[PyTorch](https://github.com/Yushi-Hu/PromptCap)][[Website](https://yushi-hu.github.io/promptcap_demo/)]
    * **NoC**: "Noise-aware Learning from Web-crawled Image-Text Data for Image Captioning", ICCV, 2023 (*Kakao*). [[Paper](https://arxiv.org/abs/2212.13563)][[PyTorch](https://github.com/kakaobrain/noc)]
    * **TSG**: "Transforming Visual Scene Graphs to Image Captions", ACL, 2023 (*Southeast University, China*). [[Paper](https://arxiv.org/abs/2305.02177)][[PyTorch](https://anonymous.4open.science/r/ACL23_TSG/README.md)]
    * **InfoMetIC**: "InfoMetIC: An Informative Metric for Reference-free Image Caption Evaluation", ACL, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2305.06002)][[Code (in construction)](https://github.com/HAWLYQ/InfoMetIC)]
    * **MultiCapCLIP**: "MultiCapCLIP: Auto-Encoding Prompts for Zero-Shot Multilingual Visual Captioning", ACL, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2308.13218)][[PyTorch (in construction)](https://github.com/yangbang18/MultiCapCLIP)]
    * **Cur-VL**: "Learning from Children: Improving Image-Caption Pretraining via Curriculum", ACL Findings, 2023 (*Columbia*). [[Paper](https://arxiv.org/abs/2305.17540)][[Code (in construction)](https://github.com/hayyubi/cur_vl)]
    * **?**: "Text-Only Training for Visual Storytelling", ACMMM, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2308.08881)]
    * **CgT-GAN**: "CgT-GAN: CLIP-guided Text GAN for Image Captioning", ACMMM, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2308.12045)][[PyTorch](https://github.com/Lihr747/CgtGAN)]
    * **CLAIR**: "CLAIR: Evaluating Image Captions with Large Language Models", EMNLP, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2310.12971)][[Code](https://github.com/davidmchan/clair)][[Website](https://davidmchan.github.io/clair/)]
    * **SCP-WGCN**: "Improving Image Captioning via Predicting Structured Concepts", EMNLP, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2311.08223)][[Code (in construction)](https://github.com/wangting0/SCP-WGCN)]
    * **ExploreCfg**: "Exploring Diverse In-Context Configurations for Image Captioning", NeurIPS, 2023 (*Southeast University, China*). [[Paper](https://arxiv.org/abs/2305.14800)][[PyTorch](https://github.com/yongliang-wu/ExploreCfg)]
    * **COLA**: "COLA: How to adapt vision-language models to Compose Objects Localized with Attributes?", NeurIPS, 2023 (*Boston*). [[Paper](https://arxiv.org/abs/2305.03689)][[Website](https://cs-people.bu.edu/array/research/cola/)]
    * **Re-ViLM**: "Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2302.04858)]
    * **Knight**: "From Association to Generation: Text-only Captioning by Unsupervised Cross-modal Mapping", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2304.13273)][[PyTorch](https://github.com/junyangwang0410/Knight)]
    * **VTT**: "Visual Transformation Telling", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2305.01928)]
    * **Caption-Anything**: "Caption Anything: Interactive Image Description with Diverse Multimodal Controls", arXiv, 2023 (*Southern University of Science and Technology*). [[Paper](https://arxiv.org/abs/2305.02677)][[PyTorch](https://github.com/ttengwang/Caption-Anything)]
    * **?**: "Data Curation for Image Captioning with Text-to-Image Generative Models", arXiv, 2023 (*University of Copenhagen, Denmark*). [[Paper](https://arxiv.org/abs/2305.03610)]
    * **TLC**: "Simple Token-Level Confidence Improves Caption Correctness", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2305.07021)]
    * **VIVID**: "Album Storytelling with Iterative Story-aware Captioning and Large Language Models", arXiv, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2305.12943)]
    * **MCDG**: "Text-Only Image Captioning with Multi-Context Data Generation", arXiv, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2305.18072)]
    * **FuseCap**: "FuseCap: Leveraging Large Language Models to Fuse Visual Data into Enriched Image Captions", arXiv, 2023 (*Israel Institute of Technology*). [[Paper](https://arxiv.org/abs/2305.17718)]
    * **StoryGen**: "Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2306.00973)][[PyTorch (in construction)](https://github.com/haoningwu3639/StoryGen)][[Website](https://haoningwu3639.github.io/StoryGen_Webpage/)]
    * **?**: "Improving Image Captioning Descriptiveness by Ranking and LLM-based Fusion", arXiv, 2023 (*University of Milano-Bicocca, Italy*). [[Paper](https://arxiv.org/abs/2306.11593)]
    * **SITTA**: "SITTA: A Semantic Image-Text Alignment for Image Captioning", arXiv, 2023 (*Johannes Kepler University, Austria*). [[Paper](https://arxiv.org/abs/2307.05591)][[PyTorch](https://github.com/ml-jku/semantic-image-text-alignment)]
    * **MMNS**: "Multimodal Neurons in Pretrained Text-Only Transformers", arXiv, 2023 (*MIT*). [[Paper](https://arxiv.org/abs/2308.01544)]
    * **RegionBLIP**: "RegionBLIP: A Unified Multi-modal Pre-training Framework for Holistic and Regional Comprehension", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2308.02299)][[PyTorch](https://github.com/mightyzau/RegionBLIP)]
    * **?**: "Visually-Aware Context Modeling for News Image Captioning", arXiv, 2023 (*KU Leuven*). [[Paper](https://arxiv.org/abs/2308.08325)]
    * **EVCap**: "EVCap: Retrieval-Augmented Image Captioning with External Visual-Name Memory for Open-World Comprehension", arXiv, 2023 (*The University of Tokyo*). [[Paper](https://arxiv.org/abs/2311.15879)][[Website](https://jiaxuan-li.github.io/EVCap/)]
    * **SCA**: "Segment and Caption Anything", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2312.00869)][[PyTorch](https://github.com/xk-huang/segment-caption-anything)][[Website](https://xk-huang.github.io/segment-caption-anything/)]
    * **sDCI**: "A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2312.08578)][[PyTorch](https://github.com/facebookresearch/DCI)]
    * **DisCLIP**: "DisCLIP: Open-Vocabulary Referring Expression Generation", arXiv, 2024 (*Bar-Ilan University, Israel*). [[Paper](https://arxiv.org/abs/2305.19108)]
    * **MacCap**: "Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only Training", AAAI, 2024 (*ShanghaiTech*). [[Paper](https://arxiv.org/abs/2401.02347)][[Code (in construction)](https://github.com/Artanic30/MacCap)]
    * **RegionGPT**: "RegionGPT: Towards Region Understanding Vision Language Model", CVPR, 2024 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2403.02330)][[Website](https://guoqiushan.github.io/regiongpt.github.io/)]
    * **MeaCap**: "MeaCap: Memory-Augmented Zero-shot Image Captioning", CVPR, 2024 (*Xidian University*). [[Paper](https://arxiv.org/abs/2403.03715)][[Code (in construction)](https://github.com/joeyz0z/MeaCap)]
* Video:
    * **Masked Transformers**: "End-to-End Dense Video Captioning with Masked Transformer", CVPR, 2018 (*UMich + Salesforce*). [[Paper](https://openaccess.thecvf.com/content_cvpr_2018/html/Zhou_End-to-End_Dense_Video_CVPR_2018_paper.html)][[PyTorch](https://github.com/salesforce/densecap)]
    * **BMT**: "A Better Use of Audio-Visual Cues: Dense Video Captioning with Bi-modal Transformer", BMVC, 2020 (*Tampere University, Finland*). [[Paper](https://arxiv.org/abs/2005.08271)][[PyTorch](https://github.com/v-iashin/bmt)][[Website](https://iashin.ai/bmt)]
    * **?**: "Optimizing Latency for Online Video Captioning Using Audio-Visual Transformers", Interspeech, 2021 (*MERL*). [[Paper](https://arxiv.org/abs/2108.02147)]
    * **PDVC**: "End-to-End Dense Video Captioning with Parallel Decoding", ICCV, 2021 (*HKU + Southern University of Science and Technology*). [[Paper](https://arxiv.org/abs/2108.07781)][[PyTorch](https://github.com/ttengwang/PDVC)]
    * **MV-GPT**: "End-to-end Generative Pretraining for Multimodal Video Captioning", CVPR, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2201.08264)]
    * **VGCL**: "Video-Guided Curriculum Learning for Spoken Video Grounding", ACMMM, 2022 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2209.00277)][[PyTorch](https://github.com/marmot-xy/Spoken-Video-Grounding)]
    * **UVC-VI**: "Aligning Source Visual and Target Language Domains for Unpaired Video Captioning", TPAMI, 2022 (*Peking University*). [[Paper](https://arxiv.org/abs/2211.12148)]
    * **D2**: "Dual-Level Decoupled Transformer for Video Captioning", arXiv, 2022 (*Northwestern Polytechnical University, China*). [[Paper](https://arxiv.org/abs/2205.03039)]
    * **VASTA**: "Diverse Video Captioning by Adaptive Spatio-temporal Attention", arXiv, 2022 (*University of Tubingen, Germany*). [[Paper](https://arxiv.org/abs/2208.09266)]
    * **VCRN**: "Visual Commonsense-aware Representation Network for Video Captioning", arXiv, 2022 (*University of Electronic Science and Technology of China (UESTC)*). [[Paper](https://arxiv.org/abs/2211.09469)][[PyTorch (in construction)](https://github.com/zchoi/VCRN)]
    * **RSFD**: "Refined Semantic Enhancement towards Frequency Diffusion for Video Captioning", arXiv, 2022 (*Wuhan University of Technology*). [[Paper](https://arxiv.org/abs/2211.15076)][[Code (in construction)](https://github.com/lzp870/RSFD)]
    * **VLTinT**: "VLTinT: Visual-Linguistic Transformer-in-Transformer for Coherent Video Paragraph Captioning", AAAI, 2023 (*University of Arkansas*). [[Paper](https://arxiv.org/abs/2211.15103)]
    * **Vid2Seq**: "Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2302.14115)][[Website](https://antoyang.github.io/vid2seq.html)]
    * **TextKG**: "Text with Knowledge Graph Augmented Transformer for Video Captioning", CVPR, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2303.12423)]
    * **G2L**: "G2L: Semantically Aligned and Uniform Video Grounding via Geodesic and Game Theory", ICCV, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2307.14277)]
    * **CoCap**: "Accurate and Fast Compressed Video Captioning", ICCV, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2309.12867)][[PyTorch](https://github.com/acherstyx/CoCap)]
    * **Movie101**: "Movie101: A New Movie Understanding Benchmark", ACL, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2305.12140)][[Code (in construction)](https://github.com/yuezih/Movie101)]
    * **VidChapters-7M**: "VidChapters-7M: Video Chapters at Scale", NeurIPS (Datasets and Benchmarks), 2023 (*INRIA*). [[Paper](https://arxiv.org/abs/2309.13952)][[PyTorch](https://github.com/antoyang/VidChapters)][[Website](https://antoyang.github.io/vidchapters.html)]
    * **?**: "Implicit and Explicit Commonsense for Multi-sentence Video Captioning", arXiv, 2023 (*UBC*). [[Paper](https://arxiv.org/abs/2303.07545)]
    * **Video-Verbalization**: "A Video Is Worth 4096 Tokens: Verbalize Story Videos To Understand Them In Zero Shot", arXiv, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2305.09758)]
    * **Dense-VOC**: "Dense Video Object Captioning from Disjoint Supervision", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2306.11729)]
    * **?**: "Exploring the Role of Audio in Video Captioning", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2306.12559)]
    * **ZeroTA**: "Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment", arXiv, 2023 (*KAIST*). [[Paper](https://arxiv.org/abs/2307.02682)]
    * **Video-CSR**: "Video-CSR: Complex Video Digest Creation for Visual-Language Models", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2310.05060)]
    * **SCG-SP**: "Set Prediction Guided by Semantic Concepts for Diverse Video Captioning", AAAI, 2024 (*CAS*). [[Paper](https://arxiv.org/abs/2312.15720)]
    * **EgoExoNCE**: "Retrieval-Augmented Egocentric Video Captioning", arXiv, 2024 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2401.00789)]
    * **Video-ReCap**: "Video ReCap: Recursive Captioning of Hour-Long Videos", arXiv, 2024 (*UNC*). [[Paper](https://arxiv.org/abs/2402.13250)][[PyTorch](https://github.com/md-mohaiminul/VideoRecap)][[Website](https://sites.google.com/view/vidrecap)]
* 3D:
    * **Vote2Cap-DETR**: "End-to-End 3D Dense Captioning with Vote2Cap-DETR", CVPR, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2301.02508)][[PyTorch](https://github.com/ch3cook-fdu/Vote2Cap-DETR)]
    * **Cap3D**: "Scalable 3D Captioning with Pretrained Models", NeurIPS, 2023 (*UMich*). [[Paper](https://arxiv.org/abs/2306.07279)][[Dataset](https://huggingface.co/datasets/tiange/Cap3D)]
    * **Vote2Cap-DETR++**: "Vote2Cap-DETR++: Decoupling Localization and Describing for End-to-End 3D Dense Captioning", arXiv, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2309.02999)][[PyTorch](https://github.com/ch3cook-fdu/Vote2Cap-DETR)]
* Others:
    * **ET-Cap**: "Explore and Tell: Embodied Visual Captioning in 3D Environments", ICCV, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2308.10447)][[PyTorch](https://github.com/HAWLYQ/ET-Cap)][[Website](https://aim3-ruc.github.io/ExploreAndTell/)]

[[Back to Overview](#overview)]

### Visual Question Answering
* General:
    * **MCAN**: "Deep Modular Co-Attention Networks for Visual Question Answering", CVPR, 2019 (*Hangzhou Dianzi University*). [[Paper](https://arxiv.org/abs/1906.10770)][[PyTorch](https://github.com/MILVLG/mcan-vqa)]
    * **M4C**: "Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA", CVPR, 2020 (*Facebook*). [[Paper](https://arxiv.org/abs/1911.06258)]
    * **SA-M4C**: "Spatially Aware Multimodal Transformers for TextVQA", ECCV, 2020 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2007.12146)][[PyTorch](https://github.com/yashkant/sam-textvqa)][[Website](https://yashkant.github.io/projects/sam-textvqa.html)]
    * **ConClaT**: "Contrast and Classify: Training Robust VQA Models", ICCV, 2021 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2010.06087)]
    * **TRAR**: "TRAR: Routing the Attention Spans in Transformer for Visual Question Answering", ICCV, 2021 (*Xiamen University*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Zhou_TRAR_Routing_the_Attention_Spans_in_Transformer_for_Visual_Question_ICCV_2021_paper.html)]
    * **UniQer**: "Unified Questioner Transformer for Descriptive Question Generation in Goal-Oriented Visual Dialogue", ICCV, 2021 (*Keio*). [[Paper](https://arxiv.org/abs/2106.15550)]
    * **TxT**: "TxT: Crossmodal End-to-End Learning with Transformers", GCPR, 2021 (*TU Darmstadt*). [[Paper](https://arxiv.org/abs/2109.04422)]
    * **ProTo**: "ProTo: Program-Guided Transformer for Program-Guided Tasks", NeurIPS, 2021 (*Georiga Tech*). [[Paper](https://arxiv.org/abs/2110.00804)]
    * **VisQA**: "VisQA: X-raying Vision and Language Reasoning in Transformers", arXiv, 2021 (*INSA-Lyon*). [[Paper](https://arxiv.org/abs/2104.00926)][[PyTorch](https://github.com/Theo-Jaunet/VisQA)]
    * **Block-Skim**: "Block-Skim: Efficient Question Answering for Transformer", AAAI, 2022 (* Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2112.08560)]
    * **RelViT**: "RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning", ICLR, 2022 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2204.11167)] [[PyTorch](https://github.com/NVlabs/RelViT)]
    * **Hypergraph-Transformer**: "Hypergraph Transformer: Weakly-supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering", ACL, 2022 (*SNU*). [[Paper](https://arxiv.org/abs/2204.10448)][[Code (in construction)](https://github.com/yujungheo/kbvqa-public)]
    * **X-Trans2Cap**: "X-Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning", CVPR, 2022 (*CUHK*). [[Paper](https://arxiv.org/abs/2203.00843)]
    * **UTC**: "UTC: A Unified Transformer with Inter-Task Contrastive Learning for Visual Dialog", CVPR, 2022 (*Fudan*). [[Paper](https://arxiv.org/abs/2205.00423)]
    * **LaTr**: "LaTr: Layout-Aware Transformer for Scene-Text VQA", CVPR, 2022 (*Amazon*). [[Paper](https://arxiv.org/abs/2112.12494)]
    * **QAA**: "Query and Attention Augmentation for Knowledge-Based Explainable Reasoning", CVPR, 2022 (*University of Minnesota*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Query_and_Attention_Augmentation_for_Knowledge-Based_Explainable_Reasoning_CVPR_2022_paper.html)][[PyTorch](https://github.com/SuperJohnZhang/QAA)]
    * **WebQA**: "WebQA: Multihop and Multimodal QA", CVPR, 2022 (*CMU + Microsoft*). [[Paper](https://arxiv.org/abs/2109.00590)][[PyTorch](https://github.com/WebQnA/WebQA_Baseline)][[Website](https://webqna.github.io/)]
    * **?**: "Efficient Adaptive Image-Language Learning for Visual Question Answering", CVPRW, 2022 (*Google*). [[Paper](https://drive.google.com/file/d/1SPeCqJ_Uzs_jk4yxxcSS8OOUKZmXf_Mt/view)]
    * **cViL**: "cViL: Cross-Lingual Training of Vision-Language Models using Knowledge Distillation", ICPR, 2022 (*IIIT, Hyderabad*). [[Paper](https://arxiv.org/abs/2206.03354)]
    * **Distinguishing-VQA**: "Overcoming Language Priors in Visual Question Answering via Distinguishing Superficially Similar Instances", COLING, 2022 (*Nankai University*). [[Paper](https://arxiv.org/abs/2209.08529)][[Code (in construction)](https://github.com/wyk-nku/Distinguishing-VQA)]
    * **?**: "Weakly Supervised Grounding for VQA in Vision-Language Transformers", ECCV, 2022 (*UCF*). [[Paper](https://arxiv.org/abs/2207.02334)][[PyTorch (in construction)](https://github.com/aurooj/WSG-VQA-VLTransformers)]
    * **MUST-VQA**: "MUST-VQA: MUltilingual Scene-text VQA", ECCVW, 2022 (*UAB, Spain*). [[Paper](https://arxiv.org/abs/2209.06730)]
    * **?**: "Training Vision-Language Models with Less Bimodal Supervision", Automated Knowledge Base Construction (AKBC), 2022 (*Tel Aviv*). [[Paper](https://arxiv.org/abs/2211.00262)]
    * **REVIVE**: "REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering", NeurIPS, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2206.01201)]
    * **ScienceQA**: "Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering", NeurIPS, 2022 (*AI2*). [[Paper](https://arxiv.org/abs/2209.09513)][[PyTorch](https://github.com/lupantech/ScienceQA)][[Website](https://scienceqa.github.io/)]
    * **FrozenBiLM**: "Zero-Shot Video Question Answering via Frozen Bidirectional Language Models", NeurIPS, 2022 (*INRIA*). [[Paper](https://arxiv.org/abs/2206.08155)][[PyTorch](https://github.com/antoyang/FrozenBiLM)]
    * **MuRAG**: "MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text", EMNLP, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2210.02928)]
    * **MMBS**: "Towards Robust Visual Question Answering: Making the Most of Biased Samples via Contrastive Learning", EMNLP, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2210.04563)][[PyTorch](https://github.com/PhoebusSi/MMBS)]
    * **EnFoRe**: "Entity-Focused Dense Passage Retrieval for Outside-Knowledge Visual Question Answering", EMNLP, 2022 (*UT Austin*). [[Paper](https://arxiv.org/abs/2210.10176)]
    * **CRIPP-VQA**: "CRIPP-VQA: Counterfactual Reasoning about Implicit Physical Properties via Video Question Answering", EMNLP, 2022 (*Arizona State University*). [[Paper](https://arxiv.org/abs/2211.03779)][[PyTorch](https://github.com/Maitreyapatel/CRIPP-VQA/)][[Website](https://maitreyapatel.com/CRIPP-VQA/)]
    * **PnP-VQA**: "Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training", EMNLP Findings, 2022 (*Salesforce*). [[Paper](https://arxiv.org/abs/2210.08773)]
    * **TMN**: "Transformer Module Networks for Systematic Generalization in Visual Question Answering", arXiv, 2022 (*Fujitsu*). [[Paper](https://arxiv.org/abs/2201.11316)]
    * **?**: "On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering", arXiv, 2022 (*Birla Institute of Technology Mesra, India*). [[Paper](https://arxiv.org/abs/2201.03965)]
    * **DST**: "Towards Efficient and Elastic Visual Question Answering with Doubly Slimmable Transformer", arXiv, 2022 (*Hangzhou Dianzi University*). [[Paper](https://arxiv.org/abs/2203.12814)]
    * **PAVCR**: "Attention Mechanism based Cognition-level Scene Understanding", arXiv, 2022 (*Leibniz University of Hannover, Germany*). [[Paper](https://arxiv.org/abs/2204.08027)]
    * **TAG**: "TAG: Boosting Text-VQA via Text-aware Visual Question-answer Generation", arXiv, 2022 (*Maryland + Salesforce*). [[Paper](https://arxiv.org/abs/2208.01813)][[PyTorch](https://github.com/HenryJunW/TAG)]
    * **UniCon**: "UniCon: Unidirectional Split Learning with Contrastive Loss for Visual Question Answering", arXiv, 2022 (*University of Tokyo*). [[Paper](https://arxiv.org/abs/2208.11435)]
    * **CLOVE**: "Symbolic Replay: Scene Graph as Prompt for Continual Learning on VQA Task", arXiv, 2022 (*NUS*). [[Paper](https://arxiv.org/abs/2208.12037)][[Code (in construction)](https://github.com/showlab/CLVQA)]
    * **mVQA**: "Towards Multi-Lingual Visual Question Answering", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2209.05401)]
    * **CIB**: "Finetuning Pretrained Vision-Language Models with Correlation Information Bottleneck for Robust Visual Question Answering", arXiv, 2022 (*Xi'an Jiaotong University*). [[Paper](https://arxiv.org/abs/2209.06954)]
    * **?**: "Compressing And Debiasing Vision-Language Pre-Trained Models for Visual Question Answering", arXiv, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2210.14558)]
    * **VLR**: "Visually Grounded VQA by Lattice-based Retrieval", arXiv, 2022 (*University of Bremen, Germany*). [[Paper](https://arxiv.org/abs/2211.08086)]
    * **CMCL**: "Cross-Modal Contrastive Learning for Robust Reasoning in VQA", arxiv, 2022 (*University of Sydney*). [[Paper](https://arxiv.org/abs/2211.11190)][[PyTorch](https://github.com/qizhust/cmcl_vqa_pl)]
    * **CL-CrossVQA**: "CL-CrossVQA: A Continual Learning Benchmark for Cross-Domain Visual Question Answering", arXiv, 2022 (*LMU Munich*). [[Paper](https://arxiv.org/abs/2211.10567)]
    * **OFA-X**: "Harnessing the Power of Multi-Task Pretraining for Ground-Truth Level Natural Language Explanations", arXiv, 2022 (*University of Hamburg, Germany*). [[Paper](https://arxiv.org/abs/2212.04231)][[Code (in construction)](https://github.com/ofa-x/OFA-X)]
    * **VLC-BERT**: "VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge", WACV, 2023 (*UBC, Canada*). [[Paper](https://arxiv.org/abs/2210.13626)][[PyTorch](https://github.com/aditya10/VLC-BERT)]
    * **LTG**: "Locate Then Generate: Bridging Vision and Language with Bounding Box for Scene-Text VQA", AAAI, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2304.01603)]
    * **SelTDA**: "Q: How to Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!", CVPR, 2023 (*NEC*). [[Paper](https://arxiv.org/abs/2306.03932)][[PyTorch](https://github.com/codezakh/SelTDA)]
    * **Prophet**: "Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering", CVPR, 2023 (*Hangzhou Dianzi University*). [[Paper](https://arxiv.org/abs/2303.01903)][[PyTorch](https://github.com/MILVLG/prophet)]
    * **GenB**: "Generative Bias for Robust Visual Question Answering", CVPR, 2023 (*KAIST*). [[Paper](https://arxiv.org/abs/2208.00690)]
    * **MixPHM**: "MixPHM: Redundancy-Aware Parameter-Efficient Tuning for Low-Resource Visual Question Answering", CVPR, 2023 (*Xi'an Jiaotong University*). [[Paper](https://arxiv.org/abs/2303.01239)]
    * **POEM**: "Divide and Conquer: Answering Questions with Object Factorization and Compositional Reasoning", CVPR, 2023 (*University of Minnesota (UMN)*). [[Paper](https://arxiv.org/abs/2303.10482)][[PyTorch](https://github.com/szzexpoi/POEM)]
    * **LYP**: "Improving Selective Visual Question Answering by Learning From Your Peers", CVPR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2306.08751)]
    * **VQACL**: "VQACL: A Novel Visual Question Answering Continual Learning Setting", CVPR, 2023 (*CAS*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_VQACL_A_Novel_Visual_Question_Answering_Continual_Learning_Setting_CVPR_2023_paper.html)][[PyTorch](https://github.com/zhangxi1997/VQACL)]
    * **Img2LLM**: "From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models", CVPR, 2023 (*Salesforce*). [[Paper](https://arxiv.org/abs/2212.10846)][[PyTorch](https://github.com/salesforce/LAVIS/tree/main/projects/img2llm-vqa)]
    * **Imp-VQA**: "Logical Implications for Visual Question Answering Consistency", CVPR, 2023 (*University of Bern, Switzerland*). [[Paper](https://arxiv.org/abs/2303.09427)][[PyTorch](https://github.com/sergiotasconmorales/imp_vqa)][[Website](https://sergiotasconmorales.github.io/conferences/cvpr2023.html)]
    * **RMLVQA**: "RMLVQA: A Margin Loss Approach For Visual Question Answering with Language Biases", CVPR, 2023 (*Indian Institute of Science*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Basu_RMLVQA_A_Margin_Loss_Approach_for_Visual_Question_Answering_With_CVPR_2023_paper.html)][[PyTorch](https://github.com/val-iisc/RMLVQA)]
    * **S3C**: "S3C: Semi-Supervised VQA Natural Language Explanation via Self-Critical Learning", CVPR, 2023 (*Northwestern Polytechnical University, China*). [[Paper](https://arxiv.org/abs/2309.02155)]
    * **?**: "Diversifying Joint Vision-Language Tokenization Learning", CVPRW, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2306.03421)]
    * **VQAAnswerTherapy**: "VQA Therapy: Exploring Answer Differences by Visually Grounding Answers", ICCV, 2023 (*UT Austin*). [[Paper](https://arxiv.org/abs/2308.11662)][[Website](https://vizwiz.org/tasks-and-datasets/vqa-answer-therapy/)]
    * **WHOOP**: "Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images", ICCV, 2023 (*Ben Gurion University of the Negev, Israel*). [[Paper](https://arxiv.org/abs/2303.07274)][[Website](https://whoops-benchmark.github.io/)]
    * **Encyclopedic-VQA**: "Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories", ICCV, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2306.09224)][[Tensorflow](https://github.com/google-research/google-research/tree/master/encyclopedic_vqa)]
    * **RVQA**: "Toward Unsupervised Realistic Visual Question Answering", ICCV, 2023 (*UCSD*). [[Paper](https://arxiv.org/abs/2303.05068)]
    * **VQA-GNN**: "VQA-GNN: Reasoning with Multimodal Knowledge via Graph Neural Networks for Visual Question Answering", ICCV, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2205.11501)]
    * **ViTiS**: "Zero-Shot and Few-Shot Video Question Answering with Multi-Modal Prompts", ICCVW, 2023 (*INRIA*). [[Paper](https://arxiv.org/abs/2309.15915)][[Website](https://engindeniz.github.io/vitis)]
    * **TwO**: "Combo of Thinking and Observing for Outside-Knowledge VQA", ACL, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2305.06407)][[Code (in construction)](https://github.com/PhoebusSi/Thinking-while-Observing)]
    * **Mod-Zero-VQA**: "Modularized Zero-shot VQA with Pre-trained Models", ACL Findings, 2023 (*Singapore Management University*). [[Paper](https://arxiv.org/abs/2305.17369)]
    * **SaL**: "Separate and Locate: Rethink the Text in Text-based Visual Question Answering", ACMMM, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2308.16383)][[Code (in construction)](https://github.com/fangbufang/SaL)]
    * **ReVisE**: "From Wrong To Right: A Recursive Approach Towards Vision-Language Explanation", EMNLP, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2311.12391)][[Code (in construction)](https://github.com/para-lost/ReVisE)]
    * **Cola**: "Large Language Models are Visual Reasoning Coordinators", NeurIPS, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2310.15166)][[PyTorch](https://github.com/cliangyu/Cola)]
    * **AVIS**: "AVIS: Autonomous Visual Information Seeking with Large Language Model Agent", NeurIPS, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2306.08129)]
    * **?**: "Exploring Question Decomposition for Zero-Shot VQA", NeurIPS, 2023 (*Northeastern*). [[Paper](https://arxiv.org/abs/2310.17050)][[Code (in construction)](https://github.com/codezakh/decomposition-0shot-vqa)][[Website](https://zaidkhan.me/decomposition-0shot-vqa/)]
    * **SeeTRUE**: "What You See is What You Read? Improving Text-Image Alignment Evaluation", NeurIPS, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2305.10400)][[PyTorch](https://github.com/yonatanbitton/wysiwyr)][[Website](https://wysiwyr-itm.github.io/)]
    * **InfoSeek**: "Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2302.11713)][[Website](https://open-vision-language.github.io/infoseek/)]
    * **CoVGT**: "Contrastive Video Question Answering via Video Graph Transformer", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2302.13668)]
    * **IVLT**: "Causality-aware Visual Scene Discovery for Cross-Modal Question Reasoning", arXiv, 2023 (*Sun-Yat-Sen University*). [[Paper](https://arxiv.org/abs/2304.08083)]
    * **MGT**: "Multimodal Graph Transformer for Multimodal Question Answering", arXiv, 2023 (*UC Santa Cruz*). [[Paper](https://arxiv.org/abs/2305.00581)]
    * **VCSR**: "Visual Causal Scene Refinement for Video Question Answering", arXiv, 2023 (*Sun-Yat-Sen University*). [[Paper](https://arxiv.org/abs/2305.04224)]
    * **JADE**: "Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2305.11769)]
    * **NuScenes-QA**: "NuScenes-QA: A Multi-modal Visual Question Answering Benchmark for Autonomous Driving Scenario", arXiv, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2305.14836)][[Code (in construction)](https://github.com/qiantianwen/NuScenes-QA)]
    * **LAMOC**: "Zero-shot Visual Question Answering with Language Model Feedback", arXiv, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2305.17006)][[PyTorch](https://github.com/RUCAIBox/LAMOC)]
    * **PW-VQA**: "Unveiling Cross Modality Bias in Visual Question Answering: A Causal View with Possible Worlds VQA", arXiv, 2023 (*University of Rochester*). [[Paper](https://arxiv.org/abs/2305.19664)]
    * **?**: "Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering", arXiv, 2023 (*Mila*). [[Paper](https://arxiv.org/abs/2306.09996)]
    * **R2A**: "Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2306.11732)]
    * **WikiTiLo**: "Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times and Location Reasoning", arXiv, 2023 (*LMU Munich*). [[Paper](https://arxiv.org/abs/2307.06166)]
    * **GenVQA**: "Generative Visual Question Answering", arXiv, 2023 (*UW*). [[Paper](https://arxiv.org/abs/2307.10405)]
    * **Context-VQA**: "Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering", arXiv, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2307.15745)]
    * **BLIVA**: "BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions", arXiv, 2023 (*USCD*). [[Paper](https://arxiv.org/abs/2308.09936)]
    * **NExT-GQA**: "Can I Trust Your Answer? Visually Grounded Video Question Answering", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2309.01327)]
    * **CURE**: "Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models", arXiv, 2023 (*SRI*). [[Paper](https://arxiv.org/abs/2309.04461)][[Code (in construction)](https://github.com/Yangyi-Chen/CoTConsistency)]
    * **RepARe**: "Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models", arXiv, 2023 (*UNC*). [[Paper](https://arxiv.org/abs/2310.05861)][[PyTorch](https://github.com/archiki/RepARe)]
    * **RVP**: "Recursive Visual Programming", arXiv, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2312.02249)]
    * **SAB**: "Sentence Attention Blocks for Answer Grounding", arXiv, 2023 (*University of Delaware, Delaware*). [[Paper](https://arxiv.org/abs/2309.11593)]
    * **DIS**: "Detection-based Intermediate Supervision for Visual Question Answering", AAAI, 2024 (*Huazhong University of Science and Technology (HUST)*). [[Paper](https://arxiv.org/abs/2312.16012)]
    * **OAM-VQA**: "Object Attribute Matters in Visual Question Answering", AAAI, 2024 (*Jilin University*). [[Paper](https://arxiv.org/abs/2401.09442)]
    * **oVQA**: "Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy", ICLR, 2024 (*University of Freiburg, Germany*). [[Paper](https://arxiv.org/abs/2402.07270)][[PyTorch](https://github.com/lmb-freiburg/ovqa)]
    * **?**: "Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation", arXiv, 2024 (*University of Tokyo*). [[Paper](https://arxiv.org/abs/2401.10005)]
    * **MultipanelVQA**: "Muffin or Chihuahua? Challenging Large Vision-Language Models with Multipanel VQA", arXiv, 2024 (*eBay*). [[Paper](https://arxiv.org/abs/2401.15847)][[Website](https://sites.google.com/view/multipanelvqa/home)]
    * **Proximity-QA**: "Proximity QA: Unleashing the Power of Multi-Modal Large Language Models for Spatial Proximity Analysis", arXiv, 2024 (*Peking*). [[Paper](https://arxiv.org/abs/2401.17862)][[Code (in construction)](https://github.com/NorthSummer/ProximityQA)]
    * **SnapNTell**: "SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM", arXiv, 2024 (*Meta*). [[Paper](https://arxiv.org/abs/2403.04735)]
* Video:
    * **?**: "Mounting Video Metadata on Transformer-based Language Model for Open-ended Video Question Answering", arXiv, 2021 (*Seoul National University*). [[Paper](https://arxiv.org/abs/2108.05158)]
    * **TPT**: "Temporal Pyramid Transformer with Multimodal Interaction for Video Question Answering", arXiv, 2021 (*CAS*). [[Paper](https://arxiv.org/abs/2109.04735)]
    * **SwinBERT**: "SwinBERT: End-to-End Transformers with Sparse Attention for Video Captioning", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.13196)][[PyTorch](https://github.com/microsoft/SwinBERT)]
    * **WildQA**: "WildQA: In-the-Wild Video Question Answering", International Conference on Computational Linguistics (COLING), 2022 (*UMich*). [[Paper](https://arxiv.org/abs/2209.06650)][[Website](https://lit.eecs.umich.edu/wildqa/)]
    * **VGT**: "Video Graph Transformer for Video Question Answering", ECCV, 2022 (*Sea AI Lab*). [[Paper](https://arxiv.org/abs/2207.05342)][[PyTorch](https://github.com/sail-sg/VGT)]
    * **?**: "Video Question Answering with Iterative Video-Text Co-Tokenization", ECCV, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2208.00934)][[Website (in construction)](https://sites.google.com/view/videoqa-cotokenization)]
    * **DeST**: "Learning Fine-Grained Visual Understanding for Video Question Answering via Decoupling Spatial-Temporal Modeling", BMVC, 2022 (*NTU*). [[Paper](https://arxiv.org/abs/2210.03941)][[PyTorch](https://github.com/shinying/dest)]
    * **ViteVQA**: "Towards Video Text Visual Question Answering: Benchmark and Baseline", NeurIPS, 2022 (*ByteDance*). [[Paper](https://openreview.net/forum?id=yPZ7w29qSNK)][[GitHub](https://github.com/bytedance/VTVQA)]
    * **WSQG**: "Frame-Subtitle Self-Supervision for Multi-Modal Video Question Answering", arXiv, 2022 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2209.03609)]
    * **LocAns**: "Locate before Answering: Answer Guided Question Localization for Video Question Answering", arXiv, 2022 (*Fudan University*). [[Paper](https://arxiv.org/abs/2210.02081)]
    * **NewsVideoQA**: "Watching the News: Towards VideoQA Models that can Read", arXiv, 2022 (*IIIT Hyderabad, India*). [[Paper](https://arxiv.org/abs/2211.05588)]
    * **SHG-VQA**: "Learning Situation Hyper-Graphs for Video Question Answering", CVPR, 2023 (*UCF*). [[Paper](https://arxiv.org/abs/2304.08682)][[PyTorch](https://github.com/aurooj/SHG-VQA)]
    * **ANetQA**: "ANetQA: A Large-scale Benchmark for Fine-grained Compositional Reasoning over Untrimmed Videos", CVPR, 2023 (*Hangzhou Dianzi University*). [[Paper](https://arxiv.org/abs/2305.02519)][[Website](https://milvlg.github.io/anetqa/)]
    * **MCR**: "Discovering the Real Association: Multimodal Causal Reasoning in Video Question Answering", CVPR, 2023 (*Beijing Institute of Technology*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zang_Discovering_the_Real_Association_Multimodal_Causal_Reasoning_in_Video_Question_CVPR_2023_paper.html)][[Code (in construction)](https://github.com/Chuanqi-Zang/Discovering-the-Real-Association)]
    * **MIST**: "MIST: Multi-modal Iterative Spatial-Temporal Transformer for Long-form Video Question Answering", CVPR, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2212.09522)][[PyTorch](https://github.com/showlab/mist)]
    * **CaKE-LM**: "Language Models are Causal Knowledge Extractors for Zero-shot Video Question Answering", CVPRW, 2023 (*NTU + Columbia*). [[Paper](https://arxiv.org/abs/2304.03754)]
    * **TransSTR**: "Discovering Spatio-Temporal Rationales for Video Question Answering", ICCV, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2307.12058)]
    * **Tem-adapter**: "Tem-adapter: Adapting Image-Text Pretraining for Video Question Answer", ICCV, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2308.08414)][[PyTorch](https://github.com/XLiu443/Tem-adapter)]
    * **OVQA**: "Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models", ICCV, 2023 (*Korea University*). [[Paper](https://arxiv.org/abs/2308.09363)][[PyTorch](https://github.com/mlvlab/OVQA)]
    * **RaFormer**: "Redundancy-aware Transformer for Video Question Answering", ACMMM, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2308.03267)]
    * **LSS**: "Long Story Short: a Summarize-then-Search Method for Long Video Question Answering", BMVC, 2023 (*Yonsei University*). [[Paper](https://arxiv.org/abs/2311.01233)]
    * **Flipped-VQA**: "Large Language Models are Temporal and Causal Reasoners for Video Question Answering", EMNLP, 2023 (*Korea University*). [[Paper](https://arxiv.org/abs/2310.15747)][[Code (in construction)](https://github.com/mlvlab/Flipped-VQA)]
    * **SeViLA**: "Self-Chained Image-Language Model for Video Localization and Question Answering", NeurIPS, 2023 (*UNC*). [[Paper](https://arxiv.org/abs/2305.06988)][[PyTorch](https://github.com/Yui010206/SeViLA)]
    * **Glance-Focus**: "Glance and Focus: Memory Prompting for Multi-Event Video Question Answering", NeurIPS, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2401.01529)][[PyTorch](https://github.com/ByZ0e/Glance-Focus)]
    * **FunQA**: "FunQA: Towards Surprising Video Comprehension", arXiv, 2023 (*Beijing University of Posts and Telecommunication*). [[Paper](https://arxiv.org/abs/2306.14899)][[Code (in construction)](https://github.com/Jingkang50/FunQA)][[Website](https://funqa-benchmark.github.io/)]
    * **ProViQ**: "Zero-Shot Video Question Answering with Procedural Programs", arXiv, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2312.00937)][[Website](https://rccchoudhury.github.io/proviq2023/)]
    * **R-VLM**: "Retrieval-based Video Language Model for Efficient Long Video Question Answering", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2312.04931)]
    * **MoVQA**: "MoVQA: A Benchmark of Versatile Question-Answering for Long-Form Movie Understanding", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2312.04817)][[Website](https://movqa.github.io/)]
    * **GroundVQA**: "Grounded Question-Answering in Long Egocentric Videos", arXiv, 2023 (*SJTU*). [[Paper](https://arxiv.org/abs/2312.06505)]
    * **VLAP**: "VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering", arXiv, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2312.08367)]
    * **Vista-LLaMA**: "Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens", arXiv, 2023 (*Zhejiang*). [[Paper](https://arxiv.org/abs/2312.08870)][[Website](https://jinxxian.github.io/Vista-LLaMA/)]
    * **LLoVi**: "An Improved Baseline for Reasoning Segmentation with Large Language Model", arXiv, 2023 (*UNC*). [[Paper](https://arxiv.org/abs/2312.17240)][[Code (in construction)](https://github.com/CeeZh/LLoVi)]
    * **STAIR**: "STAIR: Spatial-Temporal Reasoning with Auditable Intermediate Results for Video Question Answering", AAAI, 2024 (*Peking*). [[Paper](https://arxiv.org/abs/2401.03901)]
    * **YTCommentQA**: "YTCommentQA: Video Question Answerability in Instructional Videos", AAAI, 2024 (*LG*). [[Paper](https://arxiv.org/abs/2401.17343)][[Code (in construction)](https://github.com/lgresearch/YTCommentQA)]
    * **Sports-QA**: "Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex and Professional Sports", arXiv, 2024 (*University of Melbourne*). [[Paper](https://arxiv.org/abs/2401.01505)]
    * **DoraemonGPT**: "DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models", arXiv, 2024 (*Zhejiang*). [[Paper](https://arxiv.org/abs/2401.08392)][[Code (in construction)](https://github.com/z-x-yang/DoraemonGPT)]
    * **Q-ViD**: "Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering", arXiv, 2024 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2402.10698)]
    * **LSTP**: "LSTP: Language-guided Spatial-Temporal Prompt Learning for Long-form Video-Text Understanding", arXiv, 2024 (*BIGAI*). [[Paper](https://arxiv.org/abs/2402.16050)][[PyTorch](https://github.com/bigai-nlco/LSTP-Chat)]
    * **DAM**: "DAM: Dynamic Adapter Merging for Continual Video QA Learning", arXiv, 2024 (*UNC*). [[Paper](https://arxiv.org/abs/2403.08755)][[Code (in construction)](https://github.com/klauscc/DAM)]
* 3D:
    * **3D-VQA**: "CLIP-Guided Vision-Language Pre-training for Question Answering in 3D Scenes", CVPRW, 2023 (*ETHZ*). [[Paper](https://arxiv.org/abs/2304.06061)][[Code (in construction)](https://github.com/AlexDelitzas/3D-VQA)]
    * **PO3D-VQA**: "3D-Aware Visual Question Answering about Parts, Poses and Occlusions", NeurIPS, 2023 (*JHU*). [[Paper](https://arxiv.org/abs/2310.17914)][[Code (in construction)](https://github.com/XingruiWang/3D-Aware-VQA)]
    * **Multi-CLIP**: "Multi-CLIP: Contrastive Vision-Language Pre-training for Question Answering tasks in 3D Scenes", arXiv, 2023 (*ETHZ*). [[Paper](https://arxiv.org/abs/2306.02329)]
    * **Gen3DQA**: "Generating Context-Aware Natural Answers for Questions in 3D Scenes", arXiv, 2023 (*TUM*). [[Paper](https://arxiv.org/abs/2310.19516)][[Code (in construction)](https://github.com/MunzerDw/Gen3DQA)]
    * **BridgeQA**: "Bridging the Gap between 2D and 3D Visual Question Answering: A Fusion Approach for 3D VQA", AAAI, 2024 (*Peking*). [[Paper](https://arxiv.org/abs/2402.15933)][[PyTorch](https://github.com/matthewdm0816/BridgeQA)]
* Audio-Visual:
    * **PSTP-Net**: "Progressive Spatio-temporal Perception for Audio-Visual Question Answering", ACMMM, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2308.05421)][[PyTorch](https://github.com/GeWu-Lab/PSTP-Net)]

[[Back to Overview](#overview)]

### Visual Grounding
* General:
    * **TransRefer3D**: "TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding", ACMMM, 2021 (*Beihang University*). [[Paper](https://arxiv.org/abs/2108.02388)]
    * **?**: "Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers", EMNLP, 2021 (*University of Trento*). [[Paper](https://arxiv.org/abs/2109.04448)]
    * **MITVG**: "Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation", ACL Findings, 2021 (*Tencent*). [[Paper](https://arxiv.org/abs/2109.08478)]
    * **TransVG**: "TransVG: End-to-End Visual Grounding with Transformers", ICCV, 2021 (*USTC*). [[Paper](https://arxiv.org/abs/2104.08541)]
    * **GSRTR**: "Grounded Situation Recognition with Transformers", BMVC, 2021 (*POSTECH*). [[Paper](https://arxiv.org/abs/2111.10135)][[PyTorch](https://github.com/jhcho99/gsrtr)]
    * **Referring-Transformer**: "Referring Transformer: A One-step Approach to Multi-task Visual Grounding", NeurIPS, 2021 (*UBC*). [[Paper](https://arxiv.org/abs/2106.03089)]
    * **VGTR**: "Visual Grounding with Transformers", arXiv, 2021 (*Beihang University*). [[Paper](https://arxiv.org/abs/2105.04281)]
    * **UNICORN**: "Crossing the Format Boundary of Text and Boxes: Towards Unified Vision-Language Modeling", arXiv, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.12085)]
    * **Word2Pix**: "Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding", arXiv, 2021 (*A\*STAR*). [[Paper](https://arxiv.org/abs/2108.00205)]
    * **CoFormer**: "Collaborative Transformers for Grounded Situation Recognition", CVPR, 2022 (*POSTECH*). [[Paper](https://arxiv.org/abs/2203.16518)][[PyTorch](https://github.com/jhcho99/CoFormer)]
    * **MVT**: "Multi-View Transformer for 3D Visual Grounding", CVPR, 2022 (*CUHK*). [[Paper](https://arxiv.org/abs/2204.02174)][[PyTorch](https://github.com/sega-hsj/MVT-3DVG)]
    * **GLIP**: "Grounded Language-Image Pre-training", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2112.03857)][[PyTorch](https://github.com/microsoft/GLIP)]
    * **M-DGT**: "Multi-Modal Dynamic Graph Transformer for Visual Grounding", CVPR, 2022 (*University of Toronto*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Chen_Multi-Modal_Dynamic_Graph_Transformer_for_Visual_Grounding_CVPR_2022_paper.html)][[PyTorch](https://github.com/iQua/M-DGT)]
    * **QRNet**: "Shifting More Attention to Visual Backbone: Query-modulated Refinement Networks for End-to-End Visual Grounding", CVPR, 2022 (*East China Normal University*). [[Paper](https://arxiv.org/abs/2203.15442)][[PyTorch](https://github.com/LukeForeverYoung/QRNet)]
    * **SiRi**: "SiRi: A Simple Selective Retraining Mechanism for Transformer-based Visual Grounding", ECCV, 2022 (*JD*). [[Paper](https://arxiv.org/abs/2207.13325)][[PyTorch](https://github.com/qumengxue/siri-vg)]
    * **UniTAB**: "UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling", ECCV, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.12085)]
    * **TAP**: "Improving Closed and Open-Vocabulary Attribute Prediction Using Transformers", ECCV, 2022 (*Adobe*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5247_ECCV_2022_paper.php)][[GitHub](https://github.com/adobe-research/vaw_dataset)][[Website](https://vkhoi.github.io/TAP/)]
    * **YORO**: "YORO - Lightweight End to End Visual Grounding", ECCVW, 2022 (*Amazon*). [[Paper](https://arxiv.org/abs/2211.07912)]
    * **GLIPv2**: "GLIPv2: Unifying Localization and Vision-Language Understanding", NeurIPS, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2206.05836)][[PyTorch](https://github.com/microsoft/GLIP)]
    * **?**: "Do Vision-and-Language Transformers Learn Grounded Predicate-Noun Dependencies?", EMNLP, 2022 (*Aix-Marseille University, France*). [[Paper](https://arxiv.org/abs/2210.12079)]
    * **SeqTR**: "SeqTR: A Simple yet Universal Network for Visual Grounding", arXiv, 2022 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2203.16265)][[Code (in construction)](https://github.com/sean-zhuh/SeqTR)]
    * **TransVG++**: "TransVG++: End-to-End Visual Grounding with Language Conditioned Vision Transformer", arXiv, 2022 (*USTC*). [[Paper](https://arxiv.org/abs/2206.06619)]
    * **HLGT**: "Hierarchical Local-Global Transformer for Temporal Sentence Grounding", arXiv, 2022 (*Huazhong University of Science and Technology*). [[Paper](https://arxiv.org/abs/2208.14882)]
    * **Dynamic-MDETR**: "Dynamic MDETR: A Dynamic Multimodal Transformer Decoder for Visual Grounding", arXiv, 2022 (*Nanjing University*). [[Paper](https://arxiv.org/abs/2209.13959)]
    * **ClipCrop**: "ClipCrop: Conditioned Cropping Driven by Vision-Language Model", arXiv, 2022 (*The University of Tokyo*). [[Paper](https://arxiv.org/abs/2211.11492)]
    * **VL-MPAG-Net**: "Grounding Scene Graphs on Natural Images via Visio-Lingual Message Passing", WACV, 2023 (*Indian Institute of Science*). [[Paper](https://arxiv.org/abs/2211.01969)][[PyTorch](https://github.com/IISCAditayTripathi/Scene-graph-localization)][[Website](https://iiscaditaytripathi.github.io/sgl/)]
    * **CLEVER**: "Visually Grounded Commonsense Knowledge Acquisition", AAAI, 2023 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2211.12054)][[PyTorch](https://github.com/thunlp/CLEVER)]
    * **LADS**: "Referring Expression Comprehension Using Language Adaptive Inference", AAAI, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2306.04451)]
    * **?**: "Learning to Jointly Share and Prune Weights for Grounding Based Vision and Language Models", ICLR, 2023 (*Samsung*). [[Paper](https://openreview.net/forum?id=UMERaIHMwB3)]
    * **AMC**: "Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations", CVPR, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2206.15462)][[PyTorch](https://github.com/uvavision/AMC-grounding)][[Website](https://vislang.ai/amc)]
    * **CounTEX**: "Grounding Counterfactual Explanation of Image Classifiers to Textual Concept Space", CVPR, 2023 (*Amazon*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Kim_Grounding_Counterfactual_Explanation_of_Image_Classifiers_to_Textual_Concept_Space_CVPR_2023_paper.html)]
    * **SK-VG**: "Advancing Visual Grounding with Scene Knowledge: Benchmark and Method", CVPR, 2023 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2307.11558)][[Code (in construction)](https://github.com/zhjohnchan/SK-VG)]
    * **D-ViTMDETR**: "Dynamic Inference with Grounding Based Vision and Language Models", CVPR, 2023 (*Amazon*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Uzkent_Dynamic_Inference_With_Grounding_Based_Vision_and_Language_Models_CVPR_2023_paper.html)]
    * **?**: "Similarity Maps for Self-Training Weakly-Supervised Phrase Grounding", CVPR, 2023 (*Tel Aviv*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Shaharabany_Similarity_Maps_for_Self-Training_Weakly-Supervised_Phrase_Grounding_CVPR_2023_paper.html)][[Code (in construction)](https://github.com/talshaharabany/Similarity-Maps-for-Self-Training-Weakly-Supervised-Phrase-Grounding)]
    * **RefCLIP**: "RefCLIP: A Universal Teacher for Weakly Supervised Referring Expression Comprehension", CVPR, 2023 (*Xiamen University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Jin_RefCLIP_A_Universal_Teacher_for_Weakly_Supervised_Referring_Expression_Comprehension_CVPR_2023_paper.html)][[PyTorch](https://github.com/kingthreestones/RefCLIP)][[Website](https://refclip.github.io/)]
    * **FROMAGe**: "Grounding Language Models to Images for Multimodal Inputs and Outputs", ICML, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2301.13823)][[PyTorch](https://github.com/kohjingyu/fromage)][[Website](https://jykoh.com/fromage)]
    * **IR-VG**: "Iterative Robust Visual Grounding with Masked Reference based Centerpoint Supervision", ICCV, 2023 (*Beihang*). [[Paper](https://arxiv.org/abs/2307.12392)][[Code (in construction)](https://github.com/cv516Buaa/IR-VG)]
    * **RefEgo**: "RefEgo: Referring Expression Comprehension Dataset from First-Person Perception of Ego4D", ICCV, 2023 (*RIKEN*). [[Paper](https://arxiv.org/abs/2308.12035)]
    * **SLAN**: "SLAN: Self-Locator Aided Network for Vision-Language Understanding", ICCV, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2211.16208)][[Code (in construction)](https://github.com/scok30/SLAN)]
    * **GITM-MR**: "Grounded Image Text Matching with Mismatched Relation Reasoning", ICCV, 2023 (*ShanghaiTech*). [[Paper](https://arxiv.org/abs/2308.01236)]
    * **DOD**: "Described Object Detection: Liberating Object Detection with Flexible Expressions", NeurIPS, 2023 (*Tongji University*). [[Paper](https://arxiv.org/abs/2307.12813)][[PyTorch](https://github.com/shikras/d-cube)]
    * **CLIP-VG**: "CLIP-VG: Self-paced Curriculum Adapting of CLIP via Exploiting Pseudo-Language Labels for Visual Grounding", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2305.08685)][[Code (in construction)](https://github.com/linhuixiao/CLIP-VG)]
    * **TreePrompt**: "TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding", arXiv, 2023 (*HKUST*). [[Paper](https://arxiv.org/abs/2305.11497)]
    * **OctoBERT**: "World-to-Words: Grounded Open Vocabulary Acquisition through Fast Mapping in Vision-Language Models", arXiv, 2023 (*UMich*). [[Paper](https://arxiv.org/abs/2306.08685)]
    * **BuboGPT**: "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2307.08581)][[PyTorch](https://github.com/magic-research/bubogpt)][[Website](https://bubo-gpt.github.io/)]
    * **LG-DVG**: "Language-Guided Diffusion Model for Visual Grounding", arXiv, 2023 (*University of Toronto*). [[Paper](https://arxiv.org/abs/2308.09599)]
    * **VGDiffZero**: "VGDiffZero: Text-to-image Diffusion Models Can Be Zero-shot Visual Grounders", arXiv, 2023 (*Westlake University, China*). [[Paper](https://arxiv.org/abs/2309.01141)]
    * **GREC**: "GREC: Generalized Referring Expression Comprehension", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2308.16182)][[Website](https://henghuiding.github.io/GRES/)]
    * **SoM**: "Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2310.11441)][[Code (in construction)](https://github.com/microsoft/SoM)][[Website](https://som-gpt4v.github.io/)]
    * **GLaMM**: "GLaMM: Pixel Grounding Large Multimodal Model", arXiv, 2023 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2311.03356)][[Code (in construction)](https://github.com/mbzuai-oryx/groundingLMM)][[Website](https://mbzuai-oryx.github.io/groundingLMM/)]
    * **Griffon**: "Griffon: Spelling out All Object Locations at Any Granularity with Large Language Models", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2311.14552)]
    * **RelVLA**: "Zero-shot Referring Expression Comprehension via Structural Similarity Between Images and Captions", arXiv, 2023 (*Northeastern*). [[Paper](https://arxiv.org/abs/2311.17048)]
    * **Lenna**: "Lenna: Language Enhanced Reasoning Detection Assistant", arXiv, 2023 (*Meituan*). [[Paper](https://arxiv.org/abs/2312.02433)][[Code (in construction)](https://github.com/Meituan-AutoML/Lenna)]
    * **LLaVA-Grounding**: "LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2312.02949)][[PyTorch](https://github.com/UX-Decoder/LLaVA-Grounding)][[Website](https://llava-vl.github.io/llava-grounding/)]
    * **SeflEQ**: "Improved Visual Grounding through Self-Consistent Explanations", arXiv, 2023 (*Rice*). [[Paper](https://arxiv.org/abs/2312.04554)][[Website](https://catherine-r-he.github.io/SelfEQ/)]
    * **OV-VG**: "OV-VG: A Benchmark for Open-Vocabulary Visual Grounding", arXiv, 2023 (*Beihang*). [[Paper](https://arxiv.org/abs/2310.14374)][[Code (in construction)](https://github.com/cv516Buaa/OV-VG)]
    * **?**: "Augment the Pairs: Semantics-Preserving Image-Caption Pair Augmentation for Grounding-Based Vision and Language Models", WACV, 2024 (*Amazon*). [[Paper](https://arxiv.org/abs/2311.02536)][[PyTorch](https://github.com/amzn/augment-the-pairs-wacv2024)]
    * **CyCo**: "Cycle-Consistency Learning for Captioning and Grounding", AAAI, 2024 (*Huawei*). [[Paper](https://arxiv.org/abs/2312.15162)]
    * **ChatterBox**: "ChatterBox: Multi-round Multimodal Referring and Grounding", arXiv, 2024 (*CAS*). [[Paper](https://arxiv.org/abs/2401.13307)][[PyTorch](https://github.com/sunsmarterjie/ChatterBox)]
    * **PIN**: "PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs", arXiv, 2024 (*UvA*). [[Paper](https://arxiv.org/abs/2402.08657)][[Code (in construction)](https://github.com/QUVA-Lab/PIN/)][[Website](https://quva-lab.github.io/PIN/)]
    * **ViGoR**: "ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling", arXiv, 2024 (*Amazon*). [[Paper](https://arxiv.org/abs/2402.06118)]
    * **CRG**: "Contrastive Region Guidance: Improving Grounding in Vision-Language Models without Training", arXiv, 2024 (*UNC*). [[Paper](https://arxiv.org/abs/2403.02325)][[PyTorch](https://github.com/meetdavidwan/crg)][[Website](https://contrastive-region-guidance.github.io/)]
* Video:
    * **Multi-Stage-Transformer**: "Multi-Stage Aggregated Transformer Network for Temporal Language Localization in Videos", CVPR, 2021 (*University of Electronic Science and Technology of China*). [[Paper](https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Multi-Stage_Aggregated_Transformer_Network_for_Temporal_Language_Localization_in_Videos_CVPR_2021_paper.html)]
    * **GTR**: "On Pursuit of Designing Multi-modal Transformer for Video Grounding", EMNLP, 2021 (*Peking*). [[Paper](https://arxiv.org/abs/2109.06085)]
    * **STVGBert**: "STVGBert: A Visual-Linguistic Transformer Based Framework for Spatio-Temporal Video Grounding", ICCV, 2021 (*Tencent*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Su_STVGBert_A_Visual-Linguistic_Transformer_Based_Framework_for_Spatio-Temporal_Video_Grounding_ICCV_2021_paper.html)]
    * **DRFT**: "End-to-end Multi-modal Video Temporal Grounding", NeurIPS, 2021 (*UC Merced*). [[Paper](https://arxiv.org/abs/2107.05624)]
    * **TubeDETR**: "TubeDETR: Spatio-Temporal Video Grounding with Transformers", CVPR, 2022 (*INRIA*). [[Paper](https://arxiv.org/abs/2203.16434)][[Website](https://antoyang.github.io/tubedetr.html)]
    * **UMT**: "UMT: Unified Multi-modal Transformers for Joint Video Moment Retrieval and Highlight Detection", CVPR, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2203.12745)][[Code (in constrcution)](https://github.com/TencentARC/UMT)]
    * **STVGFormer**: "STVGFormer: Spatio-Temporal Video Grounding with Static-Dynamic Cross-Modal Understanding", ACMMMW, 2022 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2207.02756)]
    * **STCAT**: "Embracing Consistency: A One-Stage Approach for Spatio-Temporal Video Grounding", NeurIPS, 2022 (*Peking University*). [[Paper](https://arxiv.org/abs/2209.13306)][[PyTorch](https://github.com/jy0205/STCAT)]
    * **VideoWhisperer**: "Grounded Video Situation Recognition", NeurIPS, 2022 (*IIIT Hyderabad, India*). [[Paper](https://arxiv.org/abs/2210.10828)][[Website](https://zeeshank95.github.io/grvidsitu)]
    * **VidGTR**: "Explore and Match: End-to-End Video Grounding with Transformer", arXiv, 2022 (*KAIST*). [[Paper](https://arxiv.org/abs/2201.10168)]
    * **?**: "Language-free Training for Zero-shot Video Grounding", WACV, 2023 (*Yonsei University*). [[Paper](https://arxiv.org/abs/2210.12977)]
    * **VG-LAW**: "Language Adaptive Weight Generation for Multi-task Visual Grounding", CVPR, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2306.04652)]
    * **TCSF**: "You Can Ground Earlier than See: An Effective and Efficient Pipeline for Temporal Sentence Grounding in Compressed Videos", CVPR, 2023 (*Huazhong University of Science and Technology*). [[Paper](https://arxiv.org/abs/2303.07863)]
    * **?**: "Weakly Supervised Temporal Sentence Grounding with Uncertainty-Guided Self-training", CVPR, 2023 (*The University of Tokyo*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Weakly_Supervised_Temporal_Sentence_Grounding_With_Uncertainty-Guided_Self-Training_CVPR_2023_paper.html)]
    * **DeCo**: "DeCo: Decomposition and Reconstruction for Compositional Temporal Grounding via Coarse-To-Fine Contrastive Ranking", CVPR, 2023 (*Toyota*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Yang_DeCo_Decomposition_and_Reconstruction_for_Compositional_Temporal_Grounding_via_Coarse-To-Fine_CVPR_2023_paper.html)]
    * **HSCNet**: "Hierarchical Semantic Correspondence Networks for Video Paragraph Grounding", CVPR, 2023 (*Sun Yat-sen University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Tan_Hierarchical_Semantic_Correspondence_Networks_for_Video_Paragraph_Grounding_CVPR_2023_paper.html)]
    * **WINNER**: "WINNER: Weakly-Supervised hIerarchical decompositioN and aligNment for Spatio-tEmporal Video gRounding", CVPR, 2023 (*Zhejiang University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Li_WINNER_Weakly-Supervised_hIerarchical_decompositioN_and_aligNment_for_Spatio-tEmporal_Video_gRounding_CVPR_2023_paper.html)]
    * **IRON**: "Iterative Proposal Refinement for Weakly-Supervised Video Grounding", CVPR, 2023 (*Microsoft*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Cao_Iterative_Proposal_Refinement_for_Weakly-Supervised_Video_Grounding_CVPR_2023_paper.html)]
    * **?**: "Collaborative Static and Dynamic Vision-Language Streams for Spatio-Temporal Video Grounding", CVPR, 2023 (*Sun Yat-sen University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Collaborative_Static_and_Dynamic_Vision-Language_Streams_for_Spatio-Temporal_Video_Grounding_CVPR_2023_paper.html)]
    * **ProTeGe**: "ProTeGe: Untrimmed Pretraining for Video Temporal Grounding by Video Temporal Grounding", CVPR, 2023 (*Microsoft*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Wang_ProTeGe_Untrimmed_Pretraining_for_Video_Temporal_Grounding_by_Video_Temporal_CVPR_2023_paper.html)]
    * **VidLN**: "Connecting Vision and Language with Video Localized Narratives", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2302.11217)][[Website](https://google.github.io/video-localized-narratives/)]
    * **VDI**: "Towards Generalisable Video Moment Retrieval: Visual-Dynamic Injection to Image-Text Pre-Training", CVPR, 2023 (*Queen Mary University of London*). [[Paper](https://arxiv.org/abs/2303.00040)]
    * **UniVTG**: "UniVTG: Towards Unified Video-Language Temporal Grounding", ICCV, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2307.16715)][[PyTorch](https://github.com/showlab/UniVTG)]
    * **EaTR**: "Knowing Where to Focus: Event-aware Transformer for Video Grounding", ICCV, 2023 (*Yonsei*). [[Paper](https://arxiv.org/abs/2308.06947)][[PyTorch](https://github.com/jinhyunj/EaTR)]
    * **SOONet**: "Scanning Only Once: An End-to-end Framework for Fast Temporal Grounding in Long Videos", ICCV, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2303.08345)][[PyTorch](https://github.com/afcedf/SOONet)]
    * **TSGSV**: "Temporal Sentence Grounding in Streaming Videos", ACMMM, 2023 (*Shandong University*). [[Paper](https://arxiv.org/abs/2308.07102)]
    * **ConFormer**: "Video Referring Expression Comprehension via Transformer with Content-conditioned Query", ACMMM, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2310.16402)]
    * **CliMer**: "Learning Temporal Sentence Grounding From Narrated EgoVideos", BMVC, 2023 (*University of Bristol, UK*). [[Paper](https://arxiv.org/abs/2310.17395)][[PyTorch](https://github.com/keflanagan/CliMer)]
    * **MomentDiff**: "MomentDiff: Generative Video Moment Retrieval from Random to Real", NeurIPS, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2307.02869)][[Pytorch](https://github.com/IMCCretrieval/MomentDiff)]
    * **?**: "Learning Grounded Vision-Language Representation for Versatile Understanding in Untrimmed Videos", arXiv, 2023 (*Southern University of Science and Technology, China*). [[Paper](https://arxiv.org/abs/2303.06378)]
    * **BM-DETR**: "Overcoming Weak Visual-Textual Alignment for Video Moment Retrieval", arXiv, 2023 (*Seoul National University (SNU)*). [[Paper](https://arxiv.org/abs/2306.02728)][[PyTorch (in construction)](https://github.com/minjoong507/BM-DETR)]
    * **DiffusionVG**: "Exploring Iterative Refinement with Diffusion Models for Video Grounding", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2310.17189)][[PyTorch](https://github.com/MasterVito/DiffusionVG)]
    * **CG-DETR**: "Correlation-guided Query-Dependency Calibration in Video Representation Learning for Temporal Grounding", arXiv, 2023 (*Sungkyunkwan University, Korea*). [[Paper](https://arxiv.org/abs/2311.08835)][[Code (in construction)](https://github.com/wjun0830/CGDETR)]
    * **LLM4VG**: "LLM4VG: Large Language Models Evaluation for Video Grounding", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2312.14206)]
    * **Grounding-Prompter**: "Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2312.17117)]
    * **?**: "Zero-Shot Video Moment Retrieval from Frozen Vision-Language Models", WACV, 2024 (*Queen Mary University of London*). [[Paper](https://arxiv.org/abs/2309.00661)]
    * **Video-GroundingDINO**: "Video-GroundingDINO: Towards Open-Vocabulary Spatio-Temporal Video Grounding", arXiv, 2024 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2401.00901)][[Code (in construction)](https://github.com/TalalWasim/Video-GroundingDINO)]
    * **CG-STVG**: "Context-Guided Spatio-Temporal Video Grounding", arXiv, 2024 (*CAS*). [[Paper](https://arxiv.org/abs/2401.01578)][[Code (in construction)](https://github.com/HengLan/CGSTVG)]
* 3D:
    * **ViL3DRel**: "Language Conditioned Spatial Relation Reasoning for 3D Object Grounding", NeurIPS, 2022 (*INRIA*). [[Paper](https://arxiv.org/abs/2211.09646)][[Website](https://cshizhe.github.io/projects/vil3dref.html)]
    * **LAR**: "Look Around and Refer: 2D Synthetic Semantics Knowledge Distillation for 3D Visual Grounding", NeurIPS, 2022 (*KAUST*). [[Paper](https://arxiv.org/abs/2211.14241)][[Website](https://eslambakr.github.io/LAR.github.io/)]
    * **3D-CG**: "3D Concept Grounding on Neural Fields", NeurIPS, 2022 (*MIT*). [[Paper](https://arxiv.org/abs/2207.06403)][[Website](http://3d-cg.csail.mit.edu/)]
    * **NS3D**: "NS3D: Neuro-Symbolic Grounding of 3D Objects and Relations", CVPR, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2303.13483)]
    * **EDA**: "EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding", CVPR, 2023 (*Peking University*). [[Paper](https://arxiv.org/abs/2209.14941)]
    * **?**: "Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding", ICCV, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2307.09267)]
    * **Multi3DRefer**: "Multi3DRefer: Grounding Text Description to Multiple 3D Objects", ICCV, 2023 (*Simon Fraser*). [[Paper](https://arxiv.org/abs/2309.05251)][[PyTorch](https://github.com/3dlg-hcvc/M3DRef-CLIP)][[Website](https://3dlg-hcvc.github.io/multi3drefer/)]
    * **UniT3D**: "UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding", ICCV, 2023 (*TUM*). [[Paper](https://arxiv.org/abs/2212.00836)]
    * **ViewRefer**: "ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance", ICCV, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2303.16894)][[PyTorch](https://github.com/ZiyuGuo99/ViewRefer3D)]
    * **3DOGSFormer**: "Dense Object Grounding in 3D Scenes", ACMMM, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2309.02224)]
    * **CityRefer**: "CityRefer: Geography-aware 3D Visual Grounding Dataset on City-scale Point Cloud Data", NeurIPS (Datasets and Benchmarks), 2023 (*Advanced Telecommunications Research (ATR), Japan*). [[Paper](https://arxiv.org/abs/2310.18773)][[PyTorch](https://github.com/ATR-DBI/CityRefer)]
    * **?**: "What, when, and where? -- Self-Supervised Spatio-Temporal Grounding in Untrimmed Multi-Action Videos from Narrated Instructions", arXiv, 2023 (*Columbia*). [[Paper](https://arxiv.org/abs/2303.16990)]
    * **3DRP-Net**: "3DRP-Net: 3D Relative Position-aware Network for 3D Visual Grounding", arXiv, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2307.13363)]
    * **3DRefTR**: "A Unified Framework for 3D Point Cloud Visual Grounding", arXiv, 2023 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2308.11887)][[PyTorch](https://github.com/Leon1207/3DRefTR)]
    * **CoT3DRef**: "CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding", arXiv, 2023 (*KAUST*). [[Paper](https://arxiv.org/abs/2310.06214)]
    * **ZSVG3D**: "Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2311.15383)][[Code (in construction)](https://github.com/CurryYuan/ZSVG3D)][[Website](https://curryyuan.github.io/ZSVG3D/)]
    * **SceneVerse**: "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding", arXiv, 2024 (*BIGAI*). [[Paper](https://arxiv.org/abs/2401.09340)][[Code (in construction)](https://github.com/scene-verse/sceneverse)][[Website](https://scene-verse.github.io/)]
* Video-Audio-Text:
    * **LEGO**: "LEGO: Language Enhanced Multi-modal Grounding Model", arXiv, 2024 (*ByteDance*). [[Paper](https://arxiv.org/abs/2401.06071)][[Code (in construction)](https://github.com/lzw-lzw/LEGO)][[Website](https://lzw-lzw.github.io/LEGO.github.io/)]

[[Back to Overview](#overview)]

### Multi-Modal Representation Learning
* General:
    * **LXMERT**: "LXMERT: Learning Cross-Modality Encoder Representations from Transformers", EMNLP, 2019 (*UNC*). [[Paper](https://arxiv.org/abs/1908.07490)][[PyTorch](https://github.com/airsplay/lxmert)]
    * **ViLBERT**: "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks", NeurIPS, 2019 (*Georgia Tech*). [[Paper](https://papers.nips.cc/paper/2019/hash/c74d97b01eae257e44aa9d5bade97baf-Abstract.html)][[PyTorch](https://github.com/facebookresearch/vilbert-multi-task)]
    * **Unified-VLP**: "Unified Vision-Language Pre-Training for Image Captioning and VQA", AAAI, 2020 (*UMich + Microsoft*). [[Paper](https://arxiv.org/abs/1909.11059)][[PyTorch](https://github.com/LuoweiZhou/VLP)]
    * **UNITER**: "UNITER: UNiversal Image-TExt Representation Learning", ECCV, 2020 (*Microsoft*). [[Paper](https://arxiv.org/abs/1909.11740)][[PyTorch](https://github.com/ChenRocks/UNITER)]
    * **VinVL**: "VinVL: Revisiting Visual Representations in Vision-Language Models", CVPR, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2101.00529)][[Code](https://github.com/pzzhang/VinVL)]
    * **CATT**: "Causal Attention for Vision-Language Tasks", CVPR, 2021 (*NTU Singapore*). [[Paper](https://arxiv.org/abs/2103.03493)][[PyTorch](https://github.com/yangxuntu/lxmertcatt)]
    * **ViLT**: "ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision", ICML, 2021 (*Kakao*). [[Paper](https://arxiv.org/abs/2102.03334)][[PyTorch](https://github.com/dandelin/vilt)]
    * **MERLOT**: "MERLOT: Multimodal Neural Script Knowledge Models", NeurIPS, 2021 (*UW + AI2*). [[Paper](https://arxiv.org/abs/2106.02636)][[Tensorflow](https://github.com/rowanz/merlot)][[Website](https://rowanzellers.com/merlot/)]
    * **SVO-Probes**: "Probing Image-Language Transformers for Verb Understanding", arXiv, 2021 (*DeepMind*). [[Paper](https://arxiv.org/abs/2106.09141)]
    * **CLIP-ViL**: "How Much Can CLIP Benefit Vision-and-Language Tasks?", arXiv, 2021 (*Berkeley + UCLA*). [[Paper](https://arxiv.org/abs/2107.06383)][[PyTorch](https://github.com/clip-vil/CLIP-ViL)]
    * **Florence**: "Florence: A New Foundation Model for Computer Vision", arXiv, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.11432)]
    * **UFO**: "UFO: A UniFied TransfOrmer for Vision-Language Representation Learning", arXiv, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.10023)]
    * **SimVLM**: "SimVLM: Simple Visual Language Model Pretraining with Weak Supervision", ICLR, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2108.10904)]
    * **LiT**: "LiT: Zero-Shot Transfer with Locked-image text Tuning", CVPR, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2111.07991)]
    * **UniCL**: "Unified Contrastive Learning in Image-Text-Label Space", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2204.03610)][[PyTorch](https://github.com/microsoft/UniCL)]
    * **FLAVA**: "FLAVA: A Foundational Language And Vision Alignment Model", CVPR, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2112.04482)][[Pretrained Model](https://huggingface.co/facebook/flava-full)][[Code](https://github.com/facebookresearch/multimodal/tree/main/examples/flava)][[Dataset](https://huggingface.co/datasets/facebook/pmd)][[Website](https://flava-model.github.io/)][[Demos](https://huggingface.co/flava)]
    * **LEMON**: "Scaling Up Vision-Language Pre-training for Image Captioning", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.12233)]
    * **METER**: "An Empirical Study of Training End-to-End Vision-and-Language Transformers", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.02387)][[PyTorch](https://github.com/zdou0830/METER)]
    * **Uni-Perceiver**: "Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks", CVPR, 2022 (*SenseTime*). [[Paper](https://arxiv.org/abs/2112.01522)][[PyTorch](https://github.com/fundamentalvision/Uni-Perceiver)]
    * **MERLOT-Reserve**: "MERLOT Reserve: Neural Script Knowledge through Vision and Language and Sound", CVPR, 2022 (*UW + AI2*). [[Paper](https://arxiv.org/abs/2201.02639)][[JAX](https://github.com/rowanz/merlot_reserve)][[Website](https://rowanzellers.com/merlotreserve/)]
    * **Omnivore**: "Omnivore: A Single Model for Many Visual Modalities", CVPR, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2201.08377)][[PyTorch](https://github.com/facebookresearch/omnivore)][[Website](https://facebookresearch.github.io/omnivore/)]
    * **CM-mix**: "Pre-training image-language transformers for open-vocabulary tasks", CVPRW, 2022 (*Google*). [[Paper](https://drive.google.com/file/d/1dYM4g42rptj647v1EfNARmnCt3HdPpNR/view)]
    * **VLMixer**: "VLMixer: Unpaired Vision-Language Pre-training via Cross-Modal CutMix", ICML, 2022 (*Southern University of Science and Technology*). [[Paper](https://arxiv.org/abs/2206.08919)][[Code (in construction)](https://github.com/ttengwang/VLMixer)]
    * **VLUE**: "VLUE: A Multi-Task Benchmark for Evaluating Vision-Language Models", ICML, 2022 (*ByteDance*). [[Paper](https://arxiv.org/abs/2205.15237)][[Website](https://vlue-benchmark.github.io/)][[PyTorch](https://github.com/MichaelZhouwang/VLUE)]
    * **X-VLM**: "Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts", ICML, 2022 (*ByteDance*). [[Paper](https://arxiv.org/abs/2111.08276)][[PyTorch](https://github.com/zengyan-97/X-VLM)]
    * **BLIP**: "BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation", ICML, 2022 (*Salesforce*). [[Paper](https://arxiv.org/abs/2201.12086)][[PyTorch](https://github.com/salesforce/BLIP)]
    * **OFA**: "OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework", ICML, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2202.03052)][[PyTorch](https://github.com/OFA-Sys/OFA)]
    * **MS-CLIP**: "Learning Visual Representation from Modality-Shared Contrastive Language-Image Pre-training", ECCV, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2207.12661)][[PyTorch](https://github.com/Hxyou/MSCLIP)]
    * **GRIT-VLP**: "GRIT-VLP: Grouped Mini-batch Sampling for Efficient Vision and Language Pre-training", ECCV, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2208.04060)][[PyTorch](https://github.com/jaeseokbyun/GRIT-VLP)]
    * **SIMLA**: "Single-Stream Multi-Level Alignment for Vision-Language Pretraining", ECCV, 2022 (*Northeastern University*). [[Paper](https://arxiv.org/abs/2203.14395)][[PyTorch](https://github.com/codezakh/SIMLA)][[Website](http://zaidkhan.me/SIMLA/)]
    * **Switch-BERT**: "Switch-BERT: Learning to Model Multimodal Interactions by Switching Attention and Input", ECCV, 2022 (*Ant Group*). [[Paper](https://arxiv.org/abs/2306.14182)]
    * **OmniVL**: "OmniVL: One Foundation Model for Image-Language and Video-Language Tasks", NeurIPS, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2209.07526)]
    * **UniCLIP**: "UniCLIP: Unified Framework for Contrastive Language-Image Pre-training", NeurIPS, 2022 (*LG*). [[Paper](https://arxiv.org/abs/2209.13430)]
    * **Uni-Perceiver-MoE**: "Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs", NeurIPS, 2022 (*SenseTime*). [[Paper](https://arxiv.org/abs/2206.04674)][[PyTorch](https://github.com/fundamentalvision/Uni-Perceiver)]
    * **CLOOB**: "CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP", NeurIPS, 2022 (*Johannes Kepler University, Austria*). [[Paper](https://arxiv.org/abs/2110.11316)][[PyTorch](https://github.com/ml-jku/cloob)]
    * **CyCLIP**: "CyCLIP: Cyclic Contrastive Language-Image Pretraining", NeurIPS, 2022 (*UCLA*). [[Paper](https://arxiv.org/abs/2205.14459)]
    * **?**: "Quality Not Quantity: On the Interaction between Dataset Design and Robustness of CLIP", NeurIPS, 2022 (*UW*). [[Paper](https://openreview.net/forum?id=LTCBavFWp5C)][[Pytorch](https://github.com/mlfoundations/clip_quality_not_quantity)]
    * **PyramidCLIP**: "PyramidCLIP: Hierarchical Feature Alignment for Vision-language Model Pretraining", NeurIPS, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2204.14095)]
    * **?**: "Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning", NeurIPS, 2022 (*Stanford*). [[Paper](https://arxiv.org/abs/2203.02053)][[Website](https://modalitygap.readthedocs.io/)]
    * **LIMoE**: "Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts", NeurIPS, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2206.02770)]
    * **VLMo**: "VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts", NeurIPS, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.02358)][[PyTorch (in construction)](https://github.com/microsoft/unilm/tree/master/vlmo)]
    * **Knowledge-CLIP**: "Contrastive Language-Image Pre-Training with Knowledge Graphs", NeurIPS, 2022 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2210.08901)]
    * **Flamingo**: "Flamingo: a Visual Language Model for Few-Shot Learning", NeurIPS, 2022 (*DeepMind*). [[Paper](https://arxiv.org/abs/2204.14198)]
    * **LOUPE**: "Fine-Grained Semantically Aligned Vision-Language Pre-Training", NeurIPS, 2022 (*Huawei*). [[Paper](https://arxiv.org/abs/2208.02515)][[Code (in construction)](https://github.com/YYJMJC/LOUPE)]
    * **FIBER**: "Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone", NeurIPS, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2206.07643)][[PyTorch](https://github.com/microsoft/FIBER)]
    * **UViM**: "UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes", NeurIPS, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2205.10337)]
    * **LAION-5B**: "LAION-5B: An open large-scale dataset for training next generation image-text models", NeurIPS (Datasets and Benchmarks), 2022 (*LAION*). [[Paper](https://openreview.net/forum?id=M3Y74vmsMcY)][[Website](https://laion.ai/blog/laion-5b/)]
    * **Wukong**: "Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark", NeurIPS (Datasets and Benchmarks), 2022 (*Huawei*). [[Paper](https://arxiv.org/abs/2202.06767)][[Website](https://wukong-dataset.github.io/wukong-dataset/)]
    * **TaiSu**: "TaiSu: A 166M Large-scale High-Quality Dataset for Chinese Vision-Language Pre-training", NeurIPS (Datasets and Benchmarks), 2022 (*CAS*). [[Paper](https://openreview.net/forum?id=iAxH-ikIP0I)][[PyTorch](https://github.com/ksOAn6g5/TaiSu)]
    * **WinoGAViL**: "WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models", NeurIPS (Datasets and Benchmarks), 2022 (*The Hebrew University of Jerusalem, Israel*). [[Paper](https://arxiv.org/abs/2207.12576)][[Website](https://winogavil.github.io/)]
    * **ELEVATER**: "ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models", NeurIPS (Datasets and Benchmarks), 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2204.08790)][[Website](https://computer-vision-in-the-wild.github.io/ELEVATER/)]
    * **?**: "Robustness Analysis of Video-Language Models Against Visual and Language Perturbations", NeurIPS (Datasets and Benchmarks), 2022 (*UCF*). [[Paper](https://arxiv.org/abs/2207.02159)][[Website](https://sites.google.com/view/videolanguagerobustness/home)]
    * **GIT**: "GIT: A Generative Image-to-text Transformer for Vision and Language", TMLR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2205.14100)]
    * **CoCa**: "CoCa: Contrastive Captioners are Image-Text Foundation Models", TMLR, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2205.01917)][[PyTorch (lucidrains)](https://github.com/lucidrains/CoCa-pytorch)]
    * **MultiMAE**: "MultiMAE: Multi-modal Multi-task Masked Autoencoders", arXiv, 2022 (*EPFL*). [[Paper](https://arxiv.org/abs/2204.01678)][[PyTorch](https://github.com/EPFL-VILAB/MultiMAE)][[Website](https://multimae.epfl.ch/)]
    * **VLC**: "Training Vision-Language Transformers from Captions Alone", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2205.09256)][[Code (in construction)](https://github.com/guilk/VLC)]
    * **CCLM**: "Cross-View Language Modeling: Towards Unified Cross-Lingual Cross-Modal Pre-training", arXiv, 2022 (*ByteDance*). [[Paper](https://arxiv.org/abs/2206.00621)]
    * **VL-BEiT**: "VL-BEiT: Generative Vision-Language Pretraining", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2206.01127)]
    * **MetaLM**: "Language Models are General-Purpose Interfaces", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2206.06336)][[PyTorch](https://github.com/microsoft/unilm)]
    * **Bridge-Tower**: "Bridge-Tower: Building Bridges Between Encoders in Vision-Language Representation Learning", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2206.08657)][[Code (in construction)](https://github.com/microsoft/BridgeTower)]
    * **e-CLIP**: "e-CLIP: Large-Scale Vision-Language Representation Learning in E-commerce", arXiv, 2022 (*NAVER*). [[Paper](https://arxiv.org/abs/2207.00208)]
    * **LW-Transformer**: "Towards Lightweight Transformer via Group-wise Transformation for Vision-and-Language Tasks", arXiv, 2022 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2204.07780)][[PyTorch](https://github.com/luogen1996/LWTransformer)]
    * **UCM**: "Self-Training Vision Language BERTs with a Unified Conditional Model", arXiv, 2022 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2201.02010)]
    * **Prefix-conditioning**: "Prefix Conditioning Unifies Language and Label Supervision", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2206.01125)]
    * **VLMAE**: "VLMAE: Vision-Language Masked Autoencoder", arXiv, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2208.09374)]
    * **ViCHA**: "Efficient Vision-Language Pretraining with Visual Concepts and Hierarchical Alignment", arXiv, 2022 (*Sorbonne University, France*). [[Paper](https://arxiv.org/abs/2208.13628)][[Code (in construction)](https://github.com/mshukor/ViCHA)]
    * **DetailCLIP**: "Injecting Image Details into CLIP's Feature Space", arXiv, 2022 (*Megvii*). [[Paper](https://arxiv.org/abs/2208.14649)]
    * **?**: "Pre-training image-language transformers for open-vocabulary tasks", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2209.04372)]
    * **ERNIE**: "ERNIE-ViL 2.0: Multi-view Contrastive Learning for Image-Text Pre-training", arXiv, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2209.15270)][[Paddle](https://github.com/PaddlePaddle/ERNIE)]
    * **?**: "One does not fit all! On the Complementarity of Vision Encoders for Vision and Language Tasks", arXiv, 2022 (*Technical University of Darmstadt, Germany*). [[Paper](https://arxiv.org/abs/2210.06379)]
    * **MAPL**: "MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting", arXiv, 2022 (*Mila*). [[Paper](https://arxiv.org/abs/2210.07179)]
    * **EfficientVLM**: "EfficientVLM: Fast and Accurate Vision-Language Models via Knowledge Distillation and Modal-adaptive Pruning", arXiv, 2022 (*Bytedance*). [[Paper](https://arxiv.org/abs/2210.07795)][[PyTorch (in construction)](https://github.com/swaggy-TN/EfficientVLM)]
    * **CN-CLIP**: "Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese", arXiv, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2211.01335)]
    * **X<sup>2</sup>-VLM**: "X<sup>2</sup>-VLM: All-In-One Pre-trained Model For Vision-Language Tasks", arXiv, 2022 (*ByteDance*). [[Paper](https://arxiv.org/abs/2211.12402)][[Code (in construction)](https://github.com/zengyan-97/X2-VLM)]
    * **SkillNet**: "One Model, Multiple Modalities: A Sparsely Activated Approach for Text, Sound, Image, Video and Code", arXiv, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2205.06126)]
    * **Compound-Tokens**: "Compound Tokens: Channel Fusion for Vision-Language Representation Learning", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2212.01447)]
    * **WFH**: "Learning by Hallucinating: Vision-Language Pre-training with Weak Supervision", WACV, 2023 (*Aalto University, Finland*). [[Paper](https://arxiv.org/abs/2210.13591)]
    * **Perceiver-VL**: "Perceiver-VL: Efficient Vision-and-Language Modeling with Iterative Latent Attention", WACV, 2023 (*UNC*). [[Paper](https://arxiv.org/abs/2211.11701)][[PyTorch](https://github.com/zinengtang/Perceiver_VL)]
    * **MixGen**: "MixGen: A New Multi-Modal Data Augmentation", WACVW, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2206.08358)]
    * **?**: "Unifying Vision-Language Representation Space with Single-tower Transformer", AAAI, 2023 (*NAVER*). [[Paper](https://arxiv.org/abs/2211.11153)]
    * **PaLI**: "PaLI: A Jointly-Scaled Multilingual Language-Image Model", ICLR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2209.06794)]
    * **LilT**: "Contrastive Alignment of Vision to Language Through Parameter-Efficient Transfer Learning", ICLR, 2023 (*Northeastern University*). [[Paper](https://arxiv.org/abs/2303.11866)][[PyTorch](https://github.com/codezakh/LilT)]
    * **CLIPs**: "Is a Caption Worth a Thousand Images? A Controlled Study for Representation Learning", ICLR, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2207.07635)]
    * **HiCLIP**: "HiCLIP: Contrastive Language-Image Pretraining with Hierarchy-aware Attention", ICLR, 2023 (*Rutgers University*). [[Paper](https://arxiv.org/abs/2303.02995)]
    * **DeCap**: "DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training", ICLR, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2303.03032)][[PyTorch](https://github.com/dhg-wei/DeCap)]
    * **MaskVLM**: "Masked Vision and Language Modeling for Multi-modal Representation Learning", ICLR, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2208.02131)]
    * **DaVinci**: "Write and Paint: Generative Vision-Language Models are Unified Modal Learners", ICLR, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2206.07699)][[Code (in construction)](https://github.com/shizhediao/DaVinci)]
    * **EVA**: "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale", CVPR, 2023 (*Beijing Academy of Artificial Intelligence (BAAI)*). [[Paper](https://arxiv.org/abs/2211.07636)][[PyTorch](https://github.com/baaivision/EVA)]
    * **FLM**: "Accelerating Vision-Language Pretraining with Free Language Modeling", CVPR, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2303.14038)][[PyTorch](https://github.com/TencentARC/FLM)]
    * **FDT**: "Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens", CVPR, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2303.14865)][[Code (in construction)](https://github.com/yuxiaochen1103/FDT)]
    * **VILA**: "VILA: Learning Image Aesthetics from User Comments with Vision-Language Pretraining", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.14302)][[JAX](https://github.com/google-research/google-research/tree/master/vila)]
    * **BEiT-3**: "Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2208.10442)][[PyTorch](https://github.com/microsoft/unilm/tree/master/beit)]
    * **ReVeaL**: "REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2212.05221)][[Website](https://reveal-cvpr.github.io/)]
    * **SCL**: "Seeing What You Miss: Vision-Language Pre-training with Semantic Completion Learning", CVPR, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2211.13437)]
    * **EPIC**: "Leveraging per Image-Token Consistency for Vision-Language Pre-training", CVPR, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2211.15398)]
    * **PTP**: "Position-guided Text Prompt for Vision-Language Pre-training", CVPR, 2023 (*Sea AI Lab*). [[Paper](https://arxiv.org/abs/2212.09737)][[PyTorch](https://github.com/sail-sg/ptp)]
    * **PHASE**: "Uncurated Image-Text Datasets: Shedding Light on Demographic Bias", CVPR, 2023 (*Osaka University*). [[Paper](https://arxiv.org/abs/2304.02828)][[GitHub](https://github.com/noagarcia/phase)]
    * **Uni-Perceiver-v2**: "Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks", CVPR, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2211.09808)][[PyTorch](https://github.com/fundamentalvision/Uni-Perceiver)]
    * **?**: "Exploring the Effect of Primitives for Compositional Generalization in Vision-and-Language", CVPR, 2023 (*Beijing Institute of Technology*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Li_Exploring_the_Effect_of_Primitives_for_Compositional_Generalization_in_Vision-and-Language_CVPR_2023_paper.html)]
    * **GIVL**: "GIVL: Improving Geographical Inclusivity of Vision-Language Models with Pre-Training Methods", CVPR, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2301.01893)]
    * **FLIP**: "Scaling Language-Image Pre-training via Masking", CVPR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2212.00794)][[PyTorch](https://github.com/facebookresearch/flip)]
    * **MAP**: "MAP: Modality-Agnostic Uncertainty-Aware Vision-Language Pre-training Model", CVPR, 2023 (*Tsinghua + Waseda*). [[Paper](https://arxiv.org/abs/2210.05335)][[PyTorch](https://github.com/IIGROUP/MAP)
    * **DANCE**: "Improving Commonsense in Vision-Language Models via Knowledge Graph Riddles", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2211.16504)][[PyTorch (in construction)](https://github.com/pleaseconnectwifi/DANCE)][[Website](https://shuquanye.com/DANCE_website/)]
    * **xCLIP**: "Non-Contrastive Learning Meets Language-Image Pre-Training", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2210.09304)]
    * **SVLC**: "Teaching Structured Vision & Language Concepts to Vision&Language Models", CVPR, 2023 (*IBM*). [[Paper](https://arxiv.org/abs/2211.11733)]
    * **DeAR**: "DeAR: Debiasing Vision-Language Models with Additive Residuals", CVPR, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2303.10431)][[GitHub](https://github.com/pata-fairness/pata_dataset)]
    * **?**: "Understanding and Constructing Latent Modality Structures in Multi-modal Representation Learning", CVPR, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2303.05952)]
    * **UniHCP**: "UniHCP: A Unified Model for Human-Centric Perceptions", CVPR, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2303.02936)][[PyTorch](https://github.com/OpenGVLab/UniHCP)]
    * **HumanBench**: "HumanBench: Towards General Human-centric Perception with Projector Assisted Pretraining", CVPR, 2023 (*SenseTime*). [[Paper](https://arxiv.org/abs/2303.05675)][[PyTorch](https://github.com/OpenGVLab/HumanBench)]
    * **?**: "Joint Adaptive Representations for Image-Language Learning", CVPRW, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2305.19924)]
    * **BLIP-2**: "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models", ICML, 2023 (*Salesforce*). [[Paper](https://arxiv.org/abs/2301.12597)][[PyTorch](https://github.com/salesforce/LAVIS/tree/main/projects/blip2)]
    * **RLEG**: "RLEG: Vision-Language Representation Learning with Diffusion-based Embedding Generation", ICML, 2023 (*Alibaba*). [[Paper](https://openreview.net/forum?id=zBShO1Vmf0)]
    * **Mod-X**: "Continual Vision-Language Representation Learning with Off-Diagonal Information", ICML, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2305.07437)]
    * **ILLUME**: "ILLUME: Rationalizing Vision-Language Models through Human Interactions", ICML, 2023 (*German Center for Artificial Intelligence (DFKI)*). [[Paper](https://arxiv.org/abs/2208.08241)][[PyTorch](https://github.com/ml-research/ILLUME)]
    * **Pix2Struct**: "Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding", ICML, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2210.03347)]
    * **MERU**: "Hyperbolic Image-Text Representations", ICML, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2304.09172)]
    * **?**: "Measuring Progress in Fine-grained Vision-and-Language Understanding", ACL, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2305.07558)]
    * **RELIT**: "Weakly Supervised Vision-and-Language Pre-training with Relative Representations", ACL, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2305.15483)]
    * **PuMer**: "PuMer: Pruning and Merging Tokens for Efficient Vision Language Models", ACL, 2023 (*UW*). [[Paper](https://arxiv.org/abs/2305.17530)]
    * **SINC**: "SINC: Self-Supervised In-Context Learning for Vision-Language Tasks", ICCV, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2307.07742)]
    * **ALIP**: "ALIP: Adaptive Language-Image Pre-training with Synthetic Caption", ICCV, 2023 (*DeepGlint, China*). [[Paper](https://arxiv.org/abs/2308.08428)][[PyTorch](https://github.com/deepglint/ALIP)]
    * **SigLiT**: "Sigmoid Loss for Language Image Pre-Training", ICCV, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.15343)][[JAX](https://github.com/google-research/big_vision)]
    * **VL-PET**: "VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control", ICCV, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2308.09804)][[PyTorch](https://github.com/HenryHZY/VL-PET)][[Website](https://henryhzy.github.io/VL-PET/)]
    * **GrowCLIP**: "GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training", ICCV, 2023 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2308.11331)]
    * **ViLLA**: "ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data", ICCV, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2308.11194)][[PyTorch](https://github.com/StanfordMIMI/villa)]
    * **CFM-ViT**: "Contrastive Feature Masking Open-Vocabulary Vision Transformer", ICCV, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2309.00775)]
    * **EqSim**: "Equivariant Similarity for Vision-Language Foundation Models", ICCV, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2303.14465)][[PyTorch](https://github.com/Wangt-CN/EqBen)]
    * **A-CLIP**: "Attentive Mask CLIP", ICCV, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2212.08653)]
    * **CLOSE**: "I Can't Believe There's No Images! Learning Visual Tasks Using only Language Data", ICCV, 2023 (*AI2*). [[Paper](https://arxiv.org/abs/2211.09778)][[PyTorch](https://github.com/allenai/close)][[Website](https://prior.allenai.org/projects/close)]
    * **SyViC**: "Going Beyond Nouns With Vision & Language Models Using Synthetic Data", ICCV, 2023 (*IBM*). [[Paper](https://arxiv.org/abs/2303.17590)][[PyTorch](https://github.com/uvavision/SyViC)][[Website](https://synthetic-vic.github.io/)]
    * **ViLTA**: "ViLTA: Enhancing Vision-Language Pre-training through Textual Augmentation", ICCV, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2308.16689)]
    * **MCD**: "Misalign, Contrast then Distill: Rethinking Misalignments in Language-Image Pretraining", ICCV, 2023 (*LG*). [[Paper](https://arxiv.org/abs/2312.12661)]
    * **TL;DR**: "Too Large; Data Reduction for Vision-Language Pre-Training", ICCV, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2305.20087)][[PyTorch](https://github.com/showlab/datacentric.vlp)]
    * **DiffusionITM**: "Are Diffusion Models Vision-And-Language Reasoners?", NeurIPS, 2023 (*Mila*). [[Paper](https://arxiv.org/abs/2305.16397)][[PyTorch](https://github.com/McGill-NLP/diffusion-itm)]
    * **OPTIMA**: "Module-wise Adaptive Distillation for Multimodality Foundation Models", NeurIPS, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2310.04550)]
    * **4M**: "4M: Massively Multimodal Masked Modeling", NeurIPS, 2023 (*EPFL*). [[Paper](https://arxiv.org/abs/2312.06647)][[Website](https://4m.epfl.ch/)]
    * **P-Former**: "Bootstrapping Vision-Language Learning with Decoupled Language Pre-training", NeurIPS, 2023 (*Dartmouth College*). [[Paper](https://arxiv.org/abs/2307.07063)][[PyTorch](https://github.com/yiren-jian/BLIText)]
    * **LQAE**: "Language Quantized AutoEncoders: Towards Unsupervised Text-Image Alignment", NeurIPS, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2302.00902)]
    * **OBELISC**: "OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents", NeurIPS, 2023 (*Hugging Face*). [[Paper](https://arxiv.org/abs/2306.16527)][[GitHub](https://github.com/huggingface/OBELISC)]
    * **VoLTA**: "VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment", TMLR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2210.04135)][[PyTorch](https://github.com/ShramanPramanick/VoLTA)][[Website](https://shramanpramanick.github.io/VoLTA/)]
    * **KOSMOS-1**: "Language Is Not All You Need: Aligning Perception with Language Models", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2302.14045)][[Code](https://github.com/microsoft/unilm)]
    * **Prismer**: "Prismer: A Vision-Language Model with An Ensemble of Experts", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2303.02506)][[PyTorch](https://github.com/NVlabs/prismer)][[Website](https://shikun.io/projects/prismer)]
    * **RVLM**: "Replacement as a Self-supervision for Fine-grained Vision-language Pre-training", arXiv, 2023 (*Harbin Institute of Technology*). [[Paper](https://arxiv.org/abs/2303.05313)]
    * **MuLTI**: "MuLTI: Efficient Video-and-Language Understanding with MultiWay-Sampler and Multiple Choice Modeling", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2303.05707)]
    * **VL-MoE**: "Scaling Vision-Language Models with Sparse Mixture of Experts", arXiv, 2023 (*Berkeley + Microsoft*). [[Paper](https://arxiv.org/abs/2303.07226)]
    * **EVA-02**: "EVA-02: A Visual Representation for Neon Genesis", arXiv, 2023 (*BAAI*). [[Paper](https://arxiv.org/abs/2303.11331)][[PyTorch](https://github.com/baaivision/EVA/tree/master/EVA-02)]
    * **CoBIT**: "CoBIT: A Contrastive Bi-directional Image-Text Generation Model", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.13455)]
    * **EVA-CLIP**: "EVA-CLIP: Improved Training Techniques for CLIP at Scale", arXiv, 2023 (*BAAI*). [[Paper](https://arxiv.org/abs/2303.15389)][[PyTorch](https://github.com/baaivision/EVA/tree/master/EVA-CLIP)]
    * **Sig**: "Sigmoid Loss for Language Image Pre-Training", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.15343)]
    * **MaMMUT**: "MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.16839)]
    * **CAVL**: "CAVL: Learning Contrastive and Adaptive Representations of Vision and Language", arXiv, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2304.04399)]
    * **MoMo**: "MoMo: A shared encoder Model for text, image and multi-Modal representations", arXiv, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2304.05523)]
    * **REAVL**: "Retrieval-based Knowledge Augmented Vision Language Pre-training", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2304.13923)]
    * **ALBEF-MI**: "Vision Lanauge Pre-training by Contrastive Learning with Cross-Modal Similarity Regulation", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2305.04474)]
    * **Helip**: "Boosting Visual-Language Models by Exploiting Hard Samples", arXiv, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2305.05208)]
    * **IMP**: "Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2305.06324)]
    * **Musketeer**: "Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts", arXiv, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2305.07019)]
    * **GVT**: "What Makes for Good Visual Tokenizers for Large Language Models?", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2305.12223)][[Code (in construction)](https://github.com/TencentARC/GVT)]
    * **S-CLIP**: "S-CLIP: Semi-supervised Vision-Language Pre-training using Few Specialist Captions", NeurIPS, 2023 (*KAIST*). [[Paper](https://arxiv.org/abs/2305.14095)]
    * **VisorGPT**: "VisorGPT: Learning Visual Prior via Generative Pre-Training", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2305.13777)][[Code (in construction)](https://github.com/Sierkinhane/VisorGPT)][[Website](https://sierkinhane.github.io/visor-gpt/)]
    * **IdealGPT**: "IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models", arXiv, 2023 (*Columbia University*). [[Paper](https://arxiv.org/abs/2305.14985)][[PyTorch](https://github.com/Hxyou/IdealGPT)]
    * **PaLI-X**: "PaLI-X: On Scaling up a Multilingual Vision and Language Model", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2305.18565)]
    * **CrossGET**: "CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2305.17455)][[Code (in construction)](https://github.com/sdc17/CrossGET)]
    * **COSA**: "COSA: Concatenated Sample Pretrained Vision-Language Foundation Model", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2306.09085)][[PyTorch](https://github.com/TXH-mercury/COSA)]
    * **Babel-ImageNet**: "Babel-ImageNet: Massively Multilingual Evaluation of Vision-and-Language Representations", arXiv, 2023 (*University of Wrzburg, Germany*). [[Paper](https://arxiv.org/abs/2306.08658)][[PyTorch](https://github.com/gregor-ge/Babel-ImageNet)]
    * **Kosmos-2**: "Kosmos-2: Grounding Multimodal Large Language Models to the World", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2306.14824)][[PyTorch](https://github.com/microsoft/unilm/tree/master/kosmos-2)][[Demo](https://888e9ea5c7b6d250.gradio.app/)]
    * **LENS**: "Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language", arXiv, 2023 (*Contextual AI + Stanford*). [[Paper](https://arxiv.org/abs/2306.16410)][[PyTorch](https://github.com/ContextualAI/lens)][[Demo](https://lens.contextual.ai/)]
    * **Emu**: "Generative Pretraining in Multimodality", arXiv, 2023 (*BAAI*). [[Paper](https://arxiv.org/abs/2307.05222)][[PyTorch](https://github.com/baaivision/Emu)]
    * **mBLIP**: "mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs", arXiv, 2023 (*University of Wurzburg, Germany*). [[Paper](https://arxiv.org/abs/2307.06930)][[PyTorch](https://github.com/gregor-ge/mBLIP)]
    * **SEED-OPT**: "Planting a SEED of Vision in Large Language Model", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2307.08041)][[Code (in construction)](https://github.com/AILab-CVC/SEED)]
    * **OpenFlamingo**: "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models", arXiv, 2023 (*UW*). [[Paper](https://arxiv.org/abs/2308.01390)][[PyTorch](https://github.com/mlfoundations/open_flamingo)]
    * **Free-ATM**: "Free-ATM: Exploring Unsupervised Learning on Diffusion-Generated Images with Free Attention Masks", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2308.06739)]
    * **LCL**: "Link-Context Learning for Multimodal LLMs", arXiv, 2023 (*SenseTime*). [[Paper](https://arxiv.org/abs/2308.07891)]
    * **DLIP**: "DLIP: Distilling Language-Image Pre-training", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2308.12956)]
    * **LaVIT**: "Unified Language-Vision Pretraining with Dynamic Discrete Visual Tokenization", arXiv, 2023 (*Kuaishou*). [[Paper](https://arxiv.org/abs/2309.04669)][[Code (in construction)](https://github.com/jy0205/LaVIT)]
    * **MMICL**: "MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning", arXiv, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2309.07915)][[PyTorch](https://github.com/HaozheZhao/MIC)]
    * **ELIP**: "ELIP: Efficient Language-Image Pre-training with Fewer Vision Tokens", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2309.16738)]
    * **SEED-LLaMA**: "Making LLaMA SEE and Draw with SEED Tokenizer", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2310.01218)][[PyTorch](https://github.com/AILab-CVC/SEED)]
    * **ITIT**: "Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2310.03734)]
    * **SimVLG**: "SimVLG: Simple and Efficient Pretraining of Visual Language Generative Models", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2310.03291)]
    * **VeCLIP**: "From Scarcity to Efficiency: Improving CLIP Training via Visual-enriched Captions", arXiv, 2023 (*Apple*). [[Paper](https://arxiv.org/abs/2310.07699)]
    * **PaLI-3**: "PaLI-3 Vision Language Models: Smaller, Faster, Stronger", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2310.09199)]
    * **COMM**: "From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models", arXiv, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2310.08825)][[PyTorch (in construction)](https://github.com/YuchenLiu98/COMM)]
    * **CogVLM**: "CogVLM: Visual Expert for Pretrained Language Models", arXiv, 2023 (*Zhipu AI, China*). [[Paper](https://arxiv.org/abs/2311.03079)][[PyTorch](https://github.com/THUDM/CogVLM)]
    * **OtterHD**: "OtterHD: A High-Resolution Multi-modality Model", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2311.04219)][[PyTorch](https://github.com/Luodian/Otter)]
    * **Florence-2**: "Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2311.06242)]
    * **MLA**: "Multimodal Representation Learning by Alternating Unimodal Adaptation", arXiv, 2023 (*UNC*). [[Paper](https://arxiv.org/abs/2311.10707)]
    * **MobileCLIP**: "MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced Training", arXiv, 2023 (*Apple*). [[Paper](https://arxiv.org/abs/2311.17049)]
    * **LLaMA-VID**: "LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2311.17043)][[PyTorch](https://github.com/dvlab-research/LLaMA-VID)]
    * **?**: "MLLMs-Augmented Visual-Language Representation Learning", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2311.18765)][[Code (in construction)](https://github.com/lyq312318224/MLLMs-Augmented)]
    * **Hulk**: "Hulk: A Universal Knowledge Translator for Human-Centric Tasks", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2312.01697)][[PyTorch](https://github.com/OpenGVLab/HumanBench)]
    * **D-iGPT**: "Rejuvenating image-GPT as Strong Visual Representation Learners", arXiv, 2023 (*JHU + UC Santa Cruz*). [[Paper](https://arxiv.org/abs/2312.02147)][[PyTorch](https://github.com/OliverRensu/D-iGPT)]
    * **Vary**: "Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models", arXiv, 2023 (*Megvii*). [[Paper](https://arxiv.org/abs/2312.06109)][[PyTorch](https://github.com/Ucas-HaoranWei/Vary)][[Website](https://varybase.github.io/)]
    * **VILA**: "VILA: On Pre-training for Visual Language Models", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2312.07533)]
    * **Emu2**: "Generative Multimodal Models are In-Context Learners", arXiv, 2023 (*BAAI*). [[Paper](https://arxiv.org/abs/2312.13286)][[PyTorch](https://baaivision.github.io/emu2/)][[Website](https://baaivision.github.io/emu2/)]
    * **InternVL**: "InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2312.14238)][[PyTorch](https://github.com/OpenGVLab/InternVL)]
    * **TiMix**: "TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training", AAAI, 2024 (*Peking*). [[Paper](https://arxiv.org/abs/2312.08846)]
    * **ECLIPSE**: "Expediting Contrastive Language-Image Pretraining via Self-distilled Encoders", AAAI, 2024 (*LG*). [[Paper](https://arxiv.org/abs/2312.12659)]
    * **ASM**: "The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World", ICLR, 2024 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2308.01907)][[PyTorch](https://github.com/OpenGVLab/All-Seeing)][[Demo](https://huggingface.co/spaces/OpenGVLab/all-seeing)]
    * **NARVL**: "Non-autoregressive Sequence-to-Sequence Vision-Language Models", CVPR, 2024 (*Amazon*). [[Paper](https://arxiv.org/abs/2403.02249)]
    * **S4**: "Enhancing Vision-Language Pre-training with Rich Supervisions", CVPR, 2024 (*Amazon*). [[Paper](https://arxiv.org/abs/2403.03346)]
    * **COSMO**: "COSMO: COntrastive Streamlined MultimOdal Model with Interleaved Pre-Training", arXiv, 2024 (*Microsoft*). [[Paper](https://arxiv.org/abs/2401.00849)][[PyTorch](https://github.com/showlab/cosmo)][[Website](https://fingerrec.github.io/cosmo/)]
    * **?**: "Low-Resource Vision Challenges for Foundation Models", arXiv, 2024 (*UvA*). [[Paper](https://arxiv.org/abs/2401.04716)][[Code (in construction)](https://github.com/xiaobai1217/Low-Resource-Vision)][[Website](https://xiaobai1217.github.io/Low-Resource-Vision/)]
    * **UMG-CLIP**: "UMG-CLIP: A Unified Multi-Granularity Vision Generalist for Open-World Understanding", arXiv, 2024 (*Huawei*). [[Paper](https://arxiv.org/abs/2401.06397)]
    * **MM-Interleaved**: "MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer", arXiv, 2024 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2401.10208)][[PyTorch](https://github.com/OpenGVLab/MM-Interleaved)]
    * **SPARC**: "Improving fine-grained understanding in image-text pre-training", arXiv, 2024 (*DeepMind*). [[Paper](https://arxiv.org/abs/2401.09865)]
    * **MouSi**: "MouSi: Poly-Visual-Expert Vision-Language Models", arXiv, 2024 (*Fudan*). [[Paper](https://arxiv.org/abs/2401.17221)][[Code (in construction)](https://github.com/FudanNLPLAB/MouSi)]
    * **?**: "Enhancing Multimodal Large Language Models with Vision Detection Models: An Empirical Study", arXiv, 2024 (*Alibaba*). [[Paper](https://arxiv.org/abs/2401.17981)]
    * **QA-ViT**: "Question Aware Vision Transformer for Multimodal Reasoning", arXiv, 2024 (*Amazon*). [[Paper](https://arxiv.org/abs/2402.05472)]
    * **PaLM2-VAdapter**: "PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter", arXiv, 2024 (*Google*). [[Paper](https://arxiv.org/abs/2402.10896)]
    * **PALO**: "PALO: A Polyglot Large Multimodal Model for 5B People", arXiv, 2024 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2402.14818)][[Code (in construction)](https://github.com/mbzuai-oryx/PALO)]
    * **CogCoM**: "CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations", arXiv, 2024 (*Zhipu AI, China*). [[Paper](https://arxiv.org/abs/2402.04236)][[PyTorch](https://github.com/THUDM/CogCoM)]
    * **EVA-CLIP-18B**: "EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters", arXiv, 2024 (*BAAI*). [[Paper](https://arxiv.org/abs/2402.04252)][[PyTorch](https://github.com/baaivision/EVA/tree/master/EVA-CLIP-18B)]
    * **SynthCLIP**: "SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?", arXiv, 2024 (*KAUST*). [[Paper](https://arxiv.org/abs/2402.01832)][[PyTorch](https://github.com/hammoudhasan/SynthCLIP)]
    * **CloVe**: "CLoVe: Encoding Compositional Language in Contrastive Vision-Language Models", arXiv, 2024 (*Netflix*). [[Paper](https://arxiv.org/abs/2402.15021)]
    * **ASMv2**: "The All-Seeing Project V2: Towards General Relation Comprehension of the Open World", arXiv, 2024 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2402.19474)][[PyTorch](https://github.com/OpenGVLab/all-seeing)]
    * **Multimodal-ArXiv**: "Multimodal ArXiv: A Dataset for Improving Scientific Comprehension of Large Vision-Language Models", arXiv, 2024 (*HKU*). [[Paper](https://arxiv.org/abs/2403.00231)][[Website](https://mm-arxiv.github.io/)]
    * **Synth<sup>2</sup>**: "Synth<sup>2</sup>: Boosting Visual-Language Models with Synthetic Captions and Image Embeddings", arXiv, 2024 (*DeepMind*). [[Paper](https://arxiv.org/abs/2403.07750)]
* Video:
    * **COOT**: "COOT: Cooperative Hierarchical Transformer for Video-Text Representation Learning", NeurIPS, 2020 (*University of Freiburg*). [[Paper](https://arxiv.org/abs/2011.00597)][[PyTorch](https://github.com/gingsi/coot-videotext)]
    * **Parameter-Reduction**: "Parameter Efficient Multimodal Transformers for Video Representation Learning", ICLR, 2021 (*Seoul National University*). [[Paper](https://openreview.net/forum?id=6UdQLhqJyFD)]
    * **ClipBERT**: "Less is More: ClipBERT for Video-and-Language Learning via Sparse Sampling", CVPR, 2021 (*UNC + Microsoft*). [[Paper](https://arxiv.org/abs/2102.06183)][[PyTorch](https://github.com/jayleicn/ClipBERT)]
    * **VLM**: "VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding", ACL Findings, 2021 (*Facebook*). [[Paper](https://arxiv.org/abs/2105.09996)][[PyTorch](https://github.com/facebookresearch/fairseq/tree/main/examples/MMPT)]
    * **VideoCLIP**: "VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding", EMNLP, 2021 (*Facebook*). [[Paper](https://arxiv.org/abs/2109.14084)][[PyTorch](https://github.com/facebookresearch/fairseq/tree/main/examples/MMPT)]
    * **VALUE**: "VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation", NeurIPS (Datasets and Benchmarks), 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2106.04632)][[Website](https://value-benchmark.github.io/)]
    * **TAN**: "Temporal Alignment Networks for Long-term Video", CVPR, 2022 (*Oxford*). [[Paper](https://arxiv.org/abs/2204.02968)][[Code (in construction)](https://github.com/TengdaHan/TemporalAlignNet)][[Website](https://www.robots.ox.ac.uk/~vgg/research/tan/)]
    * **HD-VILA**: "Advancing High-Resolution Video-Language Representation with Large-Scale Video Transcriptions", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.10337)][[GitHub](https://github.com/microsoft/XPretrain)]
    * **ATP**: "Revisiting the "Video" in Video-Language Understanding", CVPR, 2022 (*Stanford*). [[Paper](https://arxiv.org/abs/2206.01720)][[Website](https://stanfordvl.github.io/atp-revisit-video-lang/)]
    * **ALPRO**: "Align and Prompt: Video-and-Language Pre-training with Entity Prompts", CVPR, 2022 (*Salesforce*). [[Paper](https://arxiv.org/abs/2112.09583)][[PyTorch](https://github.com/salesforce/ALPRO)]
    * **CLOP**: "CLOP: Video-and-Language Pre-Training with Knowledge Regularizations", ACMMM, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2211.03314)]
    * **LocVTP**: "LocVTP: Video-Text Pre-training for Temporal Localization", ECCV, 2022 (*Peking University*). [[Paper](https://arxiv.org/abs/2207.10362)][[PyTorch](https://github.com/mengcaopku/LocVTP)]
    * **FineCo**: "Contrastive Video-Language Learning with Fine-grained Frame Sampling", AACL, 2022 (*ICL, UK*). [[Paper](https://arxiv.org/abs/2210.05039)]
    * **EMCL**: "Expectation-Maximization Contrastive Learning for Compact Video-and-Language Representations", NeurIPS, 2022 (*Peking University*). [[Paper](https://arxiv.org/abs/2211.11427)][[PyTorch](https://github.com/jpthu17/EMCL)]
    * **LF-VILA**: "Long-Form Video-Language Pre-Training with Multimodal Temporal Contrastive Learning", NeurIPS, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2210.06031)][[GitHub](https://github.com/microsoft/XPretrain)]
    * **VATT-GR-CL**: "Scaling Multimodal Pre-Training via Cross-Modality Gradient Harmonization", NeurIPS, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2211.02077)]
    * **LGDN**: "LGDN: Language-Guided Denoising Network for Video-Language Modeling", NeurIPS, 2022 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2209.11388)]
    * **EgoVLP**: "Egocentric Video-Language Pretraining", NeurIPS, 2022 (*NUS*). [[Paper](https://arxiv.org/abs/2206.01670)][[PyTorch](https://github.com/showlab/EgoVLP)][[Website](https://qinghonglin.github.io/EgoVLP/)]
    * **LiteVL**: "LiteVL: Efficient Video-Language Learning with Enhanced Spatial-Temporal Modeling", EMNLP, 2022 (*Peking University*). [[Paper](https://arxiv.org/abs/2210.11929)]
    * **Singularity**: "Revealing Single Frame Bias for Video-and-Language Learning", arXiv, 2022 (*UNC*). [[Paper](https://arxiv.org/abs/2206.03428)]
    * **VIOLET**: "VIOLET: End-to-End Video-Language Transformers with Masked Visual-token Modeling", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.12681)][[PyTorch](https://github.com/tsujuifu/pytorch_violet)]
    * **SimVTP**: "SimVTP: Simple Video Text Pre-training with Masked Autoencoders", arXiv, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2212.03490)][[PyTorch (in construction)](https://github.com/mayuelala/SimVTP)]
    * **VideoCoCa**: "Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2212.04979)]
    * **i-Code**: "i-Code: An Integrative and Composable Multimodal Learning Framework", AAAI, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2205.01818)][[Code (in construction)](https://github.com/microsoft/i-Code)]
    * **TempCLR**: "TempCLR: Temporal Alignment Representation with Contrastive Learning", ICLR, 2023 (*Columbia*). [[Paper](https://arxiv.org/abs/2212.13738)]
    * **MELTR**: "MELTR: Meta Loss Transformer for Learning to Fine-tune Video Foundation Models", CVPR, 2023 (*Korea University*). [[Paper](https://arxiv.org/abs/2303.13009)][[PyTorch](https://github.com/mlvlab/MELTR)]
    * **VIOLETv2**: "An Empirical Study of End-to-End Video-Language Transformers with Masked Visual Modeling", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2209.01540)][[PyTorch](https://github.com/tsujuifu/pytorch_empirical-mvm)]
    * **LAVENDER**: "LAVENDER: Unifying Video-Language Understanding as Masked Language Modeling", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2206.07160)][[Code (in construction)](https://github.com/microsoft/LAVENDER)]
    * **SViTT**: "SViTT: Temporal Learning of Sparse Video-Text Transformers", CVPR, 2023 (*Intel*). [[Paper](https://arxiv.org/abs/2304.08809)][[Website](http://svcl.ucsd.edu/projects/svitt/)]
    * **TVTS**: "Learning Transferable Spatiotemporal Representations from Natural Script Knowledge", CVPR, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2209.15280)][[PyTorch](https://github.com/TencentARC/TVTS/tree/master/v1)]
    * **HBI**: "Video-Text as Game Players: Hierarchical Banzhaf Interaction for Cross-Modal Representation Learning", CVPR, 2023 (*Peking University*). [[Paper](https://arxiv.org/abs/2303.14369)][[Code (in construction)](https://github.com/jpthu17/HBI)][[Website](https://jpthu17.github.io/HBI/)]
    * **All-in-One**: "All in One: Exploring Unified Video-Language Pre-training", CVPR, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2203.07303)][[PyTorch](https://github.com/showlab/all-in-one)]
    * **VindLU**: "VindLU: A Recipe for Effective Video-and-Language Pretraining", CVPR, 2023 (*UNC*). [[Paper](https://arxiv.org/abs/2212.05051)][[PyTorch](https://github.com/klauscc/VindLU)]
    * **Clover**: "Clover: Towards A Unified Video-Language Alignment and Fusion Model", CVPR, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2207.07885)][[PyTorch (in construction)](https://github.com/LeeYN-43/Clover)]
    * **mPLUG-2**: "mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video", ICML, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2302.00402)][[Code (in construction)](https://github.com/alibaba/AliceMind)]
    * **BUS**: "BUS: Efficient and Effective Vision-language Pre-training with Bottom-Up Patch Summarization", ICCV, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2307.08504)]
    * **UMT**: "Unmasked Teacher: Towards Training-Efficient Video Foundation Models", ICCV, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2303.16058)][[PyTorch](https://github.com/OpenGVLab/unmasked_teacher)]
    * **?**: "Long-range Multimodal Pretraining for Movie Understanding", ICCV, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2308.09775)]
    * **EgoVLPv2**: "EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone", ICCV, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2307.05463)][[PyTorch](https://github.com/facebookresearch/EgoVLPv2)][[Website](https://shramanpramanick.github.io/EgoVLPv2/)]
    * **SMAUG**: "SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-training", ICCV, 2023 (*UW*). [[Paper](https://arxiv.org/abs/2211.11446)]
    * **VFC**: "Verbs in Action: Improving verb understanding in video-language models", ICCV, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2304.06708)]
    * **HiTeA**: "HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training", ICCV, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2212.14546)]
    * **TW-BERT**: "Learning Trajectory-Word Alignments for Video-Language Tasks", ICCV, 2023 (*Southeast University, China*). [[Paper](https://arxiv.org/abs/2301.01953)]
    * **TESTA**: "TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding", EMNLP Findings, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2310.19060)][[PyTorch](https://github.com/RenShuhuai-Andy/TESTA)]
    * **STOA-VLP**: "STOA-VLP: Spatial-Temporal Modeling of Object and Action for Video-Language Pre-training", arXiv, 2023 (*Harbin Institute of Technology*). [[Papaer](https://arxiv.org/abs/2302.09736)]
    * **G-ViLM**: "Spatiotemporally Discriminative Video-Language Pre-Training with Text Grounding", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.16341)]
    * **VLAB**: "VLAB: Enhancing Video Language Pre-training by Feature Adapting and Blending", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2305.13167)]
    * **TVTSv2**: "TVTSv2: Learning Out-of-the-box Spatiotemporal Visual Representations at Scale", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2305.14173)][[Code (in construction)](https://github.com/TencentARC/TVTS/tree/master/v2)]
    * **Youku-mPLUG**: "Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2306.04362)]
    * **VideoGLUE**: "VideoGLUE: Video General Understanding Evaluation of Foundation Models", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2307.03166)]
    * **InternVid**: "InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2307.06942)][[Code (in construction)](https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid)]
    * **EVE**: "EVE: Efficient Vision-Language Pre-training with Masked Prediction and Modality-Aware MoE", arXiv, 2023 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2308.11971)]
    * **Qwen-VL**: "Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2308.12966)][[PyTorch](https://github.com/QwenLM/Qwen-VL)]
    * **BT-Adapter**: "One For All: Video Conversation is Feasible Without Video Instruction Tuning", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2309.15785)]
    * **?**: "Harvest Video Foundation Models via Efficient Post-Pretraining", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2310.19554)][[PyTorch](https://github.com/OpenGVLab/InternVideo)]
    * **Owl-Con**: "VideoCon: Robust Video-Language Alignment via Contrast Captions", arXiv, 2023 (*UCLA*). [[Paper](https://arxiv.org/abs/2311.10111)][[PyTorch](https://github.com/Hritikbansal/videocon)]
    * **ShareGPT4V**: "ShareGPT4V: Improving Large Multi-Modal Models with Better Captions", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2311.12793)][[Code (in construction)](https://github.com/InternLM/InternLM-XComposer/tree/main/projects/ShareGPT4V)][[Website](https://sharegpt4v.github.io/)]
    * **Vamos**: "Vamos: Versatile Action Models for Video Understanding", arXiv, 2023 (*Brown*). [[Paper](https://arxiv.org/abs/2311.13627)][[Website](https://brown-palm.github.io/Vamos/)]
    * **EILEV**: "Efficient In-Context Learning in Vision-Language Models for Egocentric Videos", arXiv, 2023 (*UMich*). [[Paper](https://arxiv.org/abs/2311.17041)]
    * **E-ViLM**: "E-ViLM: Efficient Video-Language Model via Masked Video Modeling with Semantic Vector-Quantized Tokenizer", arXiv, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2311.17267)]
    * **?**: "A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames", arXiv, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2312.07395)]
    * **NSVA**: "A Strong Baseline for Temporal Video-Text Alignment", arXiv, 2023 (*SJTU*). [[Paper](https://arxiv.org/abs/2312.14055)][[Website](https://lzq5.github.io/Video-Text-Alignment/)]
    * **debias-VL**: "Debiasing Vision-Language Models via Biased Prompts", arXiv, 2023 (*MIT*). [[Paper](https://arxiv.org/abs/2302.00070)][[PyTorch](https://github.com/chingyaoc/debias_vl)]
    * **MobileVLM**: "MobileVLM: A Fast, Reproducible and Strong Vision Language Assistant for Mobile Devices", arXiv, 2023 (*Meituan*). [[Paper](https://arxiv.org/abs/2312.16886)][[Code (in construction)](https://github.com/Meituan-AutoML/MobileVLM)]
    * **READ-PVLA**: "READ-PVLA: Recurrent Adapter with Partial Video-Language Alignment for Parameter-Efficient Transfer Learning in Low-Resource Video-Language Modeling", AAAI, 2024 (*NUS*). [[Paper](https://arxiv.org/abs/2312.06950)]
    * **Panda-70M**: "Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers", CVPR, 2024 (*Snap*). [[Paper](https://arxiv.org/abs/2402.19479)][[PyTorch](https://github.com/snap-research/Panda-70M)][[Website](https://snap-research.github.io/Panda-70M/)]
    * **VIIT**: "Distilling Vision-Language Models on Millions of Videos", arXiv, 2024 (*Google*). [[Paper](https://arxiv.org/abs/2401.06129)][[Website](https://zhaoyue-zephyrus.github.io/video-instruction-tuning/)]
    * **FiGCLIP**: "FiGCLIP: Fine-Grained CLIP Adaptation via Densely Annotated Videos", arXiv, 2024 (*IIIT Hyderabad*). [[Paper](https://arxiv.org/abs/2401.07669)][[Code (in construction)](https://github.com/Darshansingh11/FiGCLIP)]
    * **LWM**: "World Model on Million-Length Video And Language With RingAttention", arXiv, 2024 (*Berkeley*). [[Paper](https://arxiv.org/abs/2402.08268)][[JAX](https://github.com/LargeWorldModel/LWM)][[Website](https://largeworldmodel.github.io/)]
    * **VideoPrism**: "VideoPrism: A Foundational Visual Encoder for Video Understanding", arXiv, 2024 (*Google*). [[Paper](https://arxiv.org/abs/2402.13217)]
    * **Slot-VLM**: "Slot-VLM: SlowFast Slots for Video-Language Modeling", arXiv, 2024 (*Microsoft*). [[Paper](https://arxiv.org/abs/2402.13088)]
    * **MobileVLM-V2**: "MobileVLM V2: Faster and Stronger Baseline for Vision Language Model", arXiv, 2024 (*Meituan*). [[Paper](https://arxiv.org/abs/2402.03766)][[PyTorch](https://github.com/Meituan-AutoML/MobileVLM)]
* 3D:
    * **CLIP<sup>2</sup>**: "CLIP<sup>2</sup>: Contrastive Language-Image-Point Pretraining from Real-World Point Cloud Data", CVPR, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2303.12417)]
    * **3D-VLP**: "Context-aware Alignment and Mutual Masking for 3D-Language Pre-training", CVPR, 2023 (*Sichuan University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Jin_Context-Aware_Alignment_and_Mutual_Masking_for_3D-Language_Pre-Training_CVPR_2023_paper.html)][[PyTorch](https://github.com/leolyj/3D-VLP)]
    * **SDFusion**: "SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation", CVPR, 2023 (*Snap*). [[Paper](https://arxiv.org/abs/2212.04493)][[PyTorch](https://github.com/yccyenchicheng/SDFusion)][[Website](https://yccyenchicheng.github.io/SDFusion/)]
    * **3D-VisTA**: "3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment", ICCV, 2023 (*Beijing Institute for General Artificial Intelligence (BIGAI)*). [[Paper](https://arxiv.org/abs/2308.04352)][[PyTorch](https://github.com/3d-vista/3D-VisTA)][[Website](https://3d-vista.github.io/)]
    * **RegionPLC**: "RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding", arXiv, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2304.00962)][[Website](https://jihanyang.github.io/projects/RegionPLC)]
    * **3DVLP**: "Vision-Language Pre-training with Object Contrastive Learning for 3D Scene Understanding", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2305.10714)]
    * **CLIPXPlore**: "CLIPXPlore: Coupled CLIP and Shape Spaces for 3D Shape Exploration", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2306.08226)]
    * **Point-PEFT**: "Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2310.03059)][[PyTorch](https://github.com/EvenJoker/Point-PEFT)]
* Vision-Audio-Text:
    * **VATT**: "VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text", NeurIPS, 2021 (*Google*). [[Paper](https://arxiv.org/abs/2104.11178)][[Tensorflow](https://github.com/google-research/google-research/tree/master/vatt)]
    * **VideoCC**: "Learning Audio-Video Modalities from Image Captions", ECCV, 2022 (*Google*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4217_ECCV_2022_paper.php)][[Website](https://a-nagrani.github.io/videocc.html)]
    * **MUGEN**: "MUGEN: A Playground for Video-Audio-Text Multimodal Understanding and GENeration", ECCV, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2204.08058)][[Website](https://mugen-org.github.io/)]
    * **VATLM**: "VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2211.11275)][[PyTorch](https://github.com/microsoft/SpeechT5/tree/main/VATLM)]
    * **CLIP4VLA**: "Accommodating Audio Modality in CLIP for Multimodal Processing", AAAI, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2303.06591)]
    * **data2vec-2.0**: "Efficient Self-supervised Learning with Contextualized Target Representations for Vision, Speech and Language", ICML, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2212.07525)][[PyTorch](https://github.com/facebookresearch/fairseq/tree/main/examples/data2vec)]
    * **VAST**: "VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset", NeurIPS, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2305.18500)][[Code (in construction)](https://github.com/TXH-mercury/VAST)]
    * **VALOR**: "VALOR: Vision-Audio-Language Omni-Perception Pretraining Model and Dataset", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2304.08345)][[PyTorch](https://github.com/TXH-mercury/VALOR)][[Website](https://casia-iva-group.github.io/projects/VALOR/)]
* More than 3 modalities:
    * **Meta-Transformer**: "Meta-Transformer: A Unified Framework for Multimodal Learning", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2307.10802)][[Code (in construction)](https://github.com/invictus717/MetaTransformer)][[Website](https://kxgong.github.io/meta_transformer/)]
    * **UnIVAL**: "Unified Model for Image, Video, Audio and Language Tasks", arXiv, 2023 (*Sorbonne University, France*). [[Paper](https://arxiv.org/abs/2307.16184)][[PyTorch](https://github.com/mshukor/UnIVAL)][[Website](https://unival-model.github.io/)]
    * **ViT-Lens**: "ViT-Lens: Towards Omni-modal Representations", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2308.10185)][[PyTorch](https://github.com/TencentARC/ViT-Lens)]
    * **ViT-Lens-2**: "ViT-Lens-2: Gateway to Omni-modal Intelligence", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2311.16081)][[PyTorch](https://github.com/TencentARC/ViT-Lens)]
    * **ModaVerse**: "ModaVerse: Efficiently Transforming Modalities with LLMs", arXiv, 2024 (*University of Adelaide*). [[Paper](https://arxiv.org/abs/2401.06395)]
    * **M2PT**: "Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities", arXiv, 2024 (*CUHK*). [[Paper](https://arxiv.org/abs/2401.14405)][[PyTorch](https://github.com/AILab-CVC/M2PT)][[Website](https://ailab-cvc.github.io/M2PT/)]
    * **DAMC**: "Model Composition for Multimodal Large Language Models", arXiv, 2024 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2402.12750)]
* Others:
    * **C-MCR**: "Connecting Multi-modal Contrastive Representations", NeurIPS, 2023 (*Zhejiang*). [[Paper](https://arxiv.org/abs/2305.14381)][[PyTorch](https://github.com/MCR-PEFT/C-MCR)][[Website](https://c-mcr.github.io/C-MCR/)]
    * **UMI**: "Learning Unseen Modality Interaction", NeurIPS, 2023 (*UvA*). [[Paper](https://arxiv.org/abs/2306.12795)][[Code (in construction)](https://github.com/xiaobai1217/Unseen-Modality-Interaction)][[Website](https://xiaobai1217.github.io/Unseen-Modality-Interaction/)]
    * **TVL**: "A Touch, Vision, and Language Dataset for Multimodal Alignment", arXiv, 2024 (*Berkeley*). [[Paper](https://arxiv.org/abs/2402.13232)][[PyTorch](https://github.com/Max-Fu/tvl)][[Website](https://tactile-vlm.github.io/)]

[[Back to Overview](#overview)]

### Multi-Modal Retrieval
* General:
    * **Fast-and-Slow**: "Thinking Fast and Slow: Efficient Text-to-Visual Retrieval with Transformers", CVPR, 2021 (*DeepMind*). [[Paper](https://arxiv.org/abs/2103.16553)]
    * **HTR**: "Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning", CVPR, 2021 (*Amazon*). [[Paper](https://arxiv.org/abs/2103.13061)][[PyTorch](https://github.com/amzn/image-to-recipe-transformers)]
    * **TERN**: "Towards Efficient Cross-Modal Visual Textual Retrieval using Transformer-Encoder Deep Features", CBMI, 2021 (*National Research Council, Italy*). [[Paper](https://arxiv.org/abs/2106.00358)]
    * **VisualSparta**: "VisualSparta: Sparse Transformer Fragment-level Matching for Large-scale Text-to-Image Search", arXiv, 2021 (*CMU*). [[Paper](https://arxiv.org/abs/2101.00265)]
    * **CCR-CCS**: "More Than Just Attention: Learning Cross-Modal Attentions with Contrastive Constraints", arXiv, 2021 (*Rutgers + Amazon*). [[Paper](https://arxiv.org/abs/2105.09597)]
    * **MCProp**: "Transformer-Based Multi-modal Proposal and Re-Rank for Wikipedia Image-Caption Matching", ICLRW, 2022 (*National Research Council, Italy*). [[Paper](https://arxiv.org/abs/2206.10436)][[PyTorch](https://github.com/mesnico/Wiki-Image-Caption-Matching)]
    * **TASK-former**: "A Sketch Is Worth a Thousand Words: Image Retrieval with Text and Sketch", ECCV, 2022 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2208.03354)][[Website](https://patsorn.me/projects/tsbir/)]
    * **CODER**: "CODER: Coupled Diversity-Sensitive Momentum Contrastive Learning for Image-Text Retrieval", ECCV, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2208.09843)]
    * **?**: "Most and Least Retrievable Images in Visual-Language Query Systems", ECCV, 2022 (*Old Dominion University, Virginia*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7050_ECCV_2022_paper.php)]
    * **MACK**: "MACK: Multimodal Aligned Conceptual Knowledge for Unpaired Image-text Matching", NeurIPS, 2022 (*CAS*). [[Paper](https://openreview.net/forum?id=7lf58jWnDIS)]
    * **MLA**: "Multi-Lingual Acquisition on Multimodal Pre-training for Cross-modal Retrieval", NeurIPS, 2022 (*Renmin University of China*). [[Paper](https://openreview.net/forum?id=h73nTbImOt9)]
    * **SpeechCLIP**: "SpeechCLIP: Integrating Speech with Pre-Trained Vision and Language Model", IEEE Workshop on Spoken Language Technology (SLT), 2022 (*NTU*). [[Paper](https://arxiv.org/abs/2210.00705)]
    * **LoopITR**: "LoopITR: Combining Dual and Cross Encoder Architectures for Image-Text Retrieval", arXiv, 2022 (*UNC*). [[Paper](https://arxiv.org/abs/2203.05465)]
    * **TNLBT**: "Transformer-based Cross-Modal Recipe Embeddings with Large Batch Training", arXiv, 2022 (*The University of Electro-Communications, Japan*). [[Paper](https://arxiv.org/abs/2205.04948)]
    * **HiVLP**: "HiVLP: Hierarchical Vision-Language Pre-Training for Fast Image-Text Retrieval", arXiv, 2022 (*Huawei*). [[Paper](https://arxiv.org/abs/2205.12105)]
    * **?**: "Revising Image-Text Retrieval via Multi-Modal Entailment". arXiv, 2022 (*Soochow University, China*). [[Paper](https://arxiv.org/abs/2208.10126)]
    * **TokenFlow**: "TokenFlow: Rethinking Fine-grained Cross-modal Alignment in Vision-Language Retrieval", arXiv, 2022 (*Kuaishou*). [[Paper](https://arxiv.org/abs/2209.13822)]
    * **VLPCook**: "Structured Vision-Language Pretraining for Computational Cooking", arXiv, 2022 (*Sorbonne University, France*). [[Paper](https://arxiv.org/abs/2212.04267)]
    * **UniVL-DR**: "Universal Vision-Language Dense Retrieval: Learning A Unified Representation Space for Multi-Modal Retrieval", ICLR, 2023 (*Northeastern University, China*). [[Paper](https://openreview.net/forum?id=PQOlkgsBsik)]
    * **HREM**: "Learning Semantic Relationship Among Instances for Image-Text Matching", CVPR, 2023 (*USTC*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Fu_Learning_Semantic_Relationship_Among_Instances_for_Image-Text_Matching_CVPR_2023_paper.html)]
    * **CHAN**: "Fine-Grained Image-Text Matching by Cross-Modal Hard Aligning Network", CVPR, 2023 (*Zhejiang University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Pan_Fine-Grained_Image-Text_Matching_by_Cross-Modal_Hard_Aligning_Network_CVPR_2023_paper.html)][[PyTorch](https://github.com/ppanzx/CHAN)]
    * **ViLEM**: "ViLEM: Visual-Language Error Modeling for Image-Text Retrieval", CVPR, 2023 (*CAS*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Chen_ViLEM_Visual-Language_Error_Modeling_for_Image-Text_Retrieval_CVPR_2023_paper.html)]
    * **SoftMask**: "Multi-Modal Representation Learning with Text-Driven Soft Masks", CVPR, 2023 (*SNU*). [[Paper](https://arxiv.org/abs/2304.00719)]
    * **MetaPer**: "Meta-Personalizing Vision-Language Models To Find Named Instances in Video", CVPR, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2306.10169)][[PyTorch](https://github.com/danielchyeh/this-is-my)][[Website](https://danielchyeh.github.io/metaper/)]
    * **DivE**: "Improving Cross-Modal Retrieval with Set of Diverse Embeddings", CVPR, 2023 (*POSTECH*). [[Paper](https://arxiv.org/abs/2211.16761)][[Website](https://cvlab.postech.ac.kr/research/DivE/)]
    * **Pic2Word**: "Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2302.03084)][[PyTorch](https://github.com/google-research/composed_image_retrieval)]
    * **LexLIP**: "LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Retrieval", ICCV, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2302.02908)]
    * **SEARLE**: "Zero-Shot Composed Image Retrieval with Textual Inversion", ICCV, 2023 (*University of Florence, Italy*). [[Paper](https://arxiv.org/abs/2303.15247)][[PyTorch (in construction)](https://github.com/miccunifi/SEARLE)]
    * **VLSlice**: "VLSlice: Interactive Vision-and-Language Slice Discovery", ICCV, 2023 (*OSU*). [[Paper](https://arxiv.org/abs/2309.06703)][[PyTorch](https://github.com/slymane/vlslice)][[Website](https://ericslyman.com/vlslice/)]
    * **ConaCLIP**: "ConaCLIP: Exploring Distillation of Fully-Connected Knowledge Interaction Graph for Lightweight Text-Image Retrieval", ACL Industry Track, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2305.17652)][[PyTorch](https://github.com/alibaba/EasyNLP)]
    * **FNE**: "Your Negative May not Be True Negative: Boosting Image-Text Matching with False Negative Elimination", ACMMM, 2023 (*University of Electronic Science and Technology of China (UESTC)*). [[Paper](https://arxiv.org/abs/2308.04380)][[PyTorch](https://github.com/LuminosityX/FNE)]
    * **HAT**: "Unifying Two-Stream Encoders with Transformers for Cross-Modal Retrieval", ACMMM, 2023 (*University of Electronic Science and Technology of China (UESTC)*). [[Paper](https://arxiv.org/abs/2308.04343)][[PyTorch](https://github.com/LuminosityX/HAT)]
    * **STAIR**: "STAIR: Learning Sparse Text and Image Representation in Grounded Tokens", arXiv, 2023 (*Apple*). [[Paper](https://arxiv.org/abs/2301.13081)]
    * **ChatIR**: "Chatting Makes Perfect - Chat-based Image Retrieval", arXiv, 2023 (*The Hebrew University of Jerusalem, Israel*). [[Paper](https://arxiv.org/abs/2305.20062)]
    * **TransAgg**: "Zero-shot Composed Text-Image Retrieval", arXiv, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2306.07272)][[PyTorch](https://github.com/Code-kunkun/ZS-CIR)][[Website](https://code-kunkun.github.io/ZS-CIR/)]
    * **CUSA**: "Cross-Modal and Uni-Modal Soft-Label Alignment for Image-Text Retrieval", AAAI, 2024 (*Beihang University*). [[Paper](https://arxiv.org/abs/2403.05261)][[PyTorch](https://github.com/lerogo/aaai24_itr_cusa)]
    * **L2RM**: "Learning to Rematch Mismatched Pairs for Robust Cross-Modal Retrieval", CVPR, 2024 (*Xi'an Jiaotong*). [[Paper](https://arxiv.org/abs/2403.05105)][[Code (in construction)](https://github.com/hhc1997/L2RM)]
* Video:
    * **MMT**: "Multi-modal Transformer for Video Retrieval", ECCV, 2020 (*INRIA + Google*). [[Paper](https://arxiv.org/abs/2007.10639)][[Website](http://thoth.inrialpes.fr/research/MMT/)]
    * **AYCE**: "All You Can Embed: Natural Language based Vehicle Retrieval with Spatio-Temporal Transformers", CVPRW, 2021 (*University of Modena and Reggio Emilia*). [[Paper](https://arxiv.org/abs/2106.10153)][[PyTorch](https://github.com/cscribano/AYCE_2021)]
    * **HiT**: "HiT: Hierarchical Transformer with Momentum Contrast for Video-Text Retrieval", ICCV, 2021 (*Kuaishou*). [[Paper](https://arxiv.org/abs/2103.15049)]
    * **Frozen**: "Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval", ICCV, 2021 (*Oxford*). [[Paper](https://arxiv.org/abs/2104.00650)][[Pytorch](https://github.com/m-bain/frozen-in-time)][[Website](https://www.robots.ox.ac.uk/~vgg/research/frozen-in-time/)][[Dataset](https://m-bain.github.io/webvid-dataset/)]
    * **CLIP4Clip**: "CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval", arXiv, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2104.08860)][[PyTorch](https://github.com/ArrowLuo/CLIP4Clip)]
    * **MMFT**: "Everything at Once - Multi-modal Fusion Transformer for Video Retrieval", CVPR, 2022 (*Goethe University Frankfurt, Germany*). [[Paper](https://arxiv.org/abs/2112.04446)]
    * **X-Pool**: "X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval", CVPR, 2022 (*Layer 6 AI, Toronto*). [[Paper](https://arxiv.org/abs/2203.15086)][[PyTorch](https://github.com/layer6ai-labs/xpool)][[Website](https://layer6ai-labs.github.io/xpool/)]
    * **MVPt**: "It's Time for Artistic Correspondence in Music and Video", CVPR, 2022 (*Adobe*). [[Paper](https://arxiv.org/abs/2206.07148)][[Website](https://musicforvideo.cs.columbia.edu/)]
    * **OA-Trans**: "Object-aware Video-language Pre-training for Retrieval", CVPR, 2022 (*NUS*). [[Paper](https://arxiv.org/abs/2112.00656)][[PyTorch](https://github.com/FingerRec/OA-Transformer)]
    * **BridgeFormer**: "Bridging Video-text Retrieval with Multiple Choice Questions", CVPR, 2022 (*HKU*). [[Paper](https://arxiv.org/abs/2201.04850)][[PyTorch](https://github.com/TencentARC/MCQ)][[Website](https://geyuying.github.io/MCQ.html)]
    * **CenterCLIP**: "CenterCLIP: Token Clustering for Efficient Text-Video Retrieval", SIGIR, 2022 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2205.00823)]
    * **X-CLIP**: "X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval", ACMMM, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2207.07285)]
    * **HiSE**: "Boosting Video-Text Retrieval with Explicit High-Level Semantics", ACMMM, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2208.04215)]
    * **TS2-Net**: "TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval", ECCV, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2207.07852)][[PyTorch](https://github.com/yuqi657/ts2_net)]
    * **LAFF**: "Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video Retrieval", ECCV, 2022 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2112.01832)]
    * **ECLIPSE**: "ECLIPSE: Efficient Long-range Video Retrieval using Sight and Sound", ECCV, 2022 (*UNC*). [[Paper](https://arxiv.org/abs/2204.02874)][[PyTorch](https://github.com/GenjiB/ECLIPSE)][[Website](https://yanbo.ml/project_page/eclipse/)]
    * **MILES**: "MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval", ECCV, 2022 (*HKU*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1159_ECCV_2022_paper.php)][[PyTorch](https://github.com/TencentARC/MCQ/blob/main/MILES.md)]
    * **VTC**: "VTC: Improving Video-Text Retrieval with User Comments", ECCV, 2022 (*Unitary, UK*). [[Paper](https://arxiv.org/abs/2210.10820)][[PyTorch](https://github.com/unitaryai/VTC)][[Website](https://unitaryai.github.io/vtc-paper/)]
    * **LINAS**: "Learning Linguistic Association towards Efficient Text-Video Retrieval", ECCV, 2022 (*CAS*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3305_ECCV_2022_paper.php)][[PyTorch](https://github.com/silenceFS/LINAS)]
    * **?**: "A Simple Transformer-Based Model for Ego4D Natural Language Queries Challenge", ECCVW, 2022 (*UW-Madison*). [[Paper](https://arxiv.org/abs/2211.08704)]
    * **?**: "Text-Adaptive Multiple Visual Prototype Matching for Video-Text Retrieval", NeurIPS, 2022 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2209.13307)]
    * **ConTra**: "ConTra: (Con)text (Tra)nsformer for Cross-Modal Video Retrieval", ACCV, 2022 (*University of Bristol, UK*). [[Paper](https://arxiv.org/abs/2210.04341)]
    * **RaP**: "RaP: Redundancy-aware Video-language Pre-training for Text-Video Retrieval", EMNLP, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2210.06881)][[PyTorch](https://github.com/caskcsg/VLP/tree/main/RaP)]
    * **MDMMT-2**: "MDMMT-2: Multidomain Multimodal Transformer for Video Retrieval, One More Step Towards Generalization", arXiv, 2022 (*Huawei*). [[Paper](https://arxiv.org/abs/2203.07086)]
    * **M2HF**: "M2HF: Multi-level Multi-modal Hybrid Fusion for Text-Video Retrieval", arXiv, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2208.07664)]
    * **FIRE**: "Fighting FIRe with FIRE: Assessing the Validity of Text-to-Video Retrieval Benchmarks", arXiv, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2210.05038)][[PyTorch](https://github.com/facebookresearch/mm-retrieval-evaluation)]
    * **Cross-Modal-Adapter**: "Cross-Modal Adapter for Text-Video Retrieval", arXiv, 2022 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2211.09623)][[PyTorch (in construction)](https://github.com/LeapLabTHU/Cross-Modal-Adapter)]
   * **MAC**: "Masked Contrastive Pre-Training for Efficient Video-Text Retrieval", arXiv, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2212.00986)]
   * **CLIP-ViP**: "CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment", ICLR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2209.06430)][[Code (in construction)](https://github.com/microsoft/XPretrain/tree/main/CLIP-ViP)]
    * **HiREST**: "Hierarchical Video-Moment Retrieval and Step-Captioning", CVPR, 2023 (*UNC + Meta*). [[Paper](https://arxiv.org/abs/2303.16406)][[PyTorch](https://github.com/j-min/HiREST)][[Website](https://hirest-cvpr2023.github.io/)]
    * **Cap4Video**: "Cap4Video: What Can Auxiliary Captions Do for Text-Video Retrieval?", CVPR, 2023 (*The University of Sydney*). [[Paper](https://arxiv.org/abs/2301.00184)][[PyTorch](https://github.com/whwu95/Cap4Video)]
    * **CLIPPING**: "CLIPPING: Distilling CLIP-Based Models With a Student Base for Video-Language Retrieval", CVPR, 2023 (*Huawei*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Pei_CLIPPING_Distilling_CLIP-Based_Models_With_a_Student_Base_for_Video-Language_CVPR_2023_paper.html)]
    * **CNVid-3.5M**: "CNVid-3.5M: Build, Filter, and Pre-Train the Large-Scale Public Chinese Video-Text Dataset", CVPR, 2023 (*Ant Group*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Gan_CNVid-3.5M_Build_Filter_and_Pre-Train_the_Large-Scale_Public_Chinese_Video-Text_CVPR_2023_paper.html)][[GitHub (in construction)](https://github.com/CNVid/CNVid-3.5M)]
    * **CelebV-Text**: "CelebV-Text: A Large-Scale Facial Text-Video Dataset", CVPR, 2023 (*University of Sydney*). [[Paper](https://arxiv.org/abs/2303.14717)][[GitHub](https://github.com/CelebV-Text/CelebV-Text)][[Website](https://celebv-text.github.io/)]
    * **ReST**: "Relational Space-Time Query in Long-Form Videos", CVPR, 2023 (*Meta*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Relational_Space-Time_Query_in_Long-Form_Videos_CVPR_2023_paper.html)]
    * **NaQ**: "NaQ: Leveraging Narrations as Queries to Supervise Episodic Memory", CVPR, 2023 (*UT Austin*). [[Paper](https://arxiv.org/abs/2301.00746)][[PyTorch](https://github.com/srama2512/NaQ)][[Website](https://vision.cs.utexas.edu/projects/naq/)]
    * **?**: "Towards Fast Adaptation of Pretrained Contrastive Models for Multi-channel Video-Language Retrieval", CVPR, 2023 (*Columbia*). [[Paper](https://arxiv.org/abs/2206.02082)][[Code (in contruction)](https://github.com/XudongLinthu/upgradable-multimodal-intelligence)]
    * **VoP**: "VoP: Text-Video Co-operative Prompt Tuning for Cross-Modal Retrieval", CVPR, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2211.12764)][[Code (in construction)](https://github.com/bighuang624/VoP)][[Website](https://kyonhuang.top/publication/text-video-cooperative-prompt-tuning)]
    * **SpotEM**: "SpotEM: Efficient Video Search for Episodic Memory", ICML, 2023 (*UT Austin*). [[Paper](https://arxiv.org/abs/2306.15850)][[Website](https://vision.cs.utexas.edu/projects/spotem/)]
    * **PromptSwitch**: "Prompt Switch: Efficient CLIP Adaptation for Text-Video Retrieval", ICCV, 2023 (*University of Adelaide*). [[Paper](https://arxiv.org/abs/2308.07648)][[PyTorch](https://github.com/bladewaltz1/PromptSwitch)]
    * **?**: "Simple Baselines for Interactive Video Retrieval with Questions and Answers", ICCV, 2023 (*Princeton*). [[Paper](https://arxiv.org/abs/2308.10402)][[PyTorch](https://github.com/kevinliang888/IVR-QA-baselines)]
    * **MeVTR**: "Multi-event Video-Text Retrieval", ICCV, 2023 (*LMU Munich*). [[Paper](https://arxiv.org/abs/2308.11551)][[PyTorch](https://github.com/gengyuanmax/MeVTR)]
    * **In-Style**: "In-Style: Bridging Text and Uncurated Videos with Style Transfer for Text-Video Retrieval", ICCV, 2023 (*MPI*). [[Paper](https://arxiv.org/abs/2309.08928)][[Code (in construction)](https://github.com/ninatu/in_style)]
    * **UCoFiA**: "Unified Coarse-to-Fine Alignment for Video-Text Retrieval", ICCV, 2023 (*UNC*). [[Paper](https://arxiv.org/abs/2309.10091)][[PyTorch](https://github.com/Ziyang412/UCoFiA)]
    * **TEFAL**: "Audio-Enhanced Text-to-Video Retrieval using Text-Conditioned Feature Alignment", ICCV, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2307.12964)]
    * **DiffusionRet**: "DiffusionRet: Generative Text-Video Retrieval with Diffusion Model", ICCV, 2023 (*Peking University*). [[Paper](https://arxiv.org/abs/2303.09867)][[PyTorch](https://github.com/jpthu17/DiffusionRet)]
    * **UATVR**: "UATVR: Uncertainty-Adaptive Text-Video Retrieval", ICCV, 2023 (*Baidu*). [[Paper](https://arxiv.org/abs/2301.06309)][[PyTorch](https://github.com/bofang98/UATVR)]
    * **In-Style**: "In-Style: Bridging Text and Uncurated Videos with Style Transfer for Text-Video Retrieval", ICCV, 2023 (*Goethe University Frankfurt, Germany*). [[Paper](https://arxiv.org/abs/2309.08928)][[Code (in construction)](https://github.com/ninatu/in_style)]
    * **ReGaDa**: "Video-adverb retrieval with compositional adverb-action embeddings", BMVC, 2023 (*University of Tbingen, Germany*). [[Paper](https://arxiv.org/abs/2309.15086)][[Code (in construction)](https://github.com/ExplainableML/ReGaDa)][[Website](https://hummelth.github.io/ReGaDa/)]
    * **TextVR**: "A Large Cross-Modal Video Retrieval Dataset with Reading Comprehension", arXiv, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2305.03347)][[PyTorch](https://github.com/callsys/TextVR)][[Website](https://sites.google.com/view/loveucvpr23/guest-track)]
    * **MASCOT**: "Mask to reconstruct: Cooperative Semantics Completion for Video-text Retrieval", arXiv, 2023 (*?*). [[Paper](https://arxiv.org/abs/2305.07910)]
    * **CrossTVR**: "Fine-grained Text-Video Retrieval with Frozen Image Encoders", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2307.09972)]
    * **TeachCLIP**: "TeachCLIP: Multi-Grained Teaching for Efficient Text-to-Video Retrieval", arXiv, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2308.01217)]
    * **CoVR**: "CoVR: Learning Composed Video Retrieval from Web Video Captions", arXiv, 2023 (*Ecole des Ponts ParisTech (ENPC), France*). [[Paper](https://arxiv.org/abs/2308.14746)][[PyTorch](https://github.com/lucas-ventura/CoVR/)][[Website](https://imagine.enpc.fr/~ventural/covr/)]
    * **LanguageBind**: "LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment", arXiv, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2310.01852)][[PyTorch](https://github.com/PKU-YuanGroup/LanguageBind)]
    * **10k-Words**: "A Video is Worth 10,000 Words: Training and Benchmarking with Diverse Captions for Better Long Video Retrieval", arXiv, 2023 (*SRI*). [[Paper](https://arxiv.org/abs/2312.00115)][[Website](https://mgwillia.github.io/10k-words/)]
    * **DGL**: "DGL: Dynamic Global-Local Prompt Tuning for Text-Video Retrieval", AAAI, 2024 (*University of Technology Sydney*). [[Paper](https://arxiv.org/abs/2401.10588)]
    * **?**: "Detours for Navigating Instructional Videos", arXiv, 2024 (*Meta*). [[Paper](https://arxiv.org/abs/2401.01823)]
* Vision-Audio-Text:
    * **Multi-SK**: "Preserving Modality Structure Improves Multi-Modal Learning", ICCV, 2023 (*UCF*). [[Paper](https://arxiv.org/abs/2308.13077)][[Code (in construction)](https://github.com/Swetha5/Multi_Sinkhorn_Knopp)]
* Others:
    * **IRRA**: "Cross-Modal Implicit Relation Reasoning and Aligning for Text-to-Image Person Retrieval", CVPR, 2023 (*Wuhan University*). [[Paper](https://arxiv.org/abs/2303.12501)][[PyTorch](https://github.com/anosorae/IRRA)]
    * **ZS-SBIR**: "CLIP for All Things Zero-Shot Sketch-Based Image Retrieval, Fine-Grained or Not", CVPR, 2023 (*University of Surrey, UK*). [[Paper](https://arxiv.org/abs/2303.13440)][[PyTorch](https://github.com/aneeshan95/Sketch_LVM)]
    * **ViML**: "Language-Guided Music Recommendation for Video via Prompt Analogies", CVPR, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2306.09327)][[Website](https://www.danielbmckee.com/language-guided-music-for-video/)]
    * **eP-ALM**: "eP-ALM: Efficient Perceptual Augmentation of Language Models", ICCV, 2023 (*Sorbonne University, France*). [[Paper](https://arxiv.org/abs/2303.11403)][[PyTorch](https://github.com/mshukor/eP-ALM)]
    * **Auto-ACD**: "A Large-scale Dataset for Audio-Language Representation Learning", arXiv, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2309.11500)][[Code (in construction)](https://github.com/LoieSun/Auto-ACD)][[Website](https://auto-acd.github.io/)]

[[Back to Overview](#overview)]

### Multi-Modal Generation
* General:
    * **AttnGAN**: "AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks", CVPR, 2018 (*Microsoft*). [[Paper](https://arxiv.org/abs/1711.10485)][[PyTorch](https://github.com/taoxugit/AttnGAN)]
    * **ControlGAN**: "Controllable Text-to-Image Generation", NeurIPS, 2019 (*Oxford*). [[Paper](https://arxiv.org/abs/1909.07083)][[PyTorch](https://github.com/mrlibw/ControlGAN)]
    * **DALL-E**: "Zero-Shot Text-to-Image Generation", ICML, 2021 (*OpenAI*). [[Paper](https://arxiv.org/abs/2102.12092)][[PyTorch](https://github.com/openai/DALL-E)][[PyTorch (lucidrains)](https://github.com/lucidrains/DALLE-pytorch)]
    * **CogView**: "CogView: Mastering Text-to-Image Generation via Transformers", NeurIPS, 2021 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2105.13290)][[PyTorch](https://github.com/THUDM/CogView)][[Website](https://lab.aminer.cn/cogview/index.html)]
    * **Layout-VQGAN**: "Text-to-Image Synthesis Based on Object-Guided Joint-Decoding Transformer", CVPR, 2022 (*CAS*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Wu_Text-to-Image_Synthesis_Based_on_Object-Guided_Joint-Decoding_Transformer_CVPR_2022_paper.html)]
    * **Lafite**: "Towards Language-Free Training for Text-to-Image Generation", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.13792)][[PyTorch](https://github.com/drboog/Lafite)]
    * **LDM**: "High-Resolution Image Synthesis with Latent Diffusion Models", CVPR, 2022 (*LMU Munich*). [[Paper](https://arxiv.org/abs/2112.10752)][[PyTorch](https://github.com/CompVis/latent-diffusion)]
    * **AvatarCLIP**: "AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars", SIGGRAPH, 2022 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2205.08535)][[PyTorch](https://github.com/hongfz16/AvatarCLIP)][[Website](https://hongfz16.github.io/projects/AvatarCLIP.html)]
    * **StoryDALL-E**: "StoryDALL-E: Adapting Pretrained Text-to-Image Transformers for Story Continuation", ECCV, 2022 (*UNC*). [[Paper](https://arxiv.org/abs/2209.06192)][[PyTorch](https://github.com/adymaharana/storydalle)]	
    * **Make-A-Scene**: "Make-A-Scene: Scene-Based Text-to-Image Generation with Human Priors", ECCV, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2203.13131)][[Video](https://www.youtube.com/watch?v=QLTyqoJJKTo&ab_channel=OranGafni)]
    * **TCTIG**: "Trace Controlled Text to Image Generation", ECCV, 2022 (*Beihang University*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1894_ECCV_2022_paper.php)]
    * **CogView2**: "CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers", NeurIPS, 2022 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2204.14217)][[PyTorch](https://github.com/THUDM/CogView2)]
    * **CLIPDraw**: "CLIPDraw: Exploring Text-to-Drawing Synthesis through Language-Image Encoders", NeurIPS, 2022 (*Cross Compass, Japan*). [[Paper](https://arxiv.org/abs/2106.14843)][[PyTorch](https://colab.research.google.com/github/kvfrans/clipdraw/blob/main/clipdraw.ipynb)][[Blog](https://kvfrans.com/clipdraw-exploring-text-to-drawing-synthesis/)]
    * **Imagen**: "Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding", NeurIPS, 2022 (*Google*). [[Paper](https://gweb-research-imagen.appspot.com/paper.pdf)][[Website](https://gweb-research-imagen.appspot.com/)]
    * **?**: "Human Evaluation of Text-to-Image Models on a Multi-Task Benchmark", NeurIPSW, 2022 (*Boston + MIT + Columbia*). [[Paper](https://arxiv.org/abs/2211.12112)]
    * **DALL-Eval**: "DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generative Transformers", arXiv, 2022 (*UNC*). [[Paper](https://arxiv.org/abs/2202.04053)][[PyTorch](https://github.com/j-min/DallEval)]
    * **DALL-E-2**: "Hierarchical Text-Conditional Image Generation with CLIP Latents", arXiv, 2022 (*OpenAI*). [[Paper](https://arxiv.org/abs/2204.06125)][[Website](https://openai.com/dall-e-2/)]
    * **?**: "A very preliminary analysis of DALL-E 2", arXiv, 2022 (*NYU*). [[Paper](https://arxiv.org/abs/2204.13807)]
    * **GLIDE**: "GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models", arXiv, 2022 (*OpenAI*). [[Paper](https://arxiv.org/abs/2112.10741)][[PyTorch](https://github.com/openai/glide-text2im)]
    * **?**: "Discovering the Hidden Vocabulary of DALLE-2", arXiv, 2022 (*UT Austin*). [[Paper](https://arxiv.org/abs/2206.00169)]
    * **Parti**: "Scaling Autoregressive Models for Content-Rich Text-to-Image Generation", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2206.10789)][[GitHub](https://github.com/google-research/parti)][[Website](https://parti.research.google/)]
    * **Textual-Inversion**: "An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion", arXiv, 2022 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2208.01618)][[Website](https://textual-inversion.github.io/)]
    * **VLMGAN**: "Vision-Language Matching for Text-to-Image Synthesis via Generative Adversarial Networks", arXiv, 2022 (*Fudan University*). [[Paper](https://arxiv.org/abs/2208.09596)]
    * **PDM**: "Progressive Denoising Model for Fine-Grained Text-to-Image Generation", arXiv, 2022 (*Meituan*). [[Paper](https://arxiv.org/abs/2210.02291)]
    * **FS-VQG**: "Few-Shot Visual Question Generation: A Novel Task and Benchmark Datasets", arXiv, 2022 (*IIT Kharagpur*). [[Paper](https://arxiv.org/abs/2210.07076)]
    * **Swinv2-Imagen**: "Swinv2-Imagen: Hierarchical Vision Transformer Diffusion Models for Text-to-Image Generation", arXiv, 2022 (*Auckland University of Technology*). [[Paper](https://arxiv.org/abs/2210.09549)]
    * **UniTune**: "UniTune: Text-Driven Image Editing by Fine Tuning an Image Generation Model on a Single Image", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2210.09477)]
    * **VSD**: "Visual Spatial Description: Controlled Spatial-Oriented Image-to-Text Generation", arXiv, 2022 (*Tianjin University*). [[Paper](https://arxiv.org/abs/2210.11109)][[Code (in construction)](https://github.com/zhaoyucs/VSD)]
    * **Lafite2**: "Lafite2: Few-shot Text-to-Image Generation", arXiv, 2022 (*SUNY, Buffalo*). [[Paper](https://arxiv.org/abs/2210.14124)]
    * **eDiffi**: "eDiffi: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers", arXiv, 2022 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2211.01324)][[Website](https://research.nvidia.com/labs/dir/eDiff-I/)]
    * **SpaText**: "SpaText: Spatio-Textual Representation for Controllable Image Generation", arXiv, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2211.14305)][[Website](https://omriavrahami.com/spatext/)]
    * **Story-LDM**: "Make-A-Story: Visual Memory Conditioned Consistent Story Generation", arXiv, 2022 (*UBC + Snap*). [[Paper](https://arxiv.org/abs/2211.13319)]
    * **Structure-Diffusion**: "Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis", arXiv, 2022 (*UCSB + UC Santa Cruz*). [[Paper](https://arxiv.org/abs/2212.05032)][[PyTorch](https://github.com/weixi-feng/Structured-Diffusion-Guidance)][[Website](https://weixi-feng.github.io/structure-diffusion-guidance/)]
    * **Re-Imagen**: "Re-Imagen: Retrieval-Augmented Text-to-Image Generator", ICLR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2209.14491)]
    * **Prompt-to-Prompt**: "Prompt-to-Prompt Image Editing with Cross Attention Control", ICLR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2208.01626)][[PyTorch](https://github.com/google/prompt-to-prompt/)][[Website](https://prompt-to-prompt.github.io/)]
    * **UniD3**: "Unified Discrete Diffusion for Simultaneous Vision-Language Generation", ICLR, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2211.14842)]
    * **T2P**: "Zero-Shot Text-to-Parameter Translation for Game Character Auto-Creation", CVPR, 2023 (*Fuxi AI Lab*). [[Paper](https://arxiv.org/abs/2303.01311)]
    * **GLIGEN**: "GLIGEN: Open-Set Grounded Text-to-Image Generation", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2301.07093)][[PyTorch](https://github.com/gligen/GLIGEN)][[Website](https://gligen.github.io/)]
    * **MAGVLT**: "MAGVLT: Masked Generative Vision-and-Language Transformer", CVPR, 2023 (*Kakao*). [[Paper](https://arxiv.org/abs/2303.12208)]
    * **ReCo**: "ReCo: Region-Controlled Text-to-Image Generation", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2211.15518)][[PyTorch](https://github.com/microsoft/ReCo)]
    * **GALIP**: "GALIP: Generative Adversarial CLIPs for Text-to-Image Synthesis", CVPR, 2023 (*Nanjing University of Posts and Telecommunications*). [[Paper](https://arxiv.org/abs/2301.12959)][[PyTorch](https://github.com/tobran/GALIP)]
    * **DreamBooth**: "DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2208.12242)][[GitHub](https://github.com/google/dreambooth)][[Website](https://dreambooth.github.io/)]
    * **RIATIG**: "RIATIG: Reliable and Imperceptible Adversarial Text-to-Image Generation With Natural Prompts", CVPR, 2023 (*Washington University in St. Louis*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Liu_RIATIG_Reliable_and_Imperceptible_Adversarial_Text-to-Image_Generation_With_Natural_Prompts_CVPR_2023_paper.html)]
    * **ERNIE-ViLG-2.0**: "ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts", CVPR, 2023 (*Baidu*). [[Paper](https://arxiv.org/abs/2210.15257)][[Website](https://wenxin.baidu.com/ernie-vilg)]
    * **GigaGAN**: "Scaling up GANs for Text-to-Image Synthesis", CVPR, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2303.05511)][[PyTorch](https://github.com/mingukkang/GigaGAN/tree/main/evaluation)][[Website](https://mingukkang.github.io/GigaGAN/)]
    * **Shifted-Diffusion**: "Shifted Diffusion for Text-to-image Generation", CVPR, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2211.15388)][[PyTorch](https://github.com/drboog/Shifted_Diffusion)]
    * **Specialist-Diffusion**: "Specialist Diffusion: Plug-and-Play Sample-Efficient Fine-Tuning of Text-to-Image Diffusion Models To Learn Any Unseen Style", CVPR, 2023 (*Picsart*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Lu_Specialist_Diffusion_Plug-and-Play_Sample-Efficient_Fine-Tuning_of_Text-to-Image_Diffusion_Models_To_CVPR_2023_paper.html)][[Website](https://specialist-diffusion.github.io/)]
    * **?**: "Toward Verifiable and Reproducible Human Evaluation for Text-to-Image Generation", CVPR, 2023 (*CyberAgent, Japan*). [[Paper](https://arxiv.org/abs/2304.01816)]
    * **Custom-Diffusion**: "Multi-Concept Customization of Text-to-Image Diffusion", CVPR, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2212.04488)]
    * **UniDiffuser**: "One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale", ICML, 2023 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2303.06555)][[Pytorch](https://github.com/thu-ml/unidiffuser)]
    * **Muse**: "Muse: Text-To-Image Generation via Masked Generative Transformers", ICML, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2301.00704)][[Website](https://muse-model.github.io/)]
    * **RA-CM3**: "Retrieval-Augmented Multimodal Language Modeling", ICML, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2211.12561)]
    * **StyleGAN-T**: "StyleGAN-T: Unlocking the Power of GANs for Fast Large-Scale Text-to-Image Synthesis", ICML, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2301.09515)][[PyTorch](https://github.com/autonomousvision/stylegan-t)][[Website](https://sites.google.com/view/stylegan-t/)]
    * **VD**: "Versatile Diffusion: Text, Images and Variations All in One Diffusion Model", ICCV, 2023 (*Oregon*). [[Paper](https://arxiv.org/abs/2211.08332)][[PyTorch](https://github.com/SHI-Labs/Versatile-Diffusion)]
    * **DiT**: "Scalable Diffusion Models with Transformers", ICCV, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2212.09748)][[PyTorch](https://github.com/facebookresearch/DiT)][[Website](https://www.wpeebles.com/DiT)]
    * **TCTS-FAS**: "Text-Conditioned Sampling Framework for Text-to-Image Generation with Masked Generative Models", ICCV, 2023 (*KAIST*). [[Paper](https://arxiv.org/abs/2304.01515)]
    * **?**: "Discriminative Class Tokens for Text-to-Image Diffusion Models", ICCV, 2023 (*Tel Aviv*). [[Paper](https://arxiv.org/abs/2303.17155)][[PyTorch](https://github.com/idansc/discriminative_class_tokens)]
    * **TIFA**: "TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering", ICCV, 2023 (*UW*). [[Paper](https://arxiv.org/abs/2303.11897)][[PyTorch](https://github.com/Yushi-Hu/tifa)][[Website](https://tifa-benchmark.github.io/)]
    * **LSDM**: "Language-driven Scene Synthesis using Multi-conditional Diffusion Model", NeurIPS, 2023 (*FSOFT AI Center, Vietnam*). [[Paper](https://arxiv.org/abs/2310.15948)][[PyTorch](https://github.com/andvg3/LSDM)][[Website](https://lang-scene-synth.github.io/)]
    * **LLMScore**: "LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation", NeurIPS, 2023 (*UCSB*). [[Paper](https://arxiv.org/abs/2305.11116)][[PyTorch](https://github.com/YujieLu10/LLMScore)]
    * **PoS-subspaces**: "Parts of Speech-Grounded Subspaces in Vision-Language Models", NeurIPS, 2023 (*Queen Mary University of London*). [[Paper](https://arxiv.org/abs/2305.14053)][[PyTorch](https://github.com/james-oldfield/PoS-subspaces)][[Website](http://eecs.qmul.ac.uk/~jo001/PoS-subspaces/)]
    * **LANCE**: "LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images", NeurIPS, 2023 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2305.19164)][[PyTorch](https://github.com/virajprabhu/LANCE)][[Website](https://virajprabhu.github.io/lance-web/)]
    * **?**: "The CLIP Model is Secretly an Image-to-Prompt Converter", NeurIPS, 2023 (*Xidian University*). [[Paper](https://arxiv.org/abs/2305.12716)]
    * **BLIP-Diffusion**: "BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing", NeurIPS, 2023 (*Salesforce*). [[Paper](https://arxiv.org/abs/2305.14720)][[Code (in construction)](https://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion)][[Website](https://dxli94.github.io/BLIP-Diffusion-website/)]
    * **CoDi**: "Any-to-Any Generation via Composable Diffusion", NeurIPS, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2305.11846)][[PyTorch](https://github.com/microsoft/i-Code/tree/main/i-Code-V3)][[Website](https://codi-gen.github.io/)]
    * **UniControl**: "UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild", NeurIPS, 2023 (*Salesforce*). [[Paper](https://arxiv.org/abs/2305.11147)][[PyTorch](https://github.com/salesforce/UniControl)]
    * **E4T**: "Designing an Encoder for Fast Personalization of Text-to-Image Models", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2302.12228)][[Website](https://tuning-encoder.github.io/)]
    * **?**: "Controlled and Conditional Text to Image Generation with Diffusion Prior", arXiv, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2302.11710)]
    * **Lformer**: "Lformer: Text-to-Image Generation with L-shape Block Parallel Decoding", arXiv, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2303.03800)]
    * **UMM-Diffusion**: "Unified Multi-Modal Latent Diffusion for Joint Subject and Text Conditional Image Generation", arXiv, 2023 (*Peking University*). [[Paper](https://arxiv.org/abs/2303.09319)]
    * **ToMESD**: "Token Merging for Fast Stable Diffusion", arXiv, 2023 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2303.17604)][[PyTorch](https://github.com/dbolya/tomesd)]
    * **layout-guidance**: "Training-Free Layout Control with Cross-Attention Guidance", arXiv, 2023 (*Oxford*). [[Paper](https://arxiv.org/abs/2304.03373)][[PyTorch](https://github.com/silent-chen/layout-guidance)][[Website](https://silent-chen.github.io/layout-guidance/)]
    * **HRS-Bench**: "HRS-Bench: Holistic, Reliable and Scalable Benchmark for Text-to-Image Models", arXiv, 2023 (*KAUST*). [[Paper](https://arxiv.org/abs/2304.05390)][[GitHub](https://github.com/eslambakr/HRS_benchmark)][[Website](https://eslambakr.github.io/hrsbench.github.io/)]
    * **SeedSelect**: "It is all about where you start: Text-to-image generation with seed selection", arXiv, 2023 (*Bar-Ilan University, Israel*). [[Paper](https://arxiv.org/abs/2304.14530)]
    * **DisenBooth**: "DisenBooth: Disentangled Parameter-Efficient Tuning for Subject-Driven Text-to-Image Generation", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2305.03374)]
    * **VideoOFA**: "VideoOFA: Two-Stage Pre-Training for Video-to-Text Generation", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/pdf/2305.03204.pdf)]
    * **FastComposer**: "FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention", arXiv, 2023 (*MIT*). [[Paper](https://arxiv.org/abs/2305.10431)][[PyTorch](https://github.com/mit-han-lab/fastcomposer)][[Website](https://fastcomposer.mit.edu/)]
    * **VPGen**: "Visual Programming for Text-to-Image Generation and Evaluation", arXiv, 2023 (*UNC*). [[Paper](https://arxiv.org/abs/2305.15328)][[PyTorch](https://github.com/j-min/VPGen)][[Website](https://vp-t2i.github.io/)]
    * **SeeCoder**: "Prompt-Free Diffusion: Taking "Text" out of Text-to-Image Diffusion Models", arXiv, 2023 (*Picsart*). [[Paper](https://arxiv.org/abs/2305.16223)][[PyTorch](https://github.com/SHI-Labs/Prompt-Free-Diffusion)]
    * **GILL**: "Generating Images with Multimodal Language Models", NeurIPS, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2305.17216)][[PyTorch](https://github.com/kohjingyu/gill)][[Website](https://jykoh.com/gill)]
    * **DA-Score**: "Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback", NeurIPS, 2023 (*ANU*). [[Paper](https://arxiv.org/abs/2307.04749)][[PyTorch](https://github.com/1jsingh/Divide-Evaluate-and-Refine)][[Website](https://1jsingh.github.io/divide-evaluate-and-refine)]
    * **GORS**: "T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation", NeurIPS, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2307.06350)][[PyTorch](https://github.com/Karine-Huang/T2I-CompBench)][[Website](https://karine-h.github.io/T2I-CompBench/)]
    * **TextDiffuser**: "TextDiffuser: Diffusion Models as Text Painters", NeurIPS, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2305.10855)][[PyTorch](https://github.com/microsoft/unilm/tree/master/textdiffuser)]
    * **CAC**: "Localized Text-to-Image Generation for Free via Cross Attention Control", arXiv, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2306.14636)]
    * **CLIPAG**: "CLIPAG: Towards Generator-Free Text-to-Image Generation", arXiv, 2023 (*Technion, Israel*). [[Paper](https://arxiv.org/abs/2306.16805)]
    * **PACGen**: "Generate Anything Anywhere in Any Scene", arXiv, 2023 (*UW Madison*). [[Paper](https://arxiv.org/abs/2306.17154)][[Code (in construction)](https://github.com/Yuheng-Li/PACGen)][[Website](https://yuheng-li.github.io/PACGen/)]
    * **SPAE**: "SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2306.17842)]
    * **HyperDreamBooth**: "HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2307.06949)][[Website](https://hyperdreambooth.github.io/)]
    * **?**: "Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2307.06925)][[Website](https://datencoder.github.io/)]
    * **IP-Adapter**: "IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2308.06721)][[Website](https://ip-adapter.github.io/)]
    * **ORES**: "ORES: Open-vocabulary Responsible Visual Synthesis", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2308.13785)]
    * **CM3Leon**: "Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2309.02591)]
    * **DreamLLM**: "DreamLLM: Synergistic Multimodal Comprehension and Creation", arXiv, 2023 (*Megvii*). [[Paper](https://arxiv.org/abs/2309.11499)][[Code (in construction)](https://github.com/RunpeiDong/DreamLLM)][[Website](https://dreamllm.github.io/)]
    * **FreeU**: "FreeU: Free Lunch in Diffusion U-Net", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2309.11497)][[Website](https://chenyangsi.top/FreeU/)][[Code (in construction)](https://github.com/ChenyangSi/FreeU)]
    * **Emu**: "Emu: Enhancing Image Generation Models Using Photogenic Needles in a Haystack", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2309.15807)]
    * **Kosmos-G**: "Kosmos-G: Generating Images in Context with Multimodal Large Language Models", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2310.02992)][[PyTorch](https://github.com/microsoft/unilm/tree/master/kosmos-g)][[Website](https://xichenpan.com/kosmosg/)]
    * **AlignProp**: "Aligning Text-to-Image Diffusion Models with Reward Backpropagation", arXiv, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2310.03739)][[PyTorch](https://github.com/mihirp1998/AlignProp/)][[Website](https://align-prop.github.io/)]
    * **Idea2Img**: "Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2310.08541)][[Website](https://idea2img.github.io/)]
    * **EasyGen**: "Making Multimodal Generation Easier: When Diffusion Models Meet LLMs", arXiv, 2023 (*The Hong Kong Polytechnic University*). [[Paper](https://arxiv.org/abs/2310.08949)][[PyTorch](https://github.com/zxy556677/EasyGen)]
    * **LLM-Blueprint**: "LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts", arXiv, 2023 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2310.10640)][[Code (in construction)](https://github.com/hananshafi/llmblueprint)]
    * **DiagrammerGPT**: "DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning", arXiv, 2023 (*UNC*). [[Paper](https://arxiv.org/abs/2310.12128)][[PyThon](https://github.com/aszala/DiagrammerGPT)][[Website](https://diagrammergpt.github.io/)]
    * **Emu-Edit**: "Emu Edit: Precise Image Editing via Recognition and Generation Tasks", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2311.10089)]
    * **CoDi-2**: "CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2311.18775)][[Code (in construction)](https://github.com/microsoft/i-Code/tree/main/CoDi-2)][[Website](https://codi-2.github.io/)]
    * **UniGS**: "UniGS: Unified Representation for Image Generation and Segmentation", arXiv, 2023 (*UC Merced*). [[Paper](https://arxiv.org/abs/2312.01985)][[PyTorch (in construction)](https://github.com/qqlu/Entity)]
    * **StoryGPT-V**: "Large Language Models as Consistent Story Visualizers", arXiv, 2023 (*KAUST*). [[Paper](https://arxiv.org/abs/2312.02252)][[PyTorch](https://anonymous.4open.science/r/story/README.md)][[Website](https://storygpt-v.s3.amazonaws.com/index.html)]
    * **StackedDiffusion**: "Generating Illustrated Instructions", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2312.04552)][[Website](https://facebookresearch.github.io/IllustratedInstructions/)]
    * **VL-GPT**: "VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2312.09251)][[Code (in construction)](https://github.com/AILab-CVC/VL-GPT)]
    * **PixArt-**: "PixArt-: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis", ICLR, 2024 (*Huawei*). [[Paper](https://arxiv.org/abs/2310.00426)][[PyTorch](https://github.com/PixArt-alpha/PixArt-alpha)][[Website](https://pixart-alpha.github.io/)]
    * **DistriFusion**: "DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models", CVPR, 2024 (*MIT*). [[Paper](https://arxiv.org/abs/2402.19481)][[PyTorch](https://github.com/mit-han-lab/distrifuser)][[Website](https://hanlab.mit.edu/projects/distrifusion)]
    * **DPT**: "Discriminative Probing and Tuning for Text-to-Image Generation", CVPR, 2024 (*NUS*). [[Paper](https://arxiv.org/abs/2403.04321)][[Code (in construction)](https://github.com/LgQu/DPT-T2I)][[Website](https://dpt-t2i.github.io/)]
    * **HcP**: "Towards Effective Usage of Human-Centric Priors in Diffusion Models for Text-based Human Image Generation", CVPR, 2024 (*University of New South Wales (UNSW), Australia*). [[Paper](https://arxiv.org/abs/2403.05239)][[Code (in construction)](https://github.com/hcplayercvpr2024/hcplayer)][[Website](https://hcplayercvpr2024.github.io/)]
    * **aMUSEd**: "aMUSEd: An Open MUSE Reproduction", arXiv, 2024 (*Hugging Face*). [[Paper](https://arxiv.org/abs/2401.01808)]
    * **Instruct-Imagen**: "Instruct-Imagen: Image Generation with Multi-modal Instruction", arXiv, 2024 (*Google*). [[Paper](https://arxiv.org/abs/2401.01952)]
    * **DiffusionGPT**: "DiffusionGPT: LLM-Driven Text-to-Image Generation System", arXiv, 2024 (*ByteDance*). [[Paper](https://arxiv.org/abs/2401.10061)][[PyTorch](https://github.com/DiffusionGPT/DiffusionGPT)][[Website](https://diffusiongpt.github.io/)]
    * **SiT**: "SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers", arXiv, 2024 (*NYU*). [[Paper](https://arxiv.org/abs/2401.08740)][[PyTorch](https://github.com/willisma/SiT)][[Website](https://scalable-interpolant.github.io/)]
    * **-ECLIPSE**: "-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space", arXiv, 2024 (*Arizona State University*). [[Paper](https://arxiv.org/abs/2402.05195)][[PyTorch](https://github.com/eclipse-t2i/lambda-eclipse-inference)][[Website](https://eclipse-t2i.github.io/Lambda-ECLIPSE/)]
    * **FiT**: "FiT: Flexible Vision Transformer for Diffusion Model", arXiv, 2024 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2402.12376)][[PyTorch (in construction)](https://github.com/whlzy/FiT)]
    * **SDXL-Lightning**: "SDXL-Lightning: Progressive Adversarial Diffusion Distillation", arXiv, 2024 (*ByteDance*). [[Paper](https://arxiv.org/abs/2402.13929)]
    * **CG**: "Contrastive Prompts Improve Disentanglement in Text-to-Image Diffusion Models", arXiv, 2024 (*CMU*). [[Paper](https://arxiv.org/abs/2402.13490)]
    * **Gen4Gen**: "Gen4Gen: Generative Data Pipeline for Generative Multi-Concept Composition", arXiv, 2024 (*Berkeley*). [[Paper](https://arxiv.org/abs/2402.15504)][[Code (in construction)](https://github.com/louisYen/Gen4Gen)][[Website](https://danielchyeh.github.io/Gen4Gen/)]
    * **PixArt-**: "PixArt-: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation", arXiv, 2024 (*Huawei*). [[Paper](https://arxiv.org/abs/2403.04692)][[Website](https://pixart-alpha.github.io/PixArt-sigma-project/)]
    * **CogView3**: "CogView3: Finer and Faster Text-to-Image Generation via Relay Diffusion", arXiv, 2024 (*Zhipu AI, China*). [[Paper](https://arxiv.org/abs/2403.05121)]
    * **SELMA**: "SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data", arXiv, 2024 (*UNC*). [[Paper](https://arxiv.org/abs/2403.06952)][[PyTorch](https://github.com/jialuli-luka/SELMA)][[Website](https://selma-t2i.github.io/)]
* Video:
    * **Imagen-Video**: "Imagen Video: High Definition Video Generation with Diffusion Models", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2210.02303)][[Website](https://imagen.research.google/video/)]
    * **Phenaki**: "Phenaki: Variable Length Video Generation From Open Domain Textual Description", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2210.02399)][[PyTorch (LAION-AI, in construction)](https://github.com/LAION-AI/phenaki)][[Website](https://phenaki.video/)]
    * **?**: "Towards Real-Time Text2Video via CLIP-Guided, Pixel-Level Optimization", arXiv, 2022 (*CMU*). [[Paper](https://arxiv.org/abs/2210.12826)][[PyTorch](https://github.com/pschaldenbrand/Text2Video)][[Website](https://pschaldenbrand.github.io/text2video/)]
    * **MagicVideo**: "MagicVideo: Efficient Video Generation With Latent Diffusion Models", arXiv, 2022 (*ByteDance*). [[Paper](https://arxiv.org/abs/2211.11018)][[Website](https://magicvideo.github.io/)]
    * **CogVideo**: "CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers", ICLR, 2023 (*Tsinghua University*) [[Paper](https://arxiv.org/abs/2205.15868)][[GitHub (in construction)](https://github.com/THUDM/CogVideo)]
    * **Make-A-Video**: "Make-A-Video: Text-to-Video Generation without Text-Video Data", ICLR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2209.14792)]
    * **VideoLDM**: "Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models", CVPR, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2304.08818)][[Website](https://research.nvidia.com/labs/toronto-ai/VideoLDM/)]
    * **MMVG**: "Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation", CVPR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2211.12824)]
    * **MM-Diffusion**: "MM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2212.09478)][[PyTorch](https://github.com/researchmm/MM-Diffusion)]
    * **PYoCo**: "Preserve Your Own Correlation: A Noise Prior for Video Diffusion Models", ICCV, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2305.10474)][[Website](https://research.nvidia.com/labs/dir/pyoco/)]
    * **Text2Video-Zero**: "Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators", ICCV, 2023 (*Picsart*). [[Paper](https://arxiv.org/abs/2303.13439)][[Code (in construction)](https://github.com/Picsart-AI-Research/Text2Video-Zero)]
    * **Text2Performer**: "Text2Performer: Text-Driven Human Video Generation", ICCV, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2304.08483)][[PyTorch](https://github.com/yumingj/Text2Performer)][[Website](https://yumingj.github.io/projects/Text2Performer.html)]
    * **GlueGen**: "GlueGen: Plug and Play Multi-modal Encoders for X-to-image Generation", ICCV, 2023 (*Salesforce*). [[Paper](https://arxiv.org/abs/2303.10056)][[PyTorch](https://github.com/salesforce/GlueGen)]
    * **VideoFactory**: "VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2305.10874)]
    * **Video-Adapter**: "Probabilistic Adaptation of Text-to-Video Models", arXiv, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2306.01872)][[Website](https://video-adapter.github.io/video-adapter/)]
    * **SimDA**: "SimDA: Simple Diffusion Adapter for Efficient Video Generation", arXiv, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2308.09710)][[Website](https://chenhsing.github.io/SimDA/)]
    * **LVD**: "LLM-grounded Video Diffusion Models", arXiv, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2309.17444)][[Code (in construction)](https://github.com/TonyLianLong/LLM-groundedVideoDiffusion)][[Website](https://llm-grounded-video-diffusion.github.io/)]
    * **VideoCrafter1**: "VideoCrafter1: Open Diffusion Models for High-Quality Video Generation", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2310.19512)][[PyTorch](https://github.com/AILab-CVC/VideoCrafter)][[Website](https://ailab-cvc.github.io/videocrafter/)]
    * **Emu-Video**: "Emu Video: Factorizing Text-to-Video Generation by Explicit Image Conditioning", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2311.10709)][[Website](https://emu-video.metademolab.com/)]
    * **PixelDance**: "Make Pixels Dance: High-Dynamic Video Generation", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2311.10982)][[Website](https://makepixelsdance.github.io/)]
    * **VideoBooth**: "VideoBooth: Diffusion-based Video Generation with Image Prompts", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2312.00777)][[Code (in construction)](https://github.com/Vchitect/VideoBooth)][[Website](https://vchitect.github.io/VideoBooth-project/)]
    * **VideoSwap**: "VideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2312.02087)][[Code (in construction)](https://github.com/showlab/VideoSwap)][[Website](https://videoswap.github.io/)]
    * **LEGO**: "LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2312.03849)]
    * **GenHowTo**: "GenHowTo: Learning to Generate Actions and State Transformations from Instructional Videos", arXiv, 2023 (*Czech Technical University*). [[Paper](https://arxiv.org/abs/2312.07322)][[Website](https://soczech.github.io/genhowto/)]
    * **VideoLCM**: "VideoLCM: Video Latent Consistency Model", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2312.09109)]
    * **VideoPoet**: "VideoPoet: A Large Language Model for Zero-Shot Video Generation", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2312.14125)][[Website](https://sites.research.google/videopoet/)]
    * **MagicVideo-V2**: "MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation", arXiv, 2024 (*ByteDance*). [[Paper](https://arxiv.org/abs/2401.04468)][[Website](https://magicvideov2.github.io/)]
    * **WorldDreamer**: "WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens", arXiv, 2024 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2401.09985)][[Code (in construction)](https://github.com/JeffWang987/WorldDreamer)][[Website](https://world-dreamer.github.io/)]
    * **Vlogger**: "Vlogger: Make Your Dream A Vlog", arXiv, 2024 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2401.09414)][[PyTorch](https://github.com/zhuangshaobin/Vlogger)]
    * **VideoCrafter2**: "VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models", arXiv, 2024 (*Tencent*). [[Paper](https://arxiv.org/abs/2401.09047)][[PyTorch](https://github.com/AILab-CVC/VideoCrafter)][[Website](https://ailab-cvc.github.io/videocrafter2/)]
    * **ActAnywhere**: "ActAnywhere: Subject-Aware Video Background Generation", arXiv, 2024 (*Adobe*). [[Paper](https://arxiv.org/abs/2401.10822)][[Website](https://actanywhere.github.io/)]
    * **Lumiere**: "Lumiere: A Space-Time Diffusion Model for Video Generation", arXiv, 2024 (*Google*). [[Paper](https://arxiv.org/abs/2401.12945)][[Website](https://lumiere-video.github.io/)]
    * **Snap-Video**: "Snap Video: Scaled Spatiotemporal Transformers for Text-to-Video Synthesis", arXiv, 2024 (*Snap*). [[Paper](https://arxiv.org/abs/2402.14797)][[Website](https://snap-research.github.io/snapvideo/)]
    * **Pix2Gif**: "Pix2Gif: Motion-Guided Diffusion for GIF Generation", arXiv, 2024 (*Microsoft*). [[Paper](https://arxiv.org/abs/2403.04634)][[PyTorch](https://github.com/hiteshK03/Pix2Gif)][[Website](https://hiteshk03.github.io/Pix2Gif/)]
    * **WorldGPT**: "WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text and Image Inputs", arXiv, 2024 (*Seeking AI, China*). [[Paper](https://arxiv.org/abs/2403.07944)]
* 3D:
    * **Magic3D**: "Magic3D: High-Resolution Text-to-3D Content Creation", CVPR, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2211.10440)][[Website](https://research.nvidia.com/labs/dir/magic3d/)]
    * **CLIP-Sculptor**: "CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Natural Language", CVPR, 2023 (*Autodesk*). [[Paper](https://arxiv.org/abs/2211.01427)][[Website](https://ivl.cs.brown.edu/#/projects/clip-sculptor)]
    * **Diffusion-SDF**: "Diffusion-SDF: Text-to-Shape via Voxelized Diffusion", CVPR, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2212.03293)][[PyTorch](https://github.com/ttlmh/Diffusion-SDF)][[Website](https://ttlmh.github.io/DiffusionSDF/)]
    * **TAPS3D**: "TAPS3D: Text-Guided 3D Textured Shape Generation from Pseudo Supervision", CVPR, 2023 (*Bytedance*). [[Paper](https://arxiv.org/abs/2303.13273)][[PyTorch](https://github.com/plusmultiply/TAPS3D)]
    * **Dream3D**: "Dream3D: Zero-Shot Text-to-3D Synthesis Using 3D Shape Prior and Text-to-Image Diffusion Models", CVPR, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2212.14704)][[Website](https://bluestyle97.github.io/dream3d/)]
    * **ATT3D**: "ATT3D: Amortized Text-to-3D Object Synthesis", ICCV, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2306.07349)][[Website](https://research.nvidia.com/labs/toronto-ai/ATT3D/)]
    * **InstructP2P**: "InstructP2P: Learning to Edit 3D Point Clouds with Text Instructions", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2306.07154)]
    * **SDS-Complete**: "Point-Cloud Completion with Pretrained Text-to-image Diffusion Models", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2306.10533)][[Website](https://sds-complete.github.io/)]
    * **Michelangelo**: "Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2306.17115)][[Code (in construction)(https://github.com/NeuralCarver/michelangelo)]][[Website](https://neuralcarver.github.io/michelangelo/)]
    * **DiffTF**: "Large-Vocabulary 3D Diffusion Model with Transformer", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2309.07920)][[Code (in construction)](https://github.com/ziangcao0312/DiffTF)][[Website](https://ziangcao0312.github.io/difftf_pages/)]
    * **3D-GPT**: "3D-GPT: Procedural 3D Modeling with Large Language Models", arXiv, 2023 (*ANU*). [[Paper](https://arxiv.org/abs/2310.12945)][[Website](https://chuny1.github.io/3DGPT/3dgpt.html)]
* Others:
    * **DiffGesture**: "Taming Diffusion Models for Audio-Driven Co-Speech Gesture Generation", CVPR, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2303.09119)][[PyTorch](https://github.com/Advocate99/DiffGesture)]
    * **CondFoleyGen**: "Conditional Generation of Audio from Video via Foley Analogies", CVPR, 2023 (*UMich*). [[Paper](https://arxiv.org/abs/2304.08490)][[PyTorch (in construction)](https://github.com/XYPB/CondFoleyGen)][[Website](https://xypb.github.io/CondFoleyGen/)]
    * **Physics-Diffusion**: "Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos", CVPR, 2023 (*IBM*). [[Paper](https://arxiv.org/abs/2303.16897)][[PyTorch](https://github.com/sukun1045/video-physics-sound-diffusion)][[Website](https://sukun1045.github.io/video-physics-sound-diffusion/)]
    * **RACER**: "Co-Speech Gesture Synthesis by Reinforcement Learning With Contrastive Pre-Trained Rewards", CVPR, 2023 (*Dalian University of Technology*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Sun_Co-Speech_Gesture_Synthesis_by_Reinforcement_Learning_With_Contrastive_Pre-Trained_Rewards_CVPR_2023_paper.html)][[Code (in construction)](https://github.com/RLracer/RACER)]
    * **ReVISE**: "ReVISE: Self-Supervised Speech Resynthesis with Visual Input for Universal and Generalized Speech Regeneration", CVPR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2212.11377)][[PyTorch](https://github.com/facebookresearch/av_hubert)][[Website](https://wnhsu.github.io/ReVISE/)]
    * **MAV3D**: "Text-To-4D Dynamic Scene Generation", ICML, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2301.11280)][[Website](https://make-a-video3d.github.io/)]
    * **LORIS**: "Long-Term Rhythmic Video Soundtracker", ICML, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2305.01319)][[PyTorch](https://github.com/OpenGVLab/LORIS)]
    * **NExT-GPT**: "NExT-GPT: Any-to-Any Multimodal LLM", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2309.05519)][[Code (in construction)](https://github.com/NExT-GPT/NExT-GPT)][[Website](https://next-gpt.github.io/)]

[[Back to Overview](#overview)]

### Prompt Learning/Tuning:
* **CLIP-Adapter**: "CLIP-Adapter: Better Vision-Language Models with Feature Adapters", arXiv, 2021 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2110.04544)][[PyTorch](https://github.com/gaopengcuhk/CLIP-Adapter)]
* **CoCoOp**: "Conditional Prompt Learning for Vision-Language Models", CVPR, 2022 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2203.05557)][[PyTorch](https://github.com/KaiyangZhou/CoOp)]
* **ProDA**: "Prompt Distribution Learning", CVPR, 2022 (*Huawei*). [[Paper](https://arxiv.org/abs/2205.03340)]
* **VPT**: "Visual Prompt Tuning", ECCV, 2022 (*Cornell*). [[Paper](https://arxiv.org/abs/2203.12119)][[PyTorch](https://github.com/kmnp/vpt)]
* **PerVL**: "This is my unicorn, Fluffy": Personalizing frozen vision-language representations", ECCV, 2022 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2204.01694)][[PyTorch](https://github.com/NVlabs/PALAVRA)]
* **OrdinalCLIP**: "OrdinalCLIP: Learning Rank Prompts for Language-Guided Ordinal Regression", NeurIPS, 2022 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2206.02338)][[PyTorch](https://github.com/xk-huang/OrdinalCLIP)]
* **BeamCLIP**: "Transferring Pre-trained Multimodal Representations with Cross-modal Similarity Matching", NeurIPS, 2022 (*LG*). [[Paper](https://openreview.net/forum?id=j2Vtg_jhKZ)]
* **TPT**: "Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models", NeurIPS, 2022 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2209.07511)][[PyTorch](https://github.com/azshue/TPT)][[Website](https://azshue.github.io/TPT/)]
* **CoOp**: "Learning to Prompt for Vision-Language Models", IJCV, 2022 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2109.01134)][[PyTorch](https://github.com/KaiyangZhou/CoOp)]
* **CAVPT**: "Class-Aware Visual Prompt Tuning for Vision-Language Pre-Trained Model", arXiv, 2022 (*Northwestern Polytechnical University, China*). [[Paper](https://arxiv.org/abs/2208.08340)]
* **Visual-Prompting**: "Exploring Visual Prompts for Adapting Large-Scale Models", arXiv, 2022 (*MIT*). [[Paper](https://arxiv.org/abs/2203.17274)][[PyTorch](https://github.com/hjbahng/visual_prompting)][[Website](https://hjbahng.github.io/visual_prompting/)]
* **PGN**: "Prompt Generation Networks for Efficient Adaptation of Frozen Vision Transformers", arXiv, 2022 (*University of Amsterdam*). [[Paper](https://arxiv.org/abs/2210.06466)][[PyTorch](https://github.com/jochemloedeman/PGN)]
* **UPT**: "Unified Vision and Language Prompt Learning", arXiv, 2022 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2210.07225)][[Code (in construction)](https://github.com/yuhangzang/UPT)]
* **CPL**: "CPL: Counterfactual Prompt Learning for Vision and Language Models", arXiv, 2022 (*UC Santa Cruz*). [[Paper](https://arxiv.org/abs/2210.10362)]
* **PTP**: "Prompting through Prototype: A Prototype-based Prompt Learning on Pretrained Vision-Language Models", arXiv, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2210.10841)]
* **MVLPT**: "Multitask Vision-Language Prompt Tuning", arXiv, 2022 (*Berkeley*). [[Paper](https://arxiv.org/abs/2211.11720)][[PyTorch](https://github.com/sIncerass/MVLPT)]
* **?**: "Task Bias in Vision-Language Models", arXiv, 2022 (*Columbia*). [[Paper](https://arxiv.org/abs/2212.04412)]
* **UPL**: "Unsupervised Prompt Learning for Vision-Language Models", arXiv, 2022 (*Peking*). [[Paper](https://arxiv.org/abs/2204.03649)][[PyTorch](https://github.com/tonyhuang2022/UPL)]
* **DeFo**: "Learning to Decompose Visual Features with Latent Textual Prompts", ICLR, 2023 (*UIUC*). [[Paper](https://arxiv.org/abs/2210.04287)]
* **PLOT**: "Prompt Learning with Optimal Transport for Vision-Language Models", ICLR, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2210.01253)]
* **?**: "Visual Classification via Description from Large Language Models", ICLR, 2023 (*Columbia*). [[Paper](https://arxiv.org/abs/2210.07183)]
* **CSP**: "Learning to Compose Soft Prompts for Compositional Zero-Shot Learning", ICLR, 2023 (*Brown University*). [[Paper](https://arxiv.org/abs/2204.03574)][[PyTorch](https://github.com/BatsResearch/csp)]
* **CaFo**: "Prompt, Generate, then Cache: Cascade of Foundation Models makes Strong Few-shot Learners", CVPR, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2303.02151)][[PyTorch](https://github.com/ZrrSkywalker/CaFo)]
* **?**: "Multimodal Prompting with Missing Modalities for Visual Recognition", CVPR, 2023 (*NYCU*). [[Paper](https://arxiv.org/abs/2303.03369)][[PyTorch](https://github.com/YiLunLee/Missing_aware_prompts)][[Website](https://yilunlee.github.io/missing_aware_prompts/)]
* **DAM-VP**: "Diversity-Aware Meta Visual Prompting", CVPR, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2303.08138)][[PyTorch](https://github.com/shikiw/DAM-VP)]
* **ILM-VP**: "Understanding and Improving Visual Prompting: A Label-Mapping Perspective", CVPR, 2023 (*Michigan State*). [[Paper](https://arxiv.org/abs/2211.11635)][[PyTorch](https://github.com/OPTML-Group/ILM-VP)]
* **KgCoOp**: "Visual-Language Prompt Tuning with Knowledge-guided Context Optimization", CVPR, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2303.13283)][[PyTorch](https://github.com/htyao89/KgCoOp)]
* **BlackVIP**: "BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning", CVPR, 2023 (*University of Seoul*). [[Paper](https://arxiv.org/abs/2303.14773)][[PyTorch](https://github.com/changdaeoh/BlackVIP)]
* **EXPRES**: "Learning Expressive Prompting With Residuals for Vision Transformers", CVPR, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2303.15591)]
* **?**: "Learning to Name Classes for Vision and Language Models", CVPR, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2304.01830)]
* **PMF**: "Efficient Multimodal Fusion via Interactive Prompting", CVPR, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2304.06306)]
* **MaPLe**: "MaPLe: Multi-modal Prompt Learning", CVPR, 2023 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2210.03117)][[PyTorch](https://github.com/muzairkhattak/multimodal-prompt-learning)]
* **HiPro**: "Hierarchical Prompt Learning for Multi-Task Learning", CVPR, 2023 (*JD*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Liu_Hierarchical_Prompt_Learning_for_Multi-Task_Learning_CVPR_2023_paper.html)]
* **DFSP**: "Decomposed Soft Prompt Guided Fusion Enhancing for Compositional Zero-Shot Learning", CVPR, 2023 (*The Hong Kong Polytechnic University*). [[Paper](https://arxiv.org/abs/2211.10681)][[PyTorch](https://github.com/Forest-art/DFSP)]
* **TaI-DP**: "Texts as Images in Prompt Tuning for Multi-Label Image Recognition", CVPR, 2023 (*Tomorrow Advancing Life (TAL)*). [[Paper](https://arxiv.org/abs/2211.12739)][[PyTorch](https://github.com/guozix/TaI-DPT)]
* **ESPER**: "Fusing Pre-Trained Language Models With Multimodal Prompts Through Reinforcement Learning", CVPR, 2023 (*Yonsei*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Fusing_Pre-Trained_Language_Models_With_Multimodal_Prompts_Through_Reinforcement_Learning_CVPR_2023_paper.html)][[PyTorch](https://github.com/JiwanChung/esper)]
* **APT**: "A-La-Carte Prompt Tuning (APT): Combining Distinct Data via Composable Prompting", CVPR, 2023 (*Amazon*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Bowman_A-La-Carte_Prompt_Tuning_APT_Combining_Distinct_Data_via_Composable_Prompting_CVPR_2023_paper.html)]
* **VQT**: "Visual Query Tuning: Towards Effective Usage of Intermediate Representations for Parameter and Memory Efficient Transfer Learning", CVPR, 2023 (*The Ohio State University (OSU)*). [[Paper](https://arxiv.org/abs/2212.03220)]
* **LaBo**: "Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification", CVPR, 2023 (*University of Pennsylvania*). [[Paper](https://arxiv.org/abs/2211.11158)][[PyTorch](https://github.com/YueYANG1996/LaBo)]
* **TaskRes**: "Task Residual for Tuning Vision-Language Models", CVPR, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2211.10277)][[PyTorch](https://github.com/geekyutao/TaskRes)]
* **LASP**: "Language-Aware Soft Prompting for Vision & Language Foundation Models", CVPR, 2023 (*Samsung*). [[Paper](https://arxiv.org/abs/2210.01115)][[Website](https://www.adrianbulat.com/lasp)]
* **POUF**: "POUF: Prompt-oriented unsupervised fine-tuning for large pre-trained models", ICML, 2023 (*UT Austin*). [[Paper](https://arxiv.org/abs/2305.00350)][[PyTorch](https://github.com/korawat-tanwisuth/POUF)]
* **GaPT**: "Improving Visual Prompt Tuning for Self-supervised Vision Transformers", ICML, 2023 (*SNU*). [[Paper](https://arxiv.org/abs/2306.05067)][[PyTorch](https://github.com/ryongithub/GatedPromptTuning)]
* **ZPE**: "A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models", ICML, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2302.06235)]
* **CMPA**: "Deeply Coupled Cross-Modal Prompt Learning", ACL Findings, 2023 (*SenseTime*). [[Paper](https://arxiv.org/abs/2305.17903)]
* **PromptSRC**: "Self-regulating Prompts: Foundational Model Adaptation without Forgetting", ICCV, 2023 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2307.06948)][[PyTorch](https://github.com/muzairkhattak/PromptSRC)][[Website](https://muzairkhattak.github.io/PromptSRC/)]
* **SHIP**: "Improving Zero-Shot Generalization for CLIP with Synthesized Prompts", ICCV, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2307.07397)]
* **PTNL**: "Why Is Prompt Tuning for Vision-Language Models Robust to Noisy Labels?", ICCV, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2307.11978)][[PyTorch](https://github.com/CEWu/PTNL)]
* **E<sup>2</sup>VPT**: "E<sup>2</sup>VPT: An Effective and Efficient Approach for Visual Prompt Tuning", ICCV, 2023 (*Rochester Institute of Technology, NY*). [[Paper](https://arxiv.org/abs/2307.13770)][[PyTorch](https://github.com/ChengHan111/E2VPT)]
* **R-AMT**: "Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models", ICCV, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2307.15049)][[PyTorch](https://github.com/wuw2019/RMT)][[Website](https://wuw2019.github.io/RMT/)]
* **DiffTPT**: "Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning", ICCV, 2023 (*A\*STAR*). [[Paper](https://arxiv.org/abs/2308.06038)][[PyTorch](https://github.com/chunmeifeng/DiffTPT)]
* **KAPT**: "Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models", ICCV, 2023 (*Southern University of Science and Technology (SUSTech)*). [[Paper](https://arxiv.org/abs/2308.11186)]
* **RPO**: "Read-only Prompt Optimization for Vision-Language Few-shot Learning", ICCV, 2023 (*Korea University*). [[Paper](https://arxiv.org/abs/2308.14960)][[PyTorch](https://github.com/mlvlab/RPO)]
* **LoGoPrompt**: "LoGoPrompt: Synthetic Text Images Can Be Good Visual Prompts for Vision-Language Models", ICCV, 2023 (*ShanghaiTech*). [[Paper](https://arxiv.org/abs/2309.01155)][[Website](https://chengshiest.github.io/logo/)]
* **DAPT**: "Distribution-Aware Prompt Tuning for Vision-Language Models", ICCV, 2023 (*Korea University*). [[Paper](https://arxiv.org/abs/2309.03406)][[PyTorch](https://github.com/mlvlab/DAPT)]
* **?**: "What does CLIP know about a red circle? Visual prompt engineering for VLMs", ICCV, 2023 (*Oxford*). [[Paper](https://arxiv.org/abs/2304.06712)]
* **GRAM**: "Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models", ICCV, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2303.06571)]
* **VPT**: "Variational prompt tuning improves generalization of vision-language models", ICCV, 2023 (*Samsung*). [[Paper](https://arxiv.org/abs/2210.02390)][[PyTorch](https://github.com/saic-fi/Bayesian-Prompt-Learning)]
* **ProGrad**: "Prompt-aligned Gradient for Prompt Tuning", ICCV, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2205.14865)][[PyTorch](https://github.com/BeierZhu/Prompt-align)]
* **CTP-TFT**: "Task-Oriented Multi-Modal Mutual Leaning for Vision-Language Models", ICCV, 2023 (*Baidu*). [[Paper](https://arxiv.org/abs/2303.17169)]
* **GOPro**: "GOPro: Generate and Optimize Prompts in CLIP using Self-Supervised Learning", BMVC, 2023 (*IIT Bombay*). [[Paper](https://arxiv.org/abs/2308.11605)][[Code (in construction)](https://github.com/mainaksingha01/GOPro)]
* **APoLLo**: "APoLLo: Unified Adapter and Prompt Learning for Vision Language Models", EMNLP, 2023 (*Maryland*). [[Paper](https://arxiv.org/abs/2312.01564)][[Website](https://gamma.umd.edu/pro/vision_language/apollo/)]
* **ALIGN**: "Tuning Multi-mode Token-level Prompt Alignment across Modalities", NeurIPS, 2023 (*Xidian University*). [[Paper](https://arxiv.org/abs/2309.13847)][[PyTorch](https://github.com/wds2014/ALIGN)]
* **GraphAdapter**: "GraphAdapter: Tuning Vision-Language Models With Dual Knowledge Graph", NeurIPS, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2309.13625)][[PyTorch (in construction)](https://github.com/lixinustc/GraphAdapter)]
* **OpenVik**: "Open Visual Knowledge Extraction via Relation-Oriented Multimodality Model Prompting", NeurIPS, 2023 (*Emory*). [[Paper](https://arxiv.org/abs/2310.18804)]
* **PromptAlign**: "Align Your Prompts: Test-Time Prompting with Distribution Alignment for Zero-Shot Generalization", NeurIPS, 2023 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2311.01459)][[PyTorch](https://github.com/jameelhassan/PromptAlign)][[Website](https://jameelhassan.github.io/promptalign/)]
* **VPGTrans**: "Transfer Visual Prompt Generator across LLMs", NeurIPS, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2305.01278)][[PyTorch](https://github.com/VPGTrans/VPGTrans)][[Website](https://vpgtrans.github.io/)]
* **TransHP**: "TransHP: Image Classification with Hierarchical Prompting", NeurIPS, 2023 (*Baidu*). [[Paper](https://arxiv.org/abs/2304.06385)][[PyTorch](https://github.com/WangWenhao0716/TransHP)]
* **UP-DP**: "UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models", NeurIPS, 2023 (*Bosch*). [[Paper](https://arxiv.org/abs/2307.11227)]
* **LaFTer**: "LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unlabeled Image Collections", NeurIPS, 2023 (*TU Graz, Austria*). [[Paper](https://arxiv.org/abs/2305.18287)][[PyTorch](https://github.com/jmiemirza/LaFTer)][[Website](https://jmiemirza.github.io/LaFTer/)]
* **DDCoT**: "DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models", NeurIPS, 2023 (*ShanghaiTech*). [[Paper](https://arxiv.org/abs/2310.16436)][[PyTorch](https://github.com/SooLab/DDCOT)][[Website](https://toneyaya.github.io/ddcot/)]
* **?**: "Enhancing CLIP with CLIP: Exploring Pseudolabeling for Limited-Label Prompt Tuning", NeurIPS, 2023 (*Brown*). [[Paper](https://arxiv.org/abs/2306.01669)][[PyTorch](https://github.com/BatsResearch/menghini-enhanceCLIPwithCLIP-code)]
* **FGVP**: "Fine-Grained Visual Prompting", NeurIPS, 2023 (*BAAI*). [[Paper](https://arxiv.org/abs/2306.04356)][[PyTorch](https://github.com/ylingfeng/FGVP)]
* **POMP**: "Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition", NeurIPS, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2304.04704)][[PyTorch](https://github.com/amazon-science/prompt-pretraining)]
* **SeMap**: "From Visual Prompt Learning to Zero-Shot Transfer: Mapping Is All You Need", arXiv, 2023 (*CISPA, Germany*). [[Paper](https://arxiv.org/abs/2303.05266)]
* **R-Tuning**: "R-Tuning: Regularized Prompt Tuning in Open-Set Scenarios", arXiv, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2303.05122)]
* **VPTM**: "Rethinking Visual Prompt Learning as Masked Visual Token Modeling", arXiv, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2303.04998)]
* **PBPrompt**: "Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models", arXiv, 2023 (*Xidian University*). [[Paper](https://arxiv.org/abs/2303.09100)]
* **Robust-ProL**: "Towards Robust Prompts on Vision-Language Models", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2304.08479)]
* **ProVP**: "Progressive Visual Prompt Learning with Contrastive Feature Re-formation", arXiv, 2023 (*vivo, China*). [[Paper](https://arxiv.org/abs/2304.08386)]
* **?**: "Chain of Thought Prompt Tuning in Vision Language Models", arXiv, 2023 (*Peking University*). [[Paper](https://arxiv.org/abs/2304.07919)]
* **Instruction-ViT**: "Instruction-ViT: Multi-Modal Prompts for Instruction Learning in ViT", arXiv, 2023 (*University of Electronic Science and Technology of China*). [[Paper](https://arxiv.org/abs/2305.00201)]
* **DRPT**: "DRPT: Disentangled and Recurrent Prompt Tuning for Compositional Zero-Shot Learning", arXiv, 2023 (*Hong Kong Polytechnic University*). [[Paper](https://arxiv.org/abs/2305.01239)][[Code (in construction)](https://github.com/Forest-art/DRPT-torch)]
* **VCoT**: "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings", arXiv, 2023 (*UCSB*). [[Paper](https://arxiv.org/abs/2305.02317)]
* **PMPO**: "Multi-Prompt with Depth Partitioned Cross-Modal Learning", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2305.06221)]
* **DSD**: "Discriminative Diffusion Models as Few-shot Vision and Language Learners", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2305.10722)]
* **PLID**: "Prompting Language-Informed Distribution for Compositional Zero-Shot Learning", arXiv, 2023 (*Michigan State*). [[Paper](https://arxiv.org/abs/2305.14428)]
* **ConES**: "ConES: Concept Embedding Search for Parameter Efficient Tuning Large Vision Language Models", arXiv, 2023 (*Sichuan University*). [[Paper](https://arxiv.org/abs/2305.18993)]
* **CoPrompt**: "Consistency-guided Prompt Learning for Vision-Language Models", arXiv, 2023 (*Queens University, Canada*). [[Paper](https://arxiv.org/abs/2306.01195)]
* **ProTeCt**: "ProTeCt: Prompt Tuning for Hierarchical Consistency", arXiv, 2023 (*UCSD*). [[Paper](https://arxiv.org/abs/2306.02240)]
* **POP**: "POP: Prompt Of Prompts for Continual Learning", arXiv, 2023 (*Qualcomm*). [[Paper](https://arxiv.org/abs/2306.08200)]
* **GAVIE**: "Aligning Large Multi-Modal Model with Robust Instruction Tuning", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2306.14565)][[PyTorch](https://github.com/FuxiaoLiu/LRV-Instruction)][[Website](https://fuxiaoliu.github.io/LRV/)]
* **NPT**: "Bridging the Gap: Neural Collapse Inspired Prompt Tuning for Generalization under Class Imbalance", arXiv, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2306.15955)]
* **APT**: "Approximated Prompt Tuning for Vision-Language Pre-trained Models", arXiv, 2023 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2306.15706)]
* **CoPL**: "Contextual Prompt Learning for Vision-Language Understanding", arXiv, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2307.00910)]
* **CiP**: "Image Captions are Natural Prompts for Text-to-Image Models", arXiv, 2023 (*The University of Sydney*). [[Paper](https://arxiv.org/abs/2307.08526)]
* **DPL**: "DPL: Decoupled Prompt Learning for Vision-Language Models", arXiv, 2023 (*vivo*). [[Paper](https://arxiv.org/abs/2308.10061)]
* **DuAl-PT**: "Context-Aware Prompt Tuning for Vision-Language Model with Dual-Alignment", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2309.04158)]
* **DePT**: "DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning", arXiv, 2023 (*UCL*). [[Paper](https://arxiv.org/abs/2309.05173)][[PyTorch](https://github.com/ZhengxiangShi/DePT)]
* **Prompting4Debugging**: "Prompting4Debugging: Red-Teaming Text-to-Image Diffusion Models by Finding Problematic Prompts", arXiv, 2023 (*NYCU*). [[Paper](https://arxiv.org/abs/2309.06135)]
* **?**: "Language Models as Black-Box Optimizers for Vision-Language Models", arXiv, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2309.05950)]
* **DePT**: "DePT: Decoupled Prompt Tuning", arXiv, 2023 (*University of Electronic Science and Technology of China*). [[Paper](https://arxiv.org/abs/2309.07439)][[PyTorch](https://github.com/Koorye/DePT)]
* **DEsignBench**: "DEsignBench: Exploring and Benchmarking DALL-E 3 for Imagining Visual Design", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2310.15144)][[Website](https://design-bench.github.io/)]
* **ArGue**: "ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models", arXiv, 2023 (*ANU*). [[Paper](https://arxiv.org/abs/2311.16494)]
* **SWIG**: "Beyond Sole Strength: Customized Ensembles for Generalized Vision-Language Models", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2311.17091)][[Code (in construction)](https://github.com/zhiheLu/Ensemble_VLM)]
* **IMProv**: "IMProv: Inpainting-based Multimodal Prompting for Computer Vision Tasks", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2312.01771)][[PyTorch](https://github.com/xvjiarui/IMProv)][[Website](https://jerryxu.net/IMProv/)]
* **CLAMP**: "CLAMP: Contrastive LAnguage Model Prompt-tuning", arXiv, 2023 (*Boston*). [[Paper](https://arxiv.org/abs/2312.01629)]
* **RLP**: "Re-parameterized Low-rank Prompt: Generalize a Vision-Language Model within 0.5K Parameters", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2312.10813)]
* **HPT**: "Learning Hierarchical Prompt with Structured Linguistic Knowledge for Vision-Language Models", AAAI, 2024 (*Tongji University*). [[Paper](https://arxiv.org/abs/2312.06323)]
* **LAMM**: "LAMM: Label Alignment for Multi-Modal Prompt Learning", AAAI, 2024 (*SJTU*). [[Paper](https://arxiv.org/abs/2312.08212)][[Code (in construction)](https://github.com/gaojingsheng/LAMM)]
* **LaViP**: "LaViP: Language-Grounded Visual Prompts", AAAI, 2024 (*Monash University*). [[Paper](https://arxiv.org/abs/2312.10945)]
* **SA<sup>2</sup>VP**: "SA<sup>2</sup>VP: Spatially Aligned-and-Adapted Visual Prompt", AAAI, 2024 (*Harbin Institute of Technology*). [[Paper](https://arxiv.org/abs/2312.10376)][[PyTorch](https://github.com/tommy-xq/SA2VP)]
* **CPL**: "Concept-Guided Prompt Learning for Generalization in Vision-Language Models", AAAI, 2024 (*Harbin Institute of Technology*). [[Paper](https://arxiv.org/abs/2401.07457)]
* **?**: "Facing the Elephant in the Room: Visual Prompt Tuning or Full Finetuning?", ICLR, 2024 (*Rochester Institute of Technology*). [[Paper](https://arxiv.org/abs/2401.12902)]
* **PromptKD**: "PromptKD: Unsupervised Prompt Distillation for Vision-Language Models", CVPR, 2024 (*Nankai University*). [[Paper](https://arxiv.org/abs/2403.02781)][[PyTorch](https://github.com/zhengli97/PromptKD)][[Website](https://zhengli97.github.io/PromptKD/)]
* **ProText**: "Learning to Prompt with Text Only Supervision for Vision-Language Models", arXiv, 2024 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2401.02418)][[PyTorch](https://github.com/muzairkhattak/ProText)][[Website](https://muzairkhattak.github.io/ProText/)]
* **Any-shift**: "Any-Shift Prompting for Generalization over Distributions", arXiv, 2024 (*UvA*). [[Paper](https://arxiv.org/abs/2402.10099)]
* **SPT**: "Revisiting the Power of Prompt for Visual Tuning", arXiv, 2024 (*Hefei University of Technology*). [[Paper](https://arxiv.org/abs/2402.02382)][[PyTorch](https://github.com/WangYZ1608/Self-Prompt-Tuning)]
* **LSPT**: "LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning", arXiv, 2024 (*Microsoft*). [[Paper](https://arxiv.org/abs/2402.17406)]

[[Back to Overview](#overview)]

### Visual Document Understanding
* **LayoutLMv2**: "LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding", ACL, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2012.14740)][[PyTorch](https://github.com/microsoft/unilm/tree/master/layoutlmv2)]
* **DocFormer**: "DocFormer: End-to-End Transformer for Document Understanding", ICCV, 2021 (*Amazon*). [[Paper](https://arxiv.org/abs/2106.11539)]
* **StrucTexT**: "StrucTexT: Structured Text Understanding with Multi-Modal Transformers", ACMMM, 2021 (*Baidu*). [[Paper](https://arxiv.org/abs/2108.02923)][[Paddle](https://github.com/PaddlePaddle/VIMER/tree/main/StrucTexT/v1)]
* **LayoutXLM**: "LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding", arXiv, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2104.08836)][[PyTorch](https://github.com/microsoft/unilm/tree/master/layoutxlm)]
* **TableFormer**: "TableFormer: Table Structure Understanding with Transformers", CVPR, 2022 (*IBM*). [[Paper](https://arxiv.org/abs/2203.01017)]
* **TSRFormer**: "TSRFormer: Table Structure Recognition with Transformers", ACMMM, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2208.04921)]
* **ERNIE-mmLayout**: "ERNIE-mmLayout: Multi-grained MultiModal Transformer for Document Understanding", ACMMM, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2209.08569)]
* **Donut**: "Donut: Document Understanding Transformer without OCR", ECCV, 2022 (*NAVER*). [[Paper](https://arxiv.org/abs/2111.15664)][[PyTorch](https://github.com/clovaai/donut)]
* **I2DFormer**: "I2DFormer: Learning Image to Document Attention for Zero-Shot Image Classification", NeurIPS, 2022 (*ETHZ*). [[Paper](https://arxiv.org/abs/2209.10304)]
* **MGDoc**: "MGDoc: Pre-training with Multi-granular Hierarchy for Document Image Understanding", EMNLP, 2022 (*Adobe*). [[Paper](https://arxiv.org/abs/2211.14958)]
* **DocEnTr**: "DocEnTr: An End-to-End Document Image Enhancement Transformer", arXiv, 2022 (*UAB, Spain*). [[Paper](https://arxiv.org/abs/2201.10252)][[PyTorch](https://github.com/dali92002/DocEnTR)]
* **DocSegTr**: "DocSegTr: An Instance-Level End-to-End Document Image Segmentation Transformer", arXiv, 2022 (*UAB, Spain*). [[Paper](https://arxiv.org/abs/2201.11438)]
* **DiT**: "DiT: Self-supervised Pre-training for Document Image Transformer", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2203.02378)][[Code (in construction)](https://github.com/microsoft/unilm/tree/master/dit)]
* **LayoutLMv3**: "LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2204.08387)][[PyTorch](https://github.com/microsoft/unilm/tree/master/layoutlmv3)]
* **MATrIX**: "MATrIX - Modality-Aware Transformer for Information eXtraction", arXiv, 2022 (*Amazon*). [[Paper](https://arxiv.org/abs/2205.08094)]
* **VLCDoC**: "VLCDoC: Vision-Language Contrastive Pre-Training Model for Cross-Modal Document Classification", arXiv, 2022 (*La Rochelle University, France*). [[Paper](https://arxiv.org/abs/2205.12029)]
* **Bi-VLDoc**: "Bi-VLDoc: Bidirectional Vision-Language Modeling for Visually-Rich Document Understanding", arXiv, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2206.13155)]
* **TRUST**: "TRUST: An Accurate and End-to-End Table structure Recognizer Using Splitting-based Transformers", arXiv, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2208.14687)]
* **Hi-VT5**: "Hierarchical multimodal transformers for Multi-Page DocVQA", arXiv, 2022 (*UAB, Spain*). [[Paper](https://arxiv.org/abs/2212.05935)]
* **OCR-VQGAN**: "OCR-VQGAN: Taming Text-within-Image Generation", WACV, 2023 (*UAB, Spain*). [[Paper](https://arxiv.org/abs/2210.11248)]
* **PIXEL**: "Language Modelling with Pixels", ICLR, 2023 (*University of Copenhagen, Denmark*). [[Paper](https://openreview.net/forum?id=FkSp8VW8RjH)]
* **Spotlight**: "Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus", ICLR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2209.14927)]
* **MaskDoc**: "Masked Visual-Textual Prediction for Document Image Representation Pretraining", ICLR, 2023 (*Baidu*). [[Paper](https://openreview.net/forum?id=HE_75XY5Ljh)]
* **StrucTexTv2**: "StrucTexTv2: Masked Visual-Textual Prediction for Document Image Pre-training", ICLR, 2023 (*Baidu*). [[Paper](https://arxiv.org/abs/2303.00289)][[Paddle](https://github.com/PaddlePaddle/VIMER/tree/main/StrucTexT/v2)]
* **FlexDM**: "Towards Flexible Multi-modal Document Models", CVPR, 2023 (*CyberAgent, Japan*). [[Paper](https://arxiv.org/abs/2303.18248)][[Tensorflow](https://github.com/CyberAgentAILab/flex-dm)][[Website](https://cyberagentailab.github.io/flex-dm/)]
* **MUI**: "Mobile User Interface Element Detection Via Adaptively Prompt Tuning", CVPR, 2023 (*Ant Group*). [[Paper](https://arxiv.org/abs/2305.09699)][[GitHub (in construction)](https://github.com/antmachineintelligence/MUI-zh)]
* **UDOP**: "Unifying Vision, Text, and Layout for Universal Document Processing", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2212.02623)][[PyTorch](https://github.com/microsoft/i-Code/tree/main/i-Code-Doc)]
* **M<sup>6</sup>Doc**: "M<sup>6</sup>Doc: A Large-Scale Multi-Format, Multi-Type, Multi-Layout, Multi-Language, Multi-Annotation Category Dataset for Modern Document Layout Analysis", CVPR, 2023 (*South China University of Technology*). [[Paper](https://arxiv.org/abs/2305.08719)][[GitHub](https://github.com/HCIILAB/M6Doc)]
* **VGT**: "Vision Grid Transformer for Document Layout Analysis", ICCV, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2308.14978)][[PyTorch](https://github.com/AlibabaResearch/AdvancedLiterateMachinery)]
* **SeRum**: "Attention Where It Matters: Rethinking Visual Document Understanding with Selective Region Concentration", ICCV, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2309.01131)]
* **DocTr**: "DocTr: Document Transformer for Structured Information Extraction in Documents", ICCV, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2307.07929)]
* **FormNetV2**: "FormNetV2: Multimodal Graph Contrastive Learning for Form Document Information Extraction", ACL, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2305.02549)]
* **mmc4**: "Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text", NeurIPS (Datasets and Benchmarks), 2023 (*AI2*). [[Paper](https://arxiv.org/abs/2304.06939)][[GitHub](https://github.com/allenai/mmc4)]
* **DUBLIN**: "DUBLIN - Document Understanding By Language-Image Network", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2305.14218)]
* **DocFormerv2**: "DocFormerv2: Local Features for Document Understanding", arXiv, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2306.01733)]
* **DocumentCLIP**: "DocumentCLIP: Linking Figures and Main Body Text in Reflowed Documents", arXiv, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2306.06306)][[PyTorch](https://github.com/FuxiaoLiu/DocumentCLIP)]
* **Kosmos-2.5**: "Kosmos-2.5: A Multimodal Literate Model", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2309.11419)]
* **UReader**: "UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2310.05126)]
* **RD**: "Efficient End-to-End Visual Document Understanding with Rationale Distillation", arXiv, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2311.09612)]
* **ScreenAI**: "ScreenAI: A Vision-Language Model for UI and Infographics Understanding", arXiv, 2024 (*Google*). [[Paper](https://arxiv.org/abs/2402.04615)]
* **DoCo**: "Enhancing Visual Document Understanding with Contrastive Learning in Large Visual-Language Models", arXiv, 2024 (*Tencent*). [[Paper](https://arxiv.org/abs/2402.19014)]

[[Back to Overview](#overview)]

### Other Multi-Modal Tasks
* Transfer Learning/Adaptation/Distillation/PEFT/MoE:
    * **FLYP**: "Finetune like you pretrain: Improved finetuning of zero-shot vision models", CVPR, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2212.00638)][[PyTorch](https://github.com/locuslab/FLYP)]
    * **Pi-Tuning**: "Pi-Tuning: Transferring Multimodal Foundation Models with Optimal Multi-task Interpolation", ICML, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2304.14381)][[Code (in construction)](https://github.com/TencentARC/pi-Tuning)]
    * **OCRA**: "Cross-Modal Fine-Tuning: Align then Refine", ICML, 2023 (*CMU + HP*). [[Paper](https://arxiv.org/abs/2302.05738)][[PyTorch](https://github.com/sjunhongshen/ORCA)]
    * **ProbVLM**: "ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models", ICCV, 2023 (*University of Tubingen, Germany*). [[Paper](https://arxiv.org/abs/2307.00398)][[PyTorch](https://github.com/ExplainableML/ProbVLM)]
    * **TeS**: "Improved Visual Fine-tuning with Natural Language Supervision", ICCV, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2304.01489)][[PyTorch](https://github.com/idstcv/TeS)]
    * **Aurora**: "Parameter-efficient Tuning of Large-scale Multimodal Foundation Model", NeurIPS, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2305.08381)][[PyTorch](https://github.com/WillDreamer/Aurora)]
    * **DAS**: "Parameter and Computation Efficient Transfer Learning for Vision-Language Pre-trained Models", NeurIPS, 2023 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2309.01479)][[PyTorch](https://github.com/DoubtedSteam/DAS)]
    * **Paxion**: "Paxion: Patching Action Knowledge in Video-Language Foundation Models", NeurIPS, 2023 (*UIUC*). [[Paper](https://arxiv.org/abs/2305.10683)][[PyTorch](https://github.com/MikeWangWZHL/Paxion)]
    * **m<sup>2</sup>-Mix**: "Geodesic Multi-Modal Mixup for Robust Fine-Tuning", NeurIPS, 2023 (*University of Seoul*). [[Paper](https://arxiv.org/abs/2203.03897)][[PyTorch (in construction)](https://github.com/changdaeoh/multimodal-mixup)]
    * **RLCF**: "Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models", arXiv, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2305.18010)][[Code (in construction)](https://github.com/mzhaoshuai/RLCF)]
    * **LMAT**: "Can Large Pre-trained Models Help Vision Models on Perception Tasks?", arXiv, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2306.00693)][[Website (in construction)](https://dingning97.github.io/imagenet-descriptions/)]
    * **TaCA**: "TaCA: Upgrading Your Visual Foundation Model with Task-agnostic Compatible Adapter", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2306.12642)][[Code (in construction)](https://github.com/TencentARC/TaCA)]
    * **CLIP-KD**: "CLIP-KD: An Empirical Study of Distilling CLIP Models", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2307.12732)][[Code (in construction)](https://github.com/winycg/CLIP-KD)]
    * **AdaLink**: "Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2310.12100)]
    * **LM4Visual**: "Frozen Transformers in Language Models Are Effective Visual Encoder Layers", arXiv, 2023 (*UIUC*). [[Paper](https://arxiv.org/abs/2310.12973)][[PyTorch (in construction)](https://github.com/ziqipang/LM4VisualEncoding)]
    * **Octavius**: "Octavius: Mitigating Task Interference in MLLMs via MoE", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2311.02684)]
    * **GDA**: "A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation", ICLR, 2024 (*CAS*). [[Paper](https://arxiv.org/abs/2402.04087)][[PyTorch](https://github.com/mrflogs/ICLR24)]
    * **LLaVA-MoLE**: "LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs", arXiv, 2024 (*Meituan*). [[Paper](https://arxiv.org/abs/2401.16160)]
    * **?**: "Routers in Vision Mixture of Experts: An Empirical Study", arXiv, 2024 (*DeepMind*). [[Paper](https://arxiv.org/abs/2401.15969)]
    * **MoE-LLaVA**: "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models", arXiv, 2024 (*Peking*). [[Paper](https://arxiv.org/abs/2401.15947)][[PyTorch](https://github.com/PKU-YuanGroup/MoE-LLaVA)]
    * **DoRA**: "DoRA: Weight-Decomposed Low-Rank Adaptation", arXiv, 2024 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2402.09353)]
    * **DeLVM**: "Data-efficient Large Vision Models through Sequential Autoregression", arXiv, 2024 (*University of Sydney*). [[Paper](https://arxiv.org/abs/2402.04841)][[PyTorch (in construction)](https://github.com/ggjy/DeLVM)]
    * **POVID**: "Aligning Modalities in Vision Large Language Models via Preference Fine-tuning", arXiv, 2024 (*UNC*). [[Paper](https://arxiv.org/abs/2402.11411)][[PyTorch](https://github.com/YiyangZhou/POVID)]
    * **MoAI**: "MoAI: Mixture of All Intelligence for Large Language and Vision Models", arXiv, 2024 (*KAIST*). [[Paper](https://arxiv.org/abs/2403.07508)][[PyTorch](https://github.com/ByungKwanLee/MoAI)]
* Zero-Shot:
    * **SMs**: "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language", ICLR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2204.00598)][[GitHub](https://github.com/google-research/google-research/tree/master/socraticmodels)][[Website](https://socraticmodels.github.io/)]
    * **iCLIP**: "iCLIP: Bridging Image Classification and Contrastive Language-Image Pre-Training for Visual Recognition", CVPR, 2023 (*Microsoft*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Wei_iCLIP_Bridging_Image_Classification_and_Contrastive_Language-Image_Pre-Training_for_Visual_CVPR_2023_paper.html)]
    * **DiffDis**: "DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability", ICCV, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2308.09306)]
    * **CuPL**: "What does a platypus look like? Generating customized prompts for zero-shot image classification", ICCV, 2023 (*UW*). [[Paper](https://arxiv.org/abs/2209.03320)][[PyTorch](https://github.com/sarahpratt/CuPL)]
    * **InMaP**: "Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP", NeurIPS, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2310.19752)][[PyTorch (in construction)](https://github.com/idstcv/InMaP)]
    * **DN**: "Test-Time Distribution Normalization for Contrastively Learned Vision-language Models", NeurIPS, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2302.11084)][[PyTorch](https://github.com/fengyuli-dev/distribution-normalization)][[Website](https://fengyuli-dev.github.io/dn-website/)]
    * **?**: "ChatGPT-Powered Hierarchical Comparisons for Image Classification", NeurIPS, 2023 (*Michigan State*). [[Paper](https://arxiv.org/abs/2311.00206)][[PyTorch](https://github.com/Zhiyuan-R/ChatGPT-Powered-Hierarchical-Comparisons-for-Image-Classification)]
    * **V-GLOSS**: "Visually-Grounded Descriptions Improve Zero-Shot Image Classification", arXiv, 2023 (*University of Alberta, Canada*). [[Paper](https://arxiv.org/abs/2306.06077)]
    * **?**: "Challenges of Zero-Shot Recognition with Vision-Language Models: Granularity and Correctness", arXiv, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2306.16048)]
    * **UniFine**: "UniFine: A Unified and Fine-grained Approach for Zero-shot Vision-Language Understanding", arXiv, 2023 (*Columbia*). [[Paper](https://arxiv.org/abs/2307.00862)][[Code (in construction)](https://github.com/ThreeSR/UniFine)]
    * **Cheetah**: "Empowering Vision-Language Models to Follow Interleaved Vision-Language Instructions", arXiv, 2023 (*Zhejiang*). [[Paper](https://arxiv.org/abs/2308.04152)]
    * **?**: "LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions", arXiv, 2023 (*Beihang*). [[Paper](https://arxiv.org/abs/2311.11904)]
    * **REAL**: "The Neglected Tails of Vision-Language Models", arXiv, 2024 (*TAMU*). [[Paper](https://arxiv.org/abs/2401.12425)][[Code (in construction)](https://github.com/shubhamprshr27/NeglectedTailsVLM)][[Website](https://shubhamprshr27.github.io/neglected-tails-of-vlms/)]
* X-Shot:
    * **Tip-Adapter**: "Tip-Adapter: Training-free Adaption of CLIP for Few-shot Classification", ECCV, 2022 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2207.09519)][[PyTorch](https://github.com/gaopengcuhk/Tip-Adapter)]
    * **VidIL**: "Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners", NeurIPS, 2022 (*UIUC*). [[Paper](https://arxiv.org/abs/2205.10747)][[PyTorch](https://github.com/MikeWangWZHL/VidIL)]
    * **ComCLIP**: "ComCLIP: Training-Free Compositional Image and Text Matching", arXiv, 2022 (*UC Santa Cruz*). [[Paper](https://arxiv.org/abs/2211.13854)]
    * **TCT**: "Efficient Zero-shot Visual Search via Target and Context-aware Transformer", arXiv, 2022 (*Baylor College of Medicine, TX*). [[Paper](https://arxiv.org/abs/2211.13470)]
    * **?**: "Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning", ICLR, 2023 (*University of Amsterdam*). [[Paper](https://arxiv.org/abs/2302.14794)]
    * **?**: "Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models", CVPR, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2301.06267)]
    * **SADA**: "Few-Shot Learning with Visual Distribution Calibration and Cross-Modal Distribution Alignment", CVPR, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2305.11439)][[PyTorch](https://github.com/bhrqw/SADA)]
    * **LFA**: "Black Box Few-Shot Adaptation for Vision-Language models", ICCV, 2023 (*Samsung*). [[Paper](https://arxiv.org/abs/2304.01752)]
    * **Meta-Adapter**: "Meta-Adapter: An Online Few-shot Learner for Vision-Language Model", NeurIPS, 2023 (*Xi'an JiaoTong*). [[Paper](https://arxiv.org/abs/2311.03774)]
    * **?**: "Making the Most of What You Have: Adapting Pre-trained Visual Language Models in the Low-data Regime", arXiv, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2305.02297)]
    * **Proto-CLIP**: "Proto-CLIP: Vision-Language Prototypical Network for Few-Shot Learning", arXiv, 2023 (*UT Dallas*). [[Paper](https://arxiv.org/abs/2307.03073)]
    * **NtUA**: "Noise-Tolerant Unsupervised Adapter for Vision-Language Models", arXiv, 2023 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2309.14928)]
    * **SeCAt**: "Small Visual Language Models can also be Open-Ended Few-Shot Learners", arXiv, 2023 (*UvA*). [[Paper](https://arxiv.org/abs/2310.00500)]
* Referring Image Segmentation:
    * **VLT**: "Vision-Language Transformer and Query Generation for Referring Segmentation", ICCV, 2021 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2108.05565)][[Tensorflow](https://github.com/henghuiding/Vision-Language-Transformer)]
    * **CRIS**: "CRIS: CLIP-Driven Referring Image Segmentation", CVPR, 2022 (*University of Sydney*). [[Paper](https://arxiv.org/abs/2111.15174)]
    * **LAVT**: "LAVT: Language-Aware Vision Transformer for Referring Image Segmentation", CVPR, 2022 (*Oxford*). [[Paper](https://arxiv.org/abs/2112.02244)]
    * **ReSTR**: "ReSTR: Convolution-free Referring Image Segmentation Using Transformers", CVPR, 2022 (*POSTECH*). [[Paper](https://arxiv.org/abs/2203.16768)][[Website](http://cvlab.postech.ac.kr/research/restr/)]
    * **ReCLIP**: "ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension", ACL, 2022 (*AI2*). [[Paper](https://arxiv.org/abs/2204.05991)]
    * **TSEG**: "Weakly-supervised segmentation of referring expressions", arXiv, 2022 (*INRIA*). [[Paper](https://arxiv.org/abs/2205.04725)]
    * **ZS-RIS**: "Zero-shot Referring Image Segmentation with Global-Local Context Features", CVPR, 2023 (*Gwangju Institute of Science and Technology (GIST)*). [[Paper](https://arxiv.org/abs/2303.17811)][[PyTorch](https://github.com/Seonghoon-Yu/Zero-shot-RIS)]
    * **PolyFormer**: "PolyFormer: Referring Image Segmentation as Sequential Polygon Generation", CVPR, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2302.07387)][[Website](https://polyformer.github.io/)]
    * **MCRES**: "Meta Compositional Referring Expression Segmentation", CVPR, 2023 (*Singapore University of Technology and Design*). [[Paper](https://arxiv.org/abs/2304.04415)]
    * **ReLA**: "GRES: Generalized Referring Expression Segmentation", CVPR, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2306.00968)][[PyTorch](https://github.com/henghuiding/ReLA)][[Website](https://henghuiding.github.io/GRES/)]
    * **CGFormer**: "Contrastive Grouping With Transformer for Referring Image Segmentation", CVPR, 2023 (*ShanghaiTech*). [[Paper](https://arxiv.org/abs/2309.01017)][[PyTorch](https://github.com/Toneyaya/CGFormer)]
    * **CCTF**: "Learning To Segment Every Referring Object Point by Point", CVPR, 2023 (*JD*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Qu_Learning_To_Segment_Every_Referring_Object_Point_by_Point_CVPR_2023_paper.html)][[Code (in construction)](https://github.com/qumengxue/Partial-RES)]
    * **ETRIS**: "Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation", ICCV, 2023 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2307.11545)][[PyTorch](https://github.com/kkakkkka/ETRIS)]
    * **DMMI**: "Beyond One-to-One: Rethinking the Referring Image Segmentation", ICCV, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2308.13853)][[PyTorch](https://github.com/toggle1995/RIS-DMMI)]
    * **TRIS**: "Referring Image Segmentation Using Text Supervision", ICCV, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2308.14575)][[PyTorch](https://github.com/fawnliu/TRIS)]
    * **SaG**: "Shatter and Gather: Learning Referring Image Segmentation with Text Supervision", ICCV, 2023 (*POSTECH*). [[Paper](https://arxiv.org/abs/2308.15512)][[PyTorch](https://github.com/kdwonn/SaG)][[Website](https://southflame.github.io/sag/)]
    * **GRSer**: "Advancing Referring Expression Segmentation Beyond Single Image", ICCV, 2023 (*SenseTime*). [[Paper](https://arxiv.org/abs/2305.12452)][[Code (in construction)](https://github.com/yixuan730/group-res)]
    * **APE**: "Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement", ICCV, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2304.01195)][[PyTorch](https://github.com/yangyangyang127/APE)]
    * **TAS**: "Text Augmented Spatial-aware Zero-shot Referring Image Segmentation", EMNLP, 2023 (*Zhejiang*). [[Paper](https://arxiv.org/abs/2310.18049)]
    * **RIO**: "RIO: A Benchmark for Reasoning Intention-Oriented Objects in Open Environments", NeurIPS, 2023 (*Beijing Jiaotong University*). [[Paper](https://arxiv.org/abs/2310.17290)][[PyTorch](https://github.com/qumengxue/RIO)][[Website](https://reasonio.github.io/)]
    * **VLT**: "VLT: Vision-Language Transformer and Query Generation for Referring Segmentation", TPAMI, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2210.15871)]
    * **IREG**: "Whether you can locate or not? Interactive Referring Expression Generation", arXiv, 2023 (*Beijing University of Posts and Telecommunications*). [[Paper](https://arxiv.org/abs/2308.09977)][[Code (in construction)](https://github.com/superhero-7/IREG)]
    * **R-RIS**: "Towards Robust Referring Image Segmentation", arXiv, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2209.09554)][[Code (in construction)](https://github.com/jzwu48033552/robust-ref-seg)][[Website](https://lxtgh.github.io/project/robust_ref_seg/)]
    * **PVD**: "Parallel Vertex Diffusion for Unified Visual Grounding", arXiv, 2023 (*Peking University*). [[Paper](https://arxiv.org/abs/2303.07216)]
    * **MMNet**: "MMNet: Multi-Mask Network for Referring Image Segmentation", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2305.14969)]
    * **LGFormer**: "Linguistic Query-Guided Mask Generation for Referring Image Segmentation", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2301.06429)]
    * **RISCLIP**: "RISCLIP: Referring Image Segmentation Framework using CLIP", arXiv, 2023 (*POSTECH*). [[Paper](https://arxiv.org/abs/2306.08498)]
    * **EAVL**: "EAVL: Explicitly Align Vision and Language for Referring Image Segmentation", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2308.09779)]
    * **Ref-Diff**: "Ref-Diff: Zero-shot Referring Image Segmentation with Generative Models", arXiv, 2023 (*Harbin Institute of Technology*). [[Paper](https://arxiv.org/abs/2308.16777)][[Code (in construction)](https://github.com/kodenii/Ref-Diff)]
    * **DuMoGa**: "Towards Complex-query Referring Image Segmentation: A Novel Benchmark", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2309.17205)]
    * **SSC**: "Segment, Select, Correct: A Framework for Weakly-Supervised Referring Segmentation", arXiv, 2023 (*Five AI, UK*). [[Paper](https://arxiv.org/abs/2310.13479)]
    * **Omni-RES**: "Towards Omni-supervised Referring Expression Segmentation", arXiv, 2023 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2311.00397)]
    * **BTMAE**: "Synchronizing Vision and Language: Bidirectional Token-Masking AutoEncoder for Referring Image Segmentation", arXiv, 2023 (*Yonsei*). [[Paper](https://arxiv.org/abs/2311.17952)]
    * **SESAME**: "See, Say, and Segment: Teaching LMMs to Overcome False Premises", arXiv, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2312.08366)][[Code (in construction)](https://github.com/see-say-segment/sesame)][[Website](https://see-say-segment.github.io/)]
    * **MRES**: "Unveiling Parts Beyond Objects: Towards Finer-Granularity Referring Expression Segmentation", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2312.08007)][[Code (in construction)](https://github.com/Rubics-Xuan/MRES)]
    * **MagNet**: "Mask Grounding for Referring Image Segmentation", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2312.12198)]
* Referring Video Segmentation:
    * **ReferFormer**: "Language as Queries for Referring Video Object Segmentation", CVPR, 2022 (*HKU*). [[Paper](https://arxiv.org/abs/2201.00487)][[PyTorch](https://github.com/wjn922/ReferFormer)]
    * **MTTR**: "End-to-End Referring Video Object Segmentation with Multimodal Transformers", CVPR, 2022 (*Technion - Israel Institute of Technology*). [[Paper](https://arxiv.org/abs/2111.14821)][[PyTorch](https://github.com/mttr2021/MTTR)]
    * **LBDT**: "Language-Bridged Spatial-Temporal Interaction for Referring Video Object Segmentation", CVPR, 2022 (*Meituan*). [[Paper](https://arxiv.org/abs/2206.03789)][[PyTorch](https://github.com/dzh19990407/LBDT)]
    * **DSA-BAS**: "Multi-Level Representation Learning With Semantic Alignment for Referring Video Object Segmentation", CVPR, 2022 (*IIAI, China*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Wu_Multi-Level_Representation_Learning_With_Semantic_Alignment_for_Referring_Video_Object_CVPR_2022_paper.html)]
    * **MANet**: "Multi-Attention Network for Compressed Video Referring Object Segmentation", ACMMM, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2207.12622)][[PyTorch](https://github.com/DexiangHong/MANet)]
    * **R<sup>2</sup>VOS**: "Robust Referring Video Object Segmentation with Cyclic Structural Consensus", ICCV, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2207.01203)][[PyTorch](https://github.com/lxa9867/R2VOS)][[Website](https://lxa9867.github.io/works/rrvos/)]
    * **OnlineRefer**: "OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation", ICCV, 2023 (*Megvii*). [[Paper](https://arxiv.org/abs/2307.09356)][[PyTorch](https://github.com/wudongming97/OnlineRefer)]
    * **SgMg**: "Spectrum-guided Multi-granularity Referring Video Object Segmentation", ICCV, 2023 (*The University of Western Australia*). [[Paper](https://arxiv.org/abs/2307.13537)][[PyTorch](https://github.com/bo-miao/SgMg)]
    * **MeViS**: "MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions", ICCV, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2308.08544)][[PyTorch](https://github.com/henghuiding/MeViS)][[Website](https://henghuiding.github.io/MeViS/)]
    * **CMA**: "Learning Cross-Modal Affinity for Referring Video Object Segmentation Targeting Limited Samples", ICCV, 2023 (*SUSTech*). [[Paper](https://arxiv.org/abs/2309.02041)][[PyTorch](https://github.com/hengliusky/Few_shot_RVOS)]
    * **TempCD**: "Temporal Collection and Distribution for Referring Video Object Segmentation", ICCV, 2023 (*ShanghaiTech*). [[Paper](https://arxiv.org/abs/2309.03473)][[Website](https://toneyaya.github.io/tempcd/)]
    * **UniRef**: "Segment Every Reference Object in Spatial and Temporal Spaces", ICCV, 2023 (*HKU*). [[Paper](https://openaccess.thecvf.com/content/ICCV2023/html/Wu_Segment_Every_Reference_Object_in_Spatial_and_Temporal_Spaces_ICCV_2023_paper.html)][[PyTorch](https://github.com/FoundationVision/UniRef)]
    * **HTML**: "HTML: Hybrid Temporal-scale Multimodal Learning Framework for Referring Video Object Segmentation", ICCV, 2023 (*University of Technology Sydney, UTS*). [[Paper](https://openaccess.thecvf.com/content/ICCV2023/html/Han_HTML_Hybrid_Temporal-scale_Multimodal_Learning_Framework_for_Referring_Video_Object_ICCV_2023_paper.html)][[Website](https://mingfei.info/HTML/)]
    * **?**: "1st Place Solution for 5th LSVOS Challenge: Referring Video Object Segmentation", ICCVW, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2401.00663)][[PyTorch](https://github.com/RobertLuo1/iccv2023_RVOS_Challenge)]
    * **SOC**: "SOC: Semantic-Assisted Object Cluster for Referring Video Object Segmentation", NeurIPS, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2305.17011)][[PyTorch](https://github.com/RobertLuo1/NeurIPS2023_SOC)]
    * **Locater**: "Local-Global Context Aware Transformer for Language-Guided Video Segmentation", TPAMI, 2023 (*Zhejiang*). [[Paper](https://arxiv.org/abs/2203.09773)][[PyTorch](https://github.com/leonnnop/Locater)]
    * **LoSh**: "LoSh: Long-Short Text Joint Prediction Network for Referring Video Object Segmentation", arXiv, 2023 (*Kings College London*). [[Paper](https://arxiv.org/abs/2306.08736)]
    * **RefSAM**: "RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation", arXiv, 2023 (*National University of Defense Technology, China*). [[Paper](https://arxiv.org/abs/2307.00997)][[Code (in construction)](https://github.com/LancasterLi/RefSAM)]
    * **IFIRVOS**: "Referring Video Object Segmentation with Inter-Frame Interaction and Cross-Modal Correlation", arXiv, 2023 (*Wuhan University*). [[Paper](https://arxiv.org/abs/2307.00536)]
    * **LGCFS**: "Learning Referring Video Object Segmentation from Weak Annotation", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2308.02162)]
    * **EPCFormer**: "EPCFormer: Expression Prompt Collaboration Transformer for Universal Referring Video Object Segmentation", arXiv, 2023 (*Hunan University*). [[Paper](https://arxiv.org/abs/2308.04162)][[Code (in construction)](https://github.com/lab206/EPCFormer)]
    * **FTEA**: "Fully Transformer-Equipped Architecture for End-to-End Referring Video Object Segmentation", arXiv, 2023 (*Hangzhou Dianzi University*). [[Paper](https://arxiv.org/abs/2309.11933)]
    * **UniRef++**: "UniRef++: Segment Every Reference Object in Spatial and Temporal Spaces", arXiv, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2312.15715)][[PyTorch](https://github.com/FoundationVision/UniRef)]
    * **MUTR**: "Referred by Multi-Modality: A Unified Temporal Transformer for Video Object Segmentation", AAAI, 2024 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2305.16318)][[PyTorch](https://github.com/OpenGVLab/MUTR)]
* Referring 3D Segmentation:
    * **3D-STMN**: "3D-STMN: Dependency-Driven Superpoint-Text Matching Network for End-to-End 3D Referring Expression Segmentation", arXiv, 2023 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2308.16632)][[PyTorch](https://github.com/sosppxo/3D-STMN)]
* Narrative Grounding:
    * **GELLA**: "Generalizable Entity Grounding via Assistance of Large Language Model", arXiv, 2024 (*UC Merced*). [[Paper](https://arxiv.org/abs/2402.02555)] 
* Tracking:
    * **ModaMixer**: "Divert More Attention to Vision-Language Tracking", NeurIPS, 2022 (*Beijing Jiaotong University*). [[Paper](https://arxiv.org/abs/2207.01076)][[PyTorch](https://github.com/JudasDie/SOTS)]
    * **TransRMOT**: "Referring Multi-Object Tracking", CVPR, 2023 (*Megvii*). [[Paper](https://arxiv.org/abs/2303.03366)][[PyTorch](https://github.com/wudongming97/RMOT)][[Website](https://referringmot.github.io/)]
    * **ModaMixer**: "Divert More Attention to Vision-Language Object Tracking", arXiv, 2023 (*Beijing Jiaotong University*). [[Paper](https://arxiv.org/abs/2307.10046)][[PyTorch](https://github.com/JudasDie/SOTS)]
* Analysis:
    * **MM-Explainability**: "Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers", ICCV, 2021 (*Tel Aviv*). [[Paper](https://arxiv.org/abs/2103.15679)][[PyTorch](https://github.com/hila-chefer/Transformer-MM-Explainability)]
    * **?**: "Are Multimodal Transformers Robust to Missing Modality?", CVPR, 2022 (*University of Delaware*). [[Paper](https://arxiv.org/abs/2204.05454)]
    * **VL-InterpreT**: "VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers", CVPR (demo), 2022 (*Intel*). [[Paper](https://arxiv.org/abs/2203.17247)][[Website](http://vlinterpretenv4env-env.eba-vmhhefup.us-east-2.elasticbeanstalk.com/)][[Video](https://www.youtube.com/watch?v=2HZ2IjzG5_4&ab_channel=MengDu)]
    * **?**: "Understanding Attention for Vision-and-Language Tasks", International Conference on Computational Linguistics (COLING), 2022 (*The University of Sydney*). [[Paper](https://arxiv.org/abs/2208.08104)]
    * **VL-CheckList**: "VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations", arXiv, 2022 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2207.00221)][[Code (in construction)](https://github.com/om-ai-lab/VL-CheckList)]
    * **?**: "Is BERT Blind? Exploring the Effect of Vision-and-Language Pretraining on Visual Language Understanding", CVPR, 2023 (*Tel Aviv*). [[Paper](https://arxiv.org/abs/2303.12513)][[PyTorch](https://github.com/TAU-VAILab/isbertblind)][[Website](https://tau-vailab.github.io/isbertblind/)]
    * **Why-Prompt**: "Doubly Right Object Recognition: A Why Prompt for Visual Rationales", CVPR, 2023 (*Columbia*). [[Paper](https://arxiv.org/abs/2212.06202)]
    * **CREPE**: "CREPE: Can Vision-Language Foundation Models Reason Compositionally?", CVPR, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2212.07796)]
    * **ZOOM**: "Zero-shot Model Diagnosis", CVPR, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2303.15441)]
    * **?**: "On the Generalization of Multi-modal Contrastive Learning", ICML, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2306.04272)][[PyTorch](https://github.com/PKU-ML/CLIP-Help-SimCLR)]
    * **?**: "Learning Concise and Descriptive Attributes for Visual Recognition", ICCV, 2023 (*UCSD*). [[Paper](https://arxiv.org/abs/2308.03685)]
    * **?**: "Linear Spaces of Meanings: Compositional Structures in Vision-Language Models", ICCV, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2302.14383)]
    * **?**: "Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models", EMNLP, 2023 (*University of Copenhagen, Denmark*). [[Paper](https://arxiv.org/abs/2310.17530)][[PyTorch](https://github.com/coastalcph/gender-neutral-vl)]
    * **GVIL**: "Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?", EMNLP, 2023 (*UMich*). [[Paper](https://arxiv.org/abs/2311.00047)][[GitHub](https://github.com/vl-illusion/dataset)][[Website](https://vl-illusion.github.io/)]
    * **LICO**: "LICO: Explainable Models with Language-Image Consistency", NeurIPS, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2310.09821)][[Code (in construction)](https://github.com/ymLeiFDU/LICO)]
    * **MultiMon**: "Mass-Producing Failures of Multimodal Systems with Language Models", NeurIPS, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2306.12105)][[PyTorch](https://github.com/tsb0601/MultiMon)]
    * **?**: "Kiki or Bouba? Sound Symbolism in Vision-and-Language Models", NeurIPS, 2023 (*Tel Aviv*). [[Paper](https://arxiv.org/abs/2310.16781)][[PyTorch](https://github.com/TAU-VAILab/kiki-bouba)][[Website](https://tau-vailab.github.io/kiki-bouba/)]
    * **M2IB**: "Visual Explanations of Image-Text Representations via Multi-Modal Information Bottleneck Attribution", NeurIPS, 2023 (*NYU*). [[Paper](https://arxiv.org/abs/2312.17174)]
    * **?**: "Interpreting CLIP's Image Representation via Text-Based Decomposition", arXiv, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2310.05916)][[PyTorch](https://github.com/yossigandelsman/clip_prs)][[Website](https://yossigandelsman.github.io/clip_decomposition/)]
    * **vit-interpret**: "Interpreting and Controlling Vision Foundation Models via Text Explanations", arXiv, 2023 (*Columbia*). [[Paper](https://arxiv.org/abs/2310.10591)][[PyTorch](https://github.com/tonychenxyz/vit-interpret)]
    * **MMVP**: "Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs", arXiv, 2024 (*NYU*). [[Paper](https://arxiv.org/abs/2401.06209)][[PyTorch](https://github.com/tsb0601/MMVP)][[Website](https://tsb0601.github.io/mmvp_blog/)]
    * **?**: "Exploring Perceptual Limitation of Multimodal Large Language Models", arXiv, 2024 (*USC*). [[Paper](https://arxiv.org/abs/2402.07384)][[PyTorch](https://github.com/saccharomycetes/mllm-perceptual-limitation)]
    * **DejaVu-Momerization**: "Dj Vu Memorization in Vision-Language Models", arXiv, 2024 (*Meta*). [[Paper](https://arxiv.org/abs/2402.02103)]
* Speaker Localization:
    * **?**: "The Right to Talk: An Audio-Visual Transformer Approach", ICCV, 2021 (*University of Arkansas*). [[Paper](https://arxiv.org/abs/2108.03256)]
* Multi-task:
    * **UniT**: "Transformer is All You Need: Multimodal Multitask Learning with a Unified Transformer", ICCV, 2021 (*Facebook*). [[Paper](https://arxiv.org/abs/2102.10772)][[PyTorch](https://github.com/facebookresearch/mmf)][[Website](https://mmf.sh/)]
    * **Pix2Seq**: "A Unified Sequence Interface for Vision Tasks", NeurIPS, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2206.07669)]
    * **LAVIS**: "LAVIS: A Library for Language-Vision Intelligence", arXiv, 2022 (*Salesforce*). [[Paper](https://arxiv.org/abs/2209.09019)][[PyTorch](https://github.com/salesforce/LAVIS)]
    * **Unified-IO**: "Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks", ICLR, 2023 (*AI2*). [[Paper](https://arxiv.org/abs/2206.08916)][[JAX](https://github.com/allenai/unified-io-inference)][[Website](https://unified-io.allenai.org/)]
    * **ImageBind**: "ImageBind: One Embedding Space To Bind Them All", CVPR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2305.05665)][[PyTorch](https://github.com/facebookresearch/ImageBind)][[Website](https://imagebind.metademolab.com/)]
    * **EgoT2**: "Egocentric Video Task Translation", CVPR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2212.06301)][[Website](https://vision.cs.utexas.edu/projects/egot2/)]
    * **VTAGML**: "Vision Transformer Adapters for Generalizable Multitask Learning", ICCV, 2023 (*EPFL*). [[Paper](https://arxiv.org/abs/2308.12372)][[Website](https://ivrl.github.io/VTAGML/)]
    * **VisionLLM**: "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks", NeurIPS, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2305.11175)][[Code (in construction)](https://github.com/OpenGVLab/VisionLLM)]
    * **CoCoCon**: "Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models", arXiv, 2023 (*AI2*). [[Paper](https://arxiv.org/abs/2303.16133)][[PyTorch](https://github.com/adymaharana/cococon)][[Website](https://adymaharana.github.io/cococon/)]
    * **ONE-PEACE**: "ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2305.11172)][[PyTorch (in construction)](https://github.com/OFA-Sys/ONE-PEACE)]
    * **VideoLLM**: "VideoLLM: Modeling Video Sequence with Large Language Models", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2305.13292)][[Code (in construction)](https://github.com/cg1177/VideoLLM)]
    * **i-Code-Studio**: "i-Code Studio: A Configurable and Composable Framework for Integrative AI", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2305.13738)][[Code (in construction)](https://github.com/microsoft/i-Code/tree/main/i-Code-Studio)][[Website](https://i-code-studio.github.io/)]
    * **Tag2Text**: "Tag2Text: Guiding Vision-Language Model via Image Tagging", arXiv, 2023 (*OPPO*). [[Paper](https://arxiv.org/abs/2303.05657)][[PyTorch](https://github.com/xinyu1205/Tag2Text)][[Website](https://tag2text.github.io/)]
    * **RAM**: "Recognize Anything: A Strong Image Tagging Model", arXiv, 2023 (*OPPO*). [[Paper](https://arxiv.org/abs/2306.03514)][[PyTorch](https://github.com/xinyu1205/Tag2Text-Recognize_Anything)][[Website](https://recognize-anything.github.io/)]
    * **InstructDiffusion**: "InstructDiffusion: A Generalist Modeling Interface for Vision Tasks", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2309.03895)][[PyTorch](https://github.com/cientgu/InstructDiffusion)][[Website](https://gengzigang.github.io/instructdiffusion.github.io/)]
    * **SPHINX**: "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2311.07575)][[PyTorch](https://github.com/Alpha-VLLM/LLaMA2-Accessory/tree/main/SPHINX)]
    * **UniLSeg**: "Universal Segmentation at Arbitrary Granularity with Language Instruction", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2312.01623)]
    * **APE**: "Aligning and Prompting Everything All at Once for Universal Visual Perception", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2312.02153)][[PyTorch](https://github.com/shenyunhang/APE)]
    * **Alpha-CLIP**: "Alpha-CLIP: A CLIP Model Focusing on Wherever You Want", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2312.03818)][[PyTorch](https://github.com/SunzeY/AlphaCLIP)][[Website](https://aleafy.github.io/alpha-clip/)]
    * **GLEE**: "General Object Foundation Model for Images and Videos at Scale", arXiv, 2023 (*Huazhong University of Science and Technology*). [[Paper](https://arxiv.org/abs/2312.09158)]
    * **VistaLLM**: "Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2312.12423)][[Website](https://shramanpramanick.github.io/VistaLLM/)]
    * **VCoder**: "VCoder: Versatile Vision Encoders for Multimodal Large Language Models", arXiv, 2023 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2312.14233)][[PyTorch](https://github.com/SHI-Labs/VCoder)][[Website](https://praeclarumjj3.github.io/vcoder/)]
    * **Unified-IO-2**: "Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action", arXiv, 2023 (*AI2*). [[Paper](https://arxiv.org/abs/2312.17172)][[JAX](https://github.com/allenai/unified-io-2)][[Website](https://unified-io-2.allenai.org/)]
    * **InstructCV**: "InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists", ICLR, 2024 (*Peking + Berkeley*). [[Paper](https://arxiv.org/abs/2310.00390)][[PyTorch](https://github.com/AlaaLab/InstructCV)]
    * **MAD**: "Masked AutoDecoder is Effective Multi-Task Vision Generalist", CVPR, 2024 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2403.07692)][[Code (in construction)](https://github.com/hanqiu-hq/MAD)]
    * **VLP**: "Using Left and Right Brains Together: Towards Vision and Language Planning", arXiv, 2024 (*Microsoft*). [[Paper](https://arxiv.org/abs/2402.10534)]
    * **V2T-Tokenizer**: "Beyond Text: Frozen Large Language Models in Visual Signal Comprehension", arXiv, 2024 (*Peking*). [[Paper](https://arxiv.org/abs/2403.07874)][[PyTorch](https://github.com/zh460045050/V2L-Tokenizer)]
    * **Lumen**: "Lumen: Unleashing Versatile Vision-Centric Capabilities of Large Multimodal Models", arXiv, 2024 (*Meituan*). [[Paper](https://arxiv.org/abs/2403.07304)][[Code (in construction)](https://github.com/SxJyJay/Lumen)]
* Language-based Video Editing:
    * **M<sup>3</sup>L**: "Language-based Video Editing via Multi-Modal Multi-Level Transformer", CVPR, 2022 (*UCSB*). [[Paper](https://arxiv.org/abs/2104.01122)]
    * **Video-P2P**: "Video-P2P: Video Editing with Cross-attention Control", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2303.04761)][[Website](https://video-p2p.github.io/)]
    * **FateZero**: "FateZero: Fusing Attentions for Zero-shot Text-based Video Editing", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2303.09535)][[PyTorch](https://github.com/ChenyangQiQi/FateZero)][[Website](https://fate-zero-edit.github.io/)]
    * **Make-A-Protagonist**: "Make-A-Protagonist: Generic Video Editing with An Ensemble of Experts", arXiv, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2305.08850)][[PyTorch](https://github.com/Make-A-Protagonist/Make-A-Protagonist)][[Website](https://make-a-protagonist.github.io/)]
    * **RAVA**: "Reframe Anything: LLM Agent for Open World Video Reframing", arXiv, 2024 (*Opus Research, Minnesota*). [[Paper](https://arxiv.org/abs/2403.06070)]
* Video Summarization:
    * **GPT2MVS**: "GPT2MVS: Generative Pre-trained Transformer-2 for Multi-modal Video Summarization", ICMR, 2021 (*BBC*). [[Paper](https://arxiv.org/abs/2104.12465)]
    * **QVHighlights**: "QVHighlights: Detecting Moments and Highlights in Videos via Natural Language Queries", NeurIPS, 2021 (*UNC*). [[Paper](https://arxiv.org/abs/2107.09609)][[PyTorch](https://github.com/jayleicn/moment_detr)]
    * **HMT**: "Hierarchical Multimodal Transformer to Summarize Videos", arXiv, 2021 (*Xidian University*). [[Paper](https://arxiv.org/abs/2109.10559)]
    * **?**: "Show Me What I Like: Detecting User-Specific Video Highlights Using Content-Based Multi-Head Attention", ACMMM, 2022 (*Adobe*). [[Paper](https://arxiv.org/abs/2207.08352)]
    * **IV-Sum**: "TL;DW? Summarizing Instructional Videos with Task Relevance & Cross-Modal Saliency", ECCV, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2208.06773)][[Website](https://medhini.github.io/ivsum/)]
    * **A2Summ**: "Align and Attend: Multimodal Summarization with Dual Contrastive Losses", CVPR, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2303.07284)][[Code (in construction)](https://github.com/boheumd/A2Summ)][[Website](https://boheumd.github.io/A2Summ/)]
    * **QD-DETR**: "Query-Dependent Video Representation for Moment Retrieval and Highlight Detection", CVPR, 2023 (*Sungkyunkwan University, Korea*). [[Paper](https://arxiv.org/abs/2303.13874)][[PyTorch](https://github.com/wjun0830/QD-DETR)]
    * **A2Summ**: "Align and Attend: Multimodal Summarization with Dual Contrastive Losses", CVPR, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2303.07284)][[PyTorch](https://github.com/boheumd/A2Summ)][[Website](https://boheumd.github.io/A2Summ/)]
    * **CLC**: "Collaborative Noisy Label Cleaner: Learning Scene-aware Trailers for Multi-modal Highlight Detection in Movies", CVPR, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2303.14768)][[Code (in construction)](https://github.com/TencentYoutuResearch/HighlightDetection-CLC)]
    * **VideoXum**: "VideoXum: Cross-modal Visual and Textural Summarization of Videos", arXiv, 2023 (*OPPO*). [[Paper](https://arxiv.org/abs/2303.12060)][[Website](https://videoxum.github.io/)]
    * **MH-DETR**: "MH-DETR: Video Moment and Highlight Detection with Cross-modal Transformer", arXiv, 2023 (*Nanjing University*). [[Paper](https://arxiv.org/abs/2305.00355)]
    * **VisionaryVid**: "Joint Moment Retrieval and Highlight Detection Via Natural Language Queries", arXiv, 2023 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2305.04961)][[PyTorch](https://github.com/Skyline-9/Visionary-Vids)]
    * **VIEWS**: "Video Summarization: Towards Entity-Aware Captions", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2312.02188)]
    * **TR-DETR**: "TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and Highlight Detection", AAAI, 2024 (*Central China Normal University*). [[Paper](https://arxiv.org/abs/2401.02309)][[PyTorch](https://github.com/mingyao1120/TR-DETR)]
* Robotics:
    * **CRT**: "Case Relation Transformer: A Crossmodal Language Generation Model for Fetching Instructions", IROS, 2021 (*Keio University*). [[Paper](https://arxiv.org/abs/2107.00789)]
    * **TraSeTR**: "TraSeTR: Track-to-Segment Transformer with Contrastive Query for Instance-level Instrument Segmentation in Robotic Surgery", ICRA, 2022 (*CUHK*). [[Paper](https://arxiv.org/abs/2202.08453)]
    * **VLMbench**: "VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation", NeurIPS (Datasets and Benchmarks), 2022 (*UC Santa Cruz*). [[Paper](https://arxiv.org/abs/2206.08522)][[Pytorch](https://github.com/eric-ai-lab/vlmbench)][[Website](https://sites.google.com/ucsc.edu/vlmbench/home)]
    * **Surgical-VQLA**: "Surgical-VQLA: Transformer with Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery", ICRA, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2305.11692)][[PyTorch](https://github.com/longbai1006/Surgical-VQLA)]
    * **?**: "Distilling Internet-Scale Vision-Language Models into Embodied Agents", ICML, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2301.12507)]
    * **LIV**: "LIV: Language-Image Representations and Rewards for Robotic Control", ICML, 2023 (*UPenn*). [[Paper](https://arxiv.org/abs/2306.00958)][[PyTorch](https://github.com/penn-pal-lab/LIV)][[Website](https://penn-pal-lab.github.io/LIV/)]
    * **PaLM-E**: "PaLM-E: An Embodied Multimodal Language Model", ICML, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.03378)][[Website](https://palm-e.github.io/)]
    * **VIMA**: "VIMA: General Robot Manipulation with Multimodal Prompts", ICML, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2210.03094)][[PyTorch](https://github.com/vimalabs/VIMA)][[Website](https://vimalabs.github.io/)]
    * **GVCCI**: "GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic Manipulation", IROS, 2023 (*SNU, Korea*). [[Paper](https://arxiv.org/abs/2307.05963)]
    * **ARNOLD**: "ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes", ICCV, 2023 (*UCLA*). [[Paper](https://arxiv.org/abs/2304.04321)][[PyTorch](https://github.com/arnold-benchmark/arnold)][[Website](https://arnold-benchmark.github.io/)]
    * **LACO**: "Language-Conditioned Path Planning", CoRL, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2308.16893)][[Code (in construction)](https://github.com/amberxie88/lapp)][[Website](https://amberxie88.github.io/lapp/)]
    * **CROG**: "Language-guided Robot Grasping: CLIP-based Referring Grasp Synthesis in Clutter", CoRL, 2023 (*University of Groningen, Netherlands*). [[Paper](https://arxiv.org/abs/2311.05779)][[PyTorch](https://github.com/gtziafas/OCID-VLG)]
    * **DiffVL**: "DiffVL: Scaling Up Soft Body Manipulation using Vision-Language Driven Differentiable Physics", NeurIPS, 2023 (*UCSD*). [[Paper](https://arxiv.org/abs/2312.06408)][[PyTorch](https://github.com/Winniechen2002/DiffVL)][[Website](https://sites.google.com/view/diffvl/home)]
    * **HiP**: "Compositional Foundation Models for Hierarchical Planning", NeurIPS, 2023 (*MIT*). [[Paper](https://arxiv.org/abs/2309.08587)][[PyTorch](https://github.com/anuragajay/hip/)][[Website](https://hierarchical-planning-foundation-model.github.io/)]
    * **Grounded-Decoding**: "Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.00855)][[Website](https://grounded-decoding.github.io/)]
    * **MOO**: "Open-World Object Manipulation using Pre-trained Vision-Language Models", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.00905)][[Website](https://robot-moo.github.io/)]
    * **?**: "Vision-Language Models as Success Detectors", arXiv, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2303.07280)]
    * **VC-1**: "Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2303.18240)][[Website](https://eai-vc.github.io/)]
    * **HomeRobot**: "HomeRobot: Open-Vocabulary Mobile Manipulation", arXiv, 2023 (*Georgia Tech + Meta*). [[Paper](https://arxiv.org/abs/2306.11565)][[PyTorch](https://github.com/facebookresearch/home-robot)][[Website](https://ovmm.github.io/)]
    * **TaPA**: "Embodied Task Planning with Large Language Models", arXiv, 2023 (*Beijing University of Posts and Telecommunications*). [[Paper](https://arxiv.org/abs/2307.01848)][[PyTorch](https://github.com/Gary3410/TaPA)][[Website](https://gary3410.github.io/TaPA/)]
    * **VoxPoser**: "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models", arXiv, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2307.05973)][[Website](https://voxposer.github.io/)]
    * **RT-2**: "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control", arXiv, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2307.15818)][[Website](https://robotics-transformer2.github.io/)]
    * **VLP**: "Video Language Planning", arXiv, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2310.10625)][[Code (in construction)](https://github.com/video-language-planning/vlp_code)][[Website](https://video-language-planning.github.io/)]
    * **RoboFlamingo**: "Vision-Language Foundation Models as Effective Robot Imitators", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2311.01378)][[Website](https://roboflamingo.github.io/)]
    * **?**: "GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2311.12015)][[Website](https://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/)]
    * **AffordanceLLM**: "AffordanceLLM: Grounding Affordance from Vision Language Models", arXiv, 2024 (*Amazon*). [[Paper](https://arxiv.org/abs/2401.06341)][[Website](https://jasonqsy.github.io/AffordanceLLM/)]
    * **MultiPLY**: "MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World", arXiv, 2024 (*UMass*). [[Paper](https://arxiv.org/abs/2401.08577)][[Code (in construction)](https://github.com/UMass-Foundation-Model/MultiPLY)][[Website](https://vis-www.cs.umass.edu/multiply/)]
    * **seeing-unseen**: "Seeing the Unseen: Visual Common Sense for Semantic Placement", arXiv, 2024 (*AI2*). [[Paper](https://arxiv.org/abs/2401.07770)][[Website](https://ram81.github.io/projects/seeing-unseen.html)]
    * **PIVOT**: "PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs", arXiv, 2024 (*DeepMind*). [[Paper](https://arxiv.org/abs/2402.07872)][[Website](https://pivot-prompt.github.io/)]
    * **VPDD**: "Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning", arXiv, 2024 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2402.14407)][[Website](https://video-diff.github.io/)]
    * **DecisionNCE**: "DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning", arXiv, 2024 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2402.18137)][[PyTorch](https://github.com/2toinf/DecisionNCE)][[Website](https://2toinf.github.io/DecisionNCE/)]
    * **3D-VLA**: "3D-VLA: A 3D Vision-Language-Action Generative World Model", arXiv, 2024 (*UMass*). [[Paper](https://arxiv.org/abs/2403.09631)][[Code (in construction)](https://github.com/UMass-Foundation-Model/3D-VLA)][[Website](https://vis-www.cs.umass.edu/3dvla/)]
* Multi-modal Fusion:
    * **MICA**: "Attention Is Not Enough: Mitigating the Distribution Discrepancy in Asynchronous Multimodal Sequence Fusion", ICCV, 2021 (*Southwest Jiaotong University*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Liang_Attention_Is_Not_Enough_Mitigating_the_Distribution_Discrepancy_in_Asynchronous_ICCV_2021_paper.html)]
    * **IFT**: "Image Fusion Transformer", arXiv, 2021 (*Johns Hopkins*). [[Paper](https://arxiv.org/abs/2107.09011)][[PyTorch](https://github.com/Vibashan/Image-Fusion-Transformer)]
    * **PPT**: "PPT Fusion: Pyramid Patch Transformer for a Case Study in Image Fusion", arXiv, 2021 (*?*). [[Paper](https://arxiv.org/abs/2107.13967)]
    * **TransFuse**: "TransFuse: A Unified Transformer-based Image Fusion Framework using Self-supervised Learning", arXiv, 2022 (*Fudan University*). [[Paper](https://arxiv.org/abs/2201.07451)]
    * **SwinFuse**: "SwinFuse: A Residual Swin Transformer Fusion Network for Infrared and Visible Images", arXiv, 2022 (*Taiyuan University of Science and Technology*). [[Paper](https://arxiv.org/abs/2204.11436)]
    * **?**: "Array Camera Image Fusion using Physics-Aware Transformers", arXiv, 2022 (*University of Arizona*). [[Paper](https://arxiv.org/abs/2207.02250)]
    * **CDDFuse**: "CDDFuse: Correlation-Driven Dual-Branch Feature Decomposition for Multi-Modality Image Fusion", CVPR, 2023 (*ETHZ*). [[Paper](https://arxiv.org/abs/2211.14461)][[PyTorch](https://github.com/Zhaozixiang1228/MMIF-CDDFuse)]
* Human Interaction:
    * **Dyadformer**: "Dyadformer: A Multi-modal Transformer for Long-Range Modeling of Dyadic Interactions", ICCVW, 2021 (*Universitat de Barcelona*). [[Paper](https://arxiv.org/abs/2109.09487)]
* 3D:
    * **3DRefTransformer**: "3DRefTransformer: Fine-Grained Object Identification in Real-World Scenes Using Natural Language", WACV, 2022 (*KAUST*). [[Paper](https://openaccess.thecvf.com/content/WACV2022/html/Abdelreheem_3DRefTransformer_Fine-Grained_Object_Identification_in_Real-World_Scenes_Using_Natural_Language_WACV_2022_paper.html)][[Website](https://vision-cair.github.io/3dreftransformer/)]
    * **EDA**: "EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual and Language Learning", arXiv, 2022 (*Peking University*). [[Paper](https://arxiv.org/abs/2209.14941)]
    * **PLA**: "Language-driven Open-Vocabulary 3D Scene Understanding", CVPR, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2211.16312)][[PyTorch](https://github.com/CVMI-Lab/PLA)][[Website](https://dingry.github.io/projects/PLA)]
    * **VL-SAT**: "VL-SAT: Visual-Linguistic Semantics Assisted Training for 3D Semantic Scene Graph Prediction in Point Cloud", CVPR, 2023 (*Beihang University*). [[Paper](https://arxiv.org/abs/2303.14408)][[PyTorch](https://github.com/wz7in/CVPR2023-VLSAT)]
    * **LERF**: "LERF: Language Embedded Radiance Fields", ICCV, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2303.09553)][[Website](https://www.lerf.io/)]
    * **ConceptFusion**: "ConceptFusion: Open-set Multimodal 3D Mapping", arXiv, 2023 (*MIT*). [[Paper](https://arxiv.org/abs/2302.07241)][[Website](https://concept-fusion.github.io/)]
    * **CG3D**: "CLIP goes 3D: Leveraging Prompt Tuning for Language Grounded 3D Recognition", arXiv, 2023 (*JHU*). [[Paper](https://arxiv.org/abs/2303.11313)][[PyTorch](https://github.com/deeptibhegde/CLIP-goes-3D)][[Website](https://jeya-maria-jose.github.io/cg3d-web/)]
    * **DiffCLIP**: "DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification", arXiv, 2023 (*Beijing Institute of Technology*). [[Paper](https://arxiv.org/abs/2305.15957)]
    * **LLM-Grounder**: "LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent", arXiv, 2023 (*UMich*). [[Paper](https://arxiv.org/abs/2309.12311)][[PyTorch](https://github.com/sled-group/chat-with-nerf)][[Website](https://chat-with-nerf.github.io/)]
    * **ShapeGPT**: "ShapeGPT: 3D Shape Generation with A Unified Multi-modal Language Model", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2311.17618)][[Website](https://shapegpt.github.io/)][[Website](https://github.com/OpenShapeLab/ShapeGPT)]
    * **LEGaussain**: "Language Embedded 3D Gaussians for Open-Vocabulary Scene Understanding", arXiv, 2023 (*Beihang*). [[Paper](https://arxiv.org/abs/2311.18482)]
    * **Gaussian-Grouping**: "Gaussian Grouping: Segment and Edit Anything in 3D Scenes", arXiv, 2023 (*ETHZ*). [[Paper](https://arxiv.org/abs/2312.00732)][[Code (in construction)](https://github.com/lkeab/gaussian-grouping)]
    * **GPT4Point**: "GPT4Point: A Unified Framework for Point-Language Understanding and Generation", arXiv, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2312.02980)][[PyTorch](https://github.com/Pointcept/PointBLIP)][[Website](https://gpt4point.github.io/)]
    * **LangSplat**: "LangSplat: 3D Language Gaussian Splatting", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2312.16084)][[PyTorch](https://github.com/minghanqin/LangSplat)][[Website](https://langsplat.github.io/)]
    * **Open-NeRF**: "Open-NeRF: Towards Open Vocabulary NeRF Decomposition", WACV, 2024 (*UIUC*). [[Paper](https://arxiv.org/abs/2310.16383)]
    * **PPT**: "Parameter-efficient Prompt Learning for 3D Point Cloud Understanding", ICRA, 2024 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2402.15823)][[PyTorch](https://github.com/auniquesun/PPT)]
    * **TAMM**: "TAMM: TriAdapter Multi-Modal Learning for 3D Shape Understanding", CVPR, 2024 (*UIUC*). [[Paper](https://arxiv.org/abs/2402.18490)][[PyTorch](https://github.com/alanzhangcs/Tamm_Code)][[Website](https://alanzhangcs.github.io/tamm-page/)]
    * **FMGS**: "FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding", arXiv, 2024 (*Google*). [[Paper](https://arxiv.org/abs/2401.01970)]
* 3D Segmentation:
    * **OpenScene**: "OpenScene: 3D Scene Understanding with Open Vocabularies", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2211.15654)][[PyTorch](https://github.com/pengsongyou/openscene)][[Website](https://pengsongyou.github.io/openscene)]
    * **PartSLIP**: "PartSLIP: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models", CVPR, 2023 (*Qualcomm*). [[Paper](https://arxiv.org/abs/2212.01558)]
    * **CLIP2Scene**: "CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP", CVPR, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2301.04926)][[PyTorch](https://github.com/runnanchen/CLIP2Scene)]
    * **PLA**: "Language-driven Open-Vocabulary 3D Scene Understanding", CVPR, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2211.16312)][[PyTorch](https://github.com/CVMI-Lab/PLA)][[Website](https://dingry.github.io/projects/PLA)]
    * **3D-Highlighter**: "3D Highlighter: Localizing Regions on 3D Shapes via Text Descriptions", CVPR, 2023 (*University of Chicago*). [[Paper](https://arxiv.org/abs/2212.11263)][[PyTorch](https://github.com/threedle/3DHighlighter)][[Website](https://threedle.github.io/3DHighlighter/)]
    * **CLIP-FO3D**: "CLIP-FO3D: Learning Free Open-world 3D Scene Representations from 2D Dense CLIP", ICCVW, 2023 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2303.04748)]
    * **OpenSUN3D**: "OpenSUN3D: 1st Workshop Challenge on Open-Vocabulary 3D Scene Understanding", ICCVW, 2023 (*ETHZ*). [[Paper](https://arxiv.org/abs/2402.15321)][[Website](https://opensun3d.github.io/index_iccv23.html)]
    * **OVSG**: "Context-Aware Entity Grounding with Open-Vocabulary 3D Scene Graphs", CoRL, 2023 (*Rutgers*). [[Paper](https://arxiv.org/abs/2309.15940)][[PyTorch](https://github.com/changhaonan/OVSG)]
    * **OVIR-3D**: "OVIR-3D: Open-Vocabulary 3D Instance Retrieval Without Training on 3D Data", CoRL, 2023 (*Rutgers*). [[Paper](https://arxiv.org/abs/2311.02873)][[PyTorch](https://github.com/shiyoung77/OVIR-3D)]
    * **3D-OVS**: "Weakly Supervised 3D Open-vocabulary Segmentation", NeurIPS, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2305.14093)][[PyTorch](https://github.com/Kunhao-Liu/3D-OVS)]
    * **OpenMask3D**: "OpenMask3D: Open-Vocabulary 3D Instance Segmentation", NeurIPS, 2023 (*ETHZ*). [[Paper](https://arxiv.org/abs/2306.13631)][[PyTorch](https://github.com/OpenMask3D/openmask3d)][[Website](https://openmask3d.github.io/)]
    * **Seal**: "Segment Any Point Cloud Sequences by Distilling Vision Foundation Models", NeurIPS, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2306.09347)][[PyTorch](https://github.com/youquanl/Segment-Any-Point-Cloud)]
    * **POP-3D**: "POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images", NeurIPS, 2023 (*valeo.ai, France*). [[Paper](https://arxiv.org/abs/2401.09413)][[PyTorch](https://github.com/vobecant/POP3D)][[Website](https://vobecant.github.io/POP3D/)]
    * **OVO**: "OVO: Open-Vocabulary Occupancy", arXiv, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2305.16133)]
    * **SAM3D**: "SAM3D: Segment Anything in 3D Scenes", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2306.03908)][[PyTorch](https://github.com/Pointcept/SegmentAnything3D)]
    * **Lowis3D**: "Lowis3D: Language-Driven Open-World Instance-Level 3D Scene Understanding", arXiv, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2308.00353)]
    * **OpenIns3D**: "OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation", arXiv, 2023 (*Cambridge*). [[Paper](https://arxiv.org/abs/2309.00616)][[Website](https://zheninghuang.github.io/OpenIns3D/)]
    * **ConceptGraphs**: "ConceptGraphs: Open-Vocabulary 3D Scene Graphs for Perception and Planning", arXiv, 2023 (*University of Toronto + Universite de Montreal*). [[Paper](https://arxiv.org/abs/2309.16650)][[PyTorch](https://github.com/concept-graphs/concept-graphs)][[Website](https://concept-graphs.github.io/)]
    * **OmniSeg3D**: "OmniSeg3D: Omniversal 3D Segmentation via Hierarchical Contrastive Learning", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2311.11666)][[Website](https://oceanying.github.io/OmniSeg3D/)]
    * **SAMPro3D**: "SAMPro3D: Locating SAM Prompts in 3D for Zero-Shot Scene Segmentation", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2311.17707)][[Code (in construction)](https://github.com/GAP-LAB-CUHK-SZ/SAMPro3D)][[Website](https://mutianxu.github.io/sampro3d/)]
    * **LL3DA**: "LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning", arXiv, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2311.18651)][[Code (in construction)](https://github.com/Open3DA/LL3DA)][[Website](https://ll3da.github.io/)]
    * **PartSLIP++**: "PartSLIP++: Enhancing Low-Shot 3D Part Segmentation via Multi-View Instance Segmentation and Maximum Likelihood Estimation", arXiv, 2023 (*UCSD*). [[Paper](https://arxiv.org/abs/2312.03015)][[PyTorch](https://github.com/zyc00/PartSLIP2)]
    * **Uni3DL**: "Uni3DL: Unified Model for 3D and Language Understanding", arXiv, 2023 (*KAUST*). [[Paper](https://arxiv.org/abs/2312.03026)][[Website](https://uni3dl.github.io/)]
    * **PartDistill**: "PartDistill: 3D Shape Part Segmentation by Vision-Language Model Distillation", arXiv, 2023 (*NYCU*). [[Paper](https://arxiv.org/abs/2312.04016)]
    * **Open3DIS**: "Open3DIS: Open-vocabulary 3D Instance Segmentation with 2D Mask Guidance", arXiv, 2023 (*VinAI*). [[Paper](https://arxiv.org/abs/2312.10671)][[Website](https://open3dis.github.io/)]
    * **Segment3D**: "Segment3D: Learning Fine-Grained Class-Agnostic 3D Segmentation without Manual Labels", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2312.17232)][[Website](https://segment3d.github.io/)]
    * **?**: "3D Open-Vocabulary Panoptic Segmentation with 2D-3D Vision-Language Distillation", arXiv, 2024 (*Waymo*). [[Paper](https://arxiv.org/abs/2401.02402)]
    * **PartSTAD**: "PartSTAD: 2D-to-3D Part Segmentation Task Adaptation", arXiv, 2024 (*KAIST*). [[Paper](https://arxiv.org/abs/2401.05906)]
    * **MaskClustering**: "MaskClustering: View Consensus based Mask Graph Clustering for Open-Vocabulary 3D Instance Segmentation", arXiv, 2024 (*Peking*). [[Paper](https://arxiv.org/abs/2401.07745)][[Website](https://pku-epic.github.io/MaskClustering/)]
    * **GARField**: "GARField: Group Anything with Radiance Fields", arXiv, 2024 (*Berkeley*). [[Paper](https://arxiv.org/abs/2401.09419)][[PyTorch](https://github.com/chungmin99/garfield)][[Website](https://www.garfield.studio/)]
    * **SA-GS**: "Segment Anything in 3D Gaussians", arXiv, 2024 (*The Hong Kong Polytechnic University*). [[Paper](https://arxiv.org/abs/2401.17857)]
    * **OV-NeRF**: "OV-NeRF: Open-vocabulary Neural Radiance Fields with Vision and Language Foundation Models for 3D Semantic Understanding", arXiv, 2024 (*Peking*). [[Paper](https://arxiv.org/abs/2402.04648)]
    * **Open3DSG**: "Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships", arXiv, 2024 (*Bosch*). [[Paper](https://arxiv.org/abs/2402.12259)][[Website](https://kochsebastian.com/open3dsg)]
    * **PointSeg**: "PointSeg: A Training-Free Paradigm for 3D Scene Segmentation via Foundation Models", arXiv, 2024 (*Tencent*). [[Paper](https://arxiv.org/abs/2403.06403)]
* Speech Recognition:
    * **AV-HuBERT**: "Robust Self-Supervised Audio-Visual Speech Recognition", arXiv, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2201.01763)][[PyTorch](https://github.com/facebookresearch/av_hubert)]
    * **?**: "Transformer-Based Video Front-Ends for Audio-Visual Speech Recognition", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2201.10439)]
    * **AVFormer**: "AVFormer: Injecting Vision into Frozen Speech Models for Zero-Shot AV-ASR", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.16501)]
    * **AV-RelScore**: "Watch or Listen: Robust Audio-Visual Speech Recognition with Visual Corruption Modeling and Reliability Scoring", CVPR, 2023 (*KAIST*). [[Paper](https://arxiv.org/abs/2303.08536)][[PyTorch](https://github.com/joannahong/AV-RelScore)]
    * **SynthVSR**: "SynthVSR: Scaling Up Visual Speech Recognition With Synthetic Supervision", CVPR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2303.17200)]
    * **Lip2Vec**: "Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping", ICCV, 2023 (*Technology Innovation Institute (TII), UAE*). [[Paper](https://arxiv.org/abs/2308.06112)]
* Emotion Recognition:
    * **?**: "A Pre-trained Audio-Visual Transformer for Emotion Recognition", ICASSP, 2022 (*USC*). [[Paper](https://arxiv.org/abs/2201.09165)]
    * **MDAN**: "MDAN: Multi-level Dependent Attention Network for Visual Emotion Analysis", CVPR, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2203.13443)]
    * **DMD**: "Decoupled Multimodal Distilling for Emotion Recognition", CVPR, 2023 (*Nanjing University of Science and Technology*). [[Paper](https://arxiv.org/abs/2303.13802)][[PyTorch](https://github.com/mdswyz/DMD)]
* Sound Separation:
    * **VoViT**: "VoViT: Low Latency Graph-based Audio-Visual Voice Separation Transformer", ECCV, 2022 (*Universitat Pompeu Fabra, Spain*). [[Paper](https://arxiv.org/abs/2203.04099)][[PyTorch](https://github.com/JuanFMontesinos/VoViT)][[Website](https://ipcv.github.io/VoViT/)]
    * **iQuery**: "iQuery: Instruments as Queries for Audio-Visual Sound Separation", CVPR, 2023 (*UCSD*). [[Paper](https://arxiv.org/abs/2212.03814)][[Code (in construction)](https://github.com/JiabenChen/iQuery)]
    * **VAST**: "Language-Guided Audio-Visual Source Separation via Trimodal Consistency", CVPR, 2023 (*Boston University*). [[Paper](https://arxiv.org/abs/2303.16342)][[Website](https://cs-people.bu.edu/rxtan/projects/VAST/)]
    * **AVIN**: "Induction Network: Audio-Visual Modality Gap-Bridging for Self-Supervised Sound Source Localization", ACMMM, 2023 (*Northwestern Polytechnical University*). [[Paper](https://arxiv.org/abs/2308.04767)][[Code (in construction)](https://github.com/Tahy1/AVIN)]
    * **GAVS**: "Prompting Segmentation with Sound is Generalizable Audio-Visual Source Localizer", arXiv, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2309.07929)]
* Audio-Visual:
    * **AV-HuBERT**: "Learning Audio-Visual Speech Representation by Masked Multimodal Cluster Prediction", ICLR, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2201.02184)][[PyTorch](https://github.com/facebookresearch/av_hubert)]
    * **AVCA**: "Audio-visual Generalised Zero-shot Learning with Cross-modal Attention and Language", CVPR, 2022 (*University of Tubingen, Germany*). [[Paper](https://arxiv.org/abs/2203.03598)][[PyTorch](https://github.com/ExplainableML/AVCA-GZSL)]
    * **TCaF**: "Temporal and cross-modal attention for audio-visual zero-shot learning", ECCV, 2022 (*University of Tubingen, Germany*). [[Paper](https://arxiv.org/abs/2207.09966)][[PyTorch](https://github.com/ExplainableML/TCAF-GZSL)]
    * **AVA-Memory**: "Audio-Visual Mismatch-Aware Video Retrieval via Association and Adjustment", ECCV, 2022 (*KAIST*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5034_ECCV_2022_paper.php)]
    * **TVLT**: "TVLT: Textless Vision-Language Transformer", NeurIPS, 2022 (*UNC*). [[Paper](https://arxiv.org/abs/2209.14156)][[PyTorch](https://github.com/zinengtang/TVLT)]
    * **ANGIE**: "Audio-Driven Co-Speech Gesture Video Generation", NeurIPS, 2022 (*CUHK*). [[Paper](https://openreview.net/forum?id=VhgC3SMTiy)][[Website](https://alvinliu0.github.io/projects/ANGIE)]
    * **MGN**: "Multi-modal Grouping Network for Weakly-Supervised Audio-Visual Video Parsing", NeurIPS, 2022 (*CMU + UT Austin*). [[Paper](https://openreview.net/forum?id=zfo2LqFEVY)][[PyTorch](https://github.com/stoneMo/MGN)]
    * **FS-RIR**: "Few-Shot Audio-Visual Learning of Environment Acoustics", NeurIPS, 2022 (*UT Austin*). [[Paper](https://arxiv.org/abs/2206.04006)][[Website](https://vision.cs.utexas.edu/projects/fs_rir/)]
    * **u-HuBERT**: "u-HuBERT: Unified Mixed-Modal Speech Pretraining And Zero-Shot Transfer to Unlabeled Modality", NeurIPS, 2022 (*Meta*). [[Paper](https://openreview.net/forum?id=zrAUoI2JA2)]
    * **PC-VAE**: "Multimodal Transformer for Parallel Concatenated Variational Autoencoders", NeurIPSW, 2022 (*USC*). [[Paper](https://arxiv.org/abs/2210.16174)]
    * **AV-CAT**: "Masked Lip-Sync Prediction by Audio-Visual Contextual Exploitation in Transformers", SIGGRAPH Asia, 2022 (*Tokyo Institute of Technology + Baidu*). [[Paper](https://arxiv.org/abs/2212.04970)][[Website](https://hangz-nju-cuhk.github.io/projects/AV-CAT)]
    * **MTD**: "Multimodal Transformer Distillation for Audio-Visual Synchronization", arXiv, 2022 (*NTU*). [[Paper](https://arxiv.org/abs/2210.15563)]
    * **AVE-CLIP**: "AVE-CLIP: AudioCLIP-based Multi-window Temporal Transformer for Audio Visual Event Localization", WACV, 2023 (*UT Austin*). [[Paper](https://arxiv.org/abs/2210.05060)]
    * **CLIPSep**: "CLIPSep: Learning Text-queried Sound Separation with Noisy Unlabeled Videos", ICLR, 2023 (*Sony*). [[Paper](https://arxiv.org/abs/2212.07065)]
    * **CAV-MAE**: "Contrastive Audio-Visual Masked Autoencoder", ICLR, 2023 (*MIT + IBM*). [[Paper](https://arxiv.org/abs/2210.07839)]
    * **UnAV**: "Dense-Localizing Audio-Visual Events in Untrimmed Videos: A Large-Scale Benchmark and Baseline", CVPR, 2023 (*Southern University of Science and Technology*). [[Paper](https://arxiv.org/abs/2303.12930)][[PyTorch](https://github.com/ttgeng233/UnAV)][[Website](https://unav100.github.io/)]
    * **LAVISH**: "Vision Transformers are Parameter-Efficient Audio-Visual Learners", CVPR, 2023 (*UNC*). [[Paper](https://arxiv.org/abs/2212.07983)][[Pytorch](https://github.com/GenjiB/LAVISH)][[Website](https://yanbo.ml/project_page/LAVISH/)]
    * **OneAVM**: "A Unified Audio-Visual Learning Framework for Localization, Separation, and Recognition", ICML, 2023 (*CMU + UW Madison*). [[Paper](https://arxiv.org/abs/2305.19458)][[Code (in construction)](https://github.com/stoneMo/OneAVM)]
    * **AdVerb**: "AdVerb: Visually Guided Audio Dereverberation", ICCV, 2023 (*Maryland*). [[Paper](https://arxiv.org/abs/2308.12370)][[Website](https://gamma.umd.edu/researchdirections/speech/adverb)]
    * **CIGN**: "Class-Incremental Grouping Network for Continual Audio-Visual Learning", ICCV, 2023 (*UT Dallas*). [[Paper](https://arxiv.org/abs/2309.05281)][[PyTorch](https://github.com/stoneMo/CIGN)]
    * **AV-CIL**: "Audio-Visual Class-Incremental Learning", ICCV, 2023 (*UT Dallas*). [[Paper](https://arxiv.org/abs/2308.11073)][[PyTorch](https://github.com/weiguoPian/AV-CIL_ICCV2023)]
    * **Audiovisual-MAE**: "Audiovisual Masked Autoencoders", ICCV, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2212.05922)]
    * **MAViL**: "MAViL: Masked Audio-Video Learners", NeurIPS, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2212.08071)][[Code (in construction)](https://github.com/facebookresearch/MAViL)]
    * **LSLD**: "Revisit Weakly-Supervised Audio-Visual Video Parsing from the Language Perspective", NeurIPS, 2023 (*Wuhan University*). [[Paper](https://arxiv.org/abs/2306.00595)][[PyTorch](https://github.com/fyyCS/LSLD)]
    * **DG-SCT**: "Cross-modal Prompts: Adapting Large Pre-trained Models for Audio-Visual Downstream Tasks", NeurIPS, 2023 (*Zhejiang*). [[Paper](https://arxiv.org/abs/2311.05152)][[PyTorch](https://github.com/haoyi-duan/DG-SCThttps://arxiv.org/abs/2311.05152)]
    * **VALOR**: "Modality-Independent Teachers Meet Weakly-Supervised Audio-Visual Event Parser", NeurIPS, 2023 (*NTU*). [[Paper](https://arxiv.org/abs/2305.17343)][[PyTorch](https://github.com/Franklin905/VALOR)]
    * **GestureDiffuCLIP**: "GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents", arXiv, 2023 (*Peking University*). [[Paper](https://arxiv.org/abs/2303.14613)]
    * **MMViT**: "MMViT: Multiscale Multiview Vision Transformers", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2305.00104)]
    * **?**: "Learning Spatial Features from Audio-Visual Correspondence in Egocentric Videos" arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2307.04760)]
* Audio-Visual Localization/Segmentation:
    * **AVSBench**: "Audio-Visual Segmentation", ECCV, 2022 (*SenseTime*). [[Paper](https://arxiv.org/abs/2207.05042)][[PyTorch](https://github.com/OpenNLPLab/AVSBench)][[Website](https://opennlplab.github.io/AVSBench/)]
    * **ECMVAE**: "Multimodal Variational Auto-encoder based Audio-Visual Segmentation", ICCV, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2310.08303)][[PyTorch](https://github.com/OpenNLPLab/MMVAE-AVS)][[Website](https://npucvr.github.io/MMVAE-AVS/)]
    * **WS-AVS**: "Weakly-Supervised Audio-Visual Segmentation", NeurIPS, 2023 (*CMU + MBZUAI*). [[Paper](https://arxiv.org/abs/2311.15080)]
    * **AV-SAM**: "AV-SAM: Segment Anything Model Meets Audio-Visual Localization and Segmentation", arXiv, 2023 (*CMU + UT Dallas*). [[Paper](https://arxiv.org/abs/2305.01836)]
    * **AUSS**: "Hear to Segment: Unmixing the Audio to Guide the Semantic Segmentation", arXiv, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2305.07223)]
    * **AuTR**: "Annotation-free Audio-Visual Segmentation", arXiv, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2305.11019)]
    * **AVSegFormer**: "AVSegFormer: Audio-Visual Segmentation with Transformer", arXiv, 2023 (*Nanjing University*). [[Paper](https://arxiv.org/abs/2307.01146)][[PyTorch](https://github.com/vvvb-github/AVSegFormer)]
    * **SQD**: "Rethinking Audiovisual Segmentation with Semantic Quantization and Decomposition", arXiv, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2310.00132)]
    * **DiffMAViL**: "Diffusion Models as Masked Audio-Video Learners", arXiv, 2023 (*Apple*). [[Paper](https://arxiv.org/abs/2310.03937)]
    * **AVIS**: "Audio-Visual Instance Segmentation", arXiv, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2310.18709)]
    * **COMBO**: "Cooperation Does Matter: Exploring Multi-Order Bilateral Relations for Audio-Visual Segmentation", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2312.06462)][[Code (in construction)](https://github.com/yannqi/COMBO-AVS)][[Website](https://combo-avs.github.io/)]
* Audio Description:
    * **AutoAD**: "AutoAD: Movie Description in Context", CVPR, 2023 (*Oxford*). [[Paper](https://arxiv.org/abs/2303.16899)][[PyTorch (in construction)](https://github.com/TengdaHan/AutoAD)][[Website](https://www.robots.ox.ac.uk/~vgg/research/autoad/)][[Website](https://www.robots.ox.ac.uk/~vgg/research/autoad/)]
    * **AutoAD-II**: "AutoAD II: The Sequel - Who, When, and What in Movie Audio Description", ICCV, 2023 (*Oxford*). [[Paper](https://arxiv.org/abs/2310.06838)][[PyTorch](https://github.com/TengdaHan/AutoAD)][[Website](https://www.robots.ox.ac.uk/~vgg/research/autoad/)]
* Sound Localization:
    * **TURN**: "Towards Effective Multi-Modal Interchanges in Zero-Resource Sounding Object Localization", NeurIPS, 2022 (*Zhejiang University*). [[Paper](https://openreview.net/forum?id=rQAJmrLmGC6)][[PyTorch (in construction)](https://github.com/AwalkZY/TURN)]
    * **AVGN**: "Audio-Visual Grouping Network for Sound Localization from Mixtures", CVPR, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2303.17056)][[PyTorch](https://github.com/stoneMo/AVGN)]
    * **?**: "Sound Source Localization is All about Cross-Modal Alignment", ICCV, 2023 (*KAIST*). [[Paper](https://arxiv.org/abs/2309.10724)]
* Sentiment Analysis:
    * **CubeMLP**: "CubeMLP: A MLP-based Model for Multimodal Sentiment Analysis and Depression Estimation", ACMMM, 2022 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2207.14087)]
    * **MCMulT**: "Multi-scale Cooperative Multimodal Transformers for Multimodal Sentiment Analysis in Videos", arXiv, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2206.07981)]
* Entity Recognition:
    * **FMIT**: "Flat Multi-modal Interaction Transformer for Named Entity Recognition", International Conference on Computational Linguistics (COLING), 2022 (*South China University of Technology*). [[Paper](https://arxiv.org/abs/2208.11039)]
    * **OVEN**: "Open-domain Visual Entity Recognition: Towards Recognizing Millions of Wikipedia Entities", ICCV, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2302.11154)][[PyTorch](https://github.com/edchengg/oven_eval)][[Website](https://open-vision-language.github.io/oven/)]
* Localization via Embodied Dialog:
    * **LED-Bert**: "Transformer-based Localization from Embodied Dialog with Large-scale Pre-training", arXiv, 2022 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2210.04864)]
* Object Captioning:
    * **GRiT**: "GRiT: A Generative Region-to-text Transformer for Object Understanding", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2212.00280)][[PyTorch](https://github.com/JialianW/GRiT)]
* Conversation:
    * **VisProg**: "Visual Programming: Compositional visual reasoning without training", CVPR, 2023 (*AI2*). [[Paper](https://arxiv.org/abs/2211.11559)][[PyTorch](https://github.com/allenai/visprog)][[Website](https://prior.allenai.org/projects/visprog)]
    * **ViperGPT**: "ViperGPT: Visual Inference via Python Execution for Reasoning", ICCV, 2023 (*Columbia*). [[Paper](https://arxiv.org/abs/2303.08128)][[PyTorch](https://github.com/cvlab-columbia/viper)][[Website](https://viper.cs.columbia.edu/)]
    * **LaVIN**: "Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models", NeurIPS, 2023 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2305.15023)][[PyTorch](https://github.com/luogen1996/LaVIN)][[Website](https://luogen1996.github.io/lavin/)]
    * **LLaVA**: "Visual Instruction Tuning", NeurIPS, 2023 (*UW-Madison*). [[Paper](https://arxiv.org/abs/2304.08485)][[PyTorch](https://github.com/haotian-liu/LLaVA)][[Website](https://llava-vl.github.io/)]
    * **LAMM**: "LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark", NeurIPS (Datasets and Benchmarks), 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2306.06687)][[PyTorch](https://github.com/OpenGVLab/LAMM)][[Website](https://openlamm.github.io/)]
    * **EmbodiedGPT**: "EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought", NeurIPS, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2305.15021)][[PyTorch (in construction)](https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch)][[Website](https://embodiedgpt.github.io/)]
    * **InstructBLIP**: "InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning", NeurIPS, 2023 (*Salesforce*). [[Paper](https://arxiv.org/abs/2305.06500)][[PyTorch](https://github.com/salesforce/LAVIS/tree/main/projects/instructblip)]
    * **AmadeusGPT**: "AmadeusGPT: a natural language interface for interactive animal behavioral analysis", NeurIPS, 2023 (*EPFL*). [[Paper](https://arxiv.org/abs/2307.04858)][[PyTorch](https://github.com/AdaptiveMotorControlLab/AmadeusGPT)][[Website](https://www.mackenziemathislab.org/amadeusgpt)]
    * **Visual-ChatGPT**: "Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2303.04671)]
    * **MM-REACT**: "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2303.11381)][[Code](https://github.com/microsoft/MM-REACT)][[Website](https://multimodal-react.github.io/)]
    * **Chameleon**: "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models", arXiv, 2023 (*UCLA + Microsoft*). [[Paper](https://arxiv.org/abs/2304.09842)][[PyTorch](https://github.com/lupantech/chameleon-llm)][[Website](https://chameleon-llm.github.io/)]
    * **MiniGPT-4**: "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models", arXiv, 2023 (*KAUST*). [[Paper](https://arxiv.org/abs/2304.10592)][[PyTorch](https://github.com/Vision-CAIR/MiniGPT-4)][[Website](https://minigpt-4.github.io/)]
    * **LLaMA-Adapter**: "LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2303.16199)][[PyTorch](https://github.com/ZrrSkywalker/LLaMA-Adapter)]
    * **LLaMA-Adapter-V2**: "LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2304.15010)][[PyTorch](https://github.com/ZrrSkywalker/LLaMA-Adapter)]
    * **Otter**: "Otter: A Multi-Modal Model with In-Context Instruction Tuning", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2305.03726)][[PyTorch](https://github.com/Luodian/Otter)]
    * **LMEye**: "LMEye: An Interactive Perception Network for Large Language Models", arXiv, 2023 (*Meituan*). [[Paper](https://arxiv.org/abs/2305.03701)]
    * **MultiModal-GPT**: "MultiModal-GPT: A Vision and Language Model for Dialogue with Humans", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2305.04790)][[PyTorch](https://github.com/open-mmlab/Multimodal-GPT)]
    * **InternChat**: "InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2305.05662)][[PyTorch](https://github.com/OpenGVLab/InternGPT)]
    * **ArtGPT-4**: "ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4", arXiv, 2023 (*Anhui Polytechnic University*). [[Paper](https://arxiv.org/abs/2305.07490)][[PyTorch](https://github.com/DLYuanGod/ArtGPT-4)]
    * **PandaGPT**: "PandaGPT: One Model To Instruction-Follow Them All", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2305.16355)][[PyTorch](https://github.com/yxuansu/PandaGPT)][[Website](https://panda-gpt.github.io/)]
    * **MIMIC-IT**: "MIMIC-IT: Multi-Modal In-Context Instruction Tuning", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2306.05425)][[PyTorch](https://github.com/Luodian/otter)][[Website](https://otter-ntu.github.io/)]
    * **?**: "Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models", arXiv, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2306.08641)]
    * **AssistGPT**: "AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2306.08640)][[Code (in construction)](https://github.com/showlab/assistgpt)][[Website](https://showlab.github.io/assistgpt/)]
    * **Macaw-LLM**: "Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2306.09093)][[PyTorch](https://github.com/lyuchenyang/Macaw-LLM)]
    * **Shikra**: "Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic", arXiv, 2023 (*SenseTime*). [[Paper](https://arxiv.org/abs/2306.15195)][[Code (in construction)](https://github.com/shikras/shikra)]
    * **LLaVAR**: "LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding", arXiv, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2306.17107)][[PyTorch](https://github.com/SALT-NLP/LLaVAR)][[Website](https://llavar.github.io/)]
    * **Polite-Flamingo**: "Visual Instruction Tuning with Polite Flamingo", arXiv, 2023 (*Xiaobing.AI*). [[Paper](https://arxiv.org/abs/2307.01003)]
    * **Lynx**: "What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2307.02469)][[Website](https://lynx-llm.github.io/)]
    * **GPT4RoI**: "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2307.03601)][[PyTorch](https://github.com/jshilong/GPT4RoI)]
    * **SVIT**: "SVIT: Scaling up Visual Instruction Tuning", arXiv, 2023 (*BAAI*). [[Paper](https://arxiv.org/abs/2307.04087)]
    * **ChatSpot**: "ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning", arXiv, 2023 (*Megvii*). [[Paper](https://arxiv.org/abs/2307.09474)][[Demo](https://chatspot.streamlit.app/)]
    * **?**: "How Good is Google Bard's Visual Understanding? An Empirical Study on Open Challenges", arXiv, 2023 (*ETHZ*). [[Paper](https://arxiv.org/abs/2307.15016)][[GitHub (in construction)](https://github.com/htqin/GoogleBard-VisUnderstand)]
    * **?**: "Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2308.00675)]
    * **MM-Vet**: "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2308.02490)][[Code](https://github.com/yuweihao/MM-Vet)]
    * **StableLLaVA**: "StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2308.10253)][[Code (in construction)](https://github.com/icoz69/StableLLAVA)]
    * **PVIT**: "Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2308.13437)]
    * **PointLLM**: "PointLLM: Empowering Large Language Models to Understand Point Clouds", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2308.16911)][[Code (in construction)](https://github.com/OpenRobotLab/PointLLM)][[Website](https://runsenxu.com/projects/PointLLM/)]
    * **Point-Bind**: "Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2309.00615)][[PyTorch](https://github.com/ZiyuGuo99/Point-Bind_Point-LLM)]
    * **ImageBind-LLM**: "ImageBind-LLM: Multi-modality Instruction Tuning", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2309.03905)]
    * **?**: "An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2309.09958)][[GitHub](https://github.com/haotian-liu/LLaVA/blob/main/docs/MODEL_ZOO.md)]
    * **InternLM-XComposer**: "InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2309.15112)][[PyTorch](https://github.com/InternLM/InternLM-XComposer)]
    * **LLaVA-RLHF**: "Aligning Large Multimodal Models with Factually Augmented RLHF", arXiv, 2023 (*Berkeley + CMU + UIUC*). [[Paper](https://arxiv.org/abs/2309.14525)][[Code (in construction)](https://github.com/llava-rlhf/LLaVA-RLHF)][[Website](https://llava-rlhf.github.io/)]
    * **Muffin**: "Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2310.00653)]
    * **Pink**: "Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs", arXiv, 2023 (*Ant*). [[Paper](https://arxiv.org/abs/2310.00582)][[Code (in construction)](https://github.com/SY-Xuan/Pink)]
    * **LLaVA-1.5**: "Improved Baselines with Visual Instruction Tuning", arXiv, 2023 (*UW Madison*). [[Paper](https://arxiv.org/abs/2310.03744)][[PyTorch](https://github.com/haotian-liu/LLaVA)][[Website](https://llava-vl.github.io/)]
    * **MiniGPT-5**: "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens", arXiv, 2023 (*UC Santa Cruz*). [[Paper](https://arxiv.org/abs/2310.02239)][[PyTorch](https://github.com/eric-ai-lab/MiniGPT-5)]
    * **Ferret**: "Ferret: Refer and Ground Anything Anywhere at Any Granularity", arXiv, 2023 (*Apple*). [[Paper](https://arxiv.org/abs/2310.07704)][[Code (in construction)](https://github.com/apple/ml-ferret)]
    * **MiniGPT-v2**: "MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2310.09478)][[PyTorch](https://github.com/Vision-CAIR/MiniGPT-4)][[Website](https://minigpt-v2.github.io/)]
    * **Woodpecker**: "Woodpecker: Hallucination Correction for Multimodal Large Language Models", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2310.16045)][[PyTorch](https://github.com/BradyFU/Woodpecker)]
    * **LLaVA-Interactive**: "LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2311.00571)][[PyTorch](https://github.com/LLaVA-VL/LLaVA-Interactive-Demo)][[Website](https://llava-vl.github.io/llava-interactive/)]
    * **NExT-Chat**: "NExT-Chat: An LMM for Chat, Detection and Segmentation", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2311.04498)][[Code (in construction)](https://github.com/NExT-ChatV/NExT-Chat)][[Website](https://next-chatv.github.io/)]
    * **mPLUG-Owl**: "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2304.14178)][[PyTorch](https://github.com/X-PLUG/mPLUG-Owl)]
    * **mPLUG-Owl2**: "mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2311.04257)][[PyTorch](https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl2)]
    * **LLaVA-Plus**: "LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2311.05437)][[PyTorch](https://github.com/LLaVA-VL/LLaVA-Plus-Codebase)][[Website](https://llava-vl.github.io/llava-plus/)]
    * **u-LLaVA**: "u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model", arXiv, 2023 (*OPPO*). [[Paper](https://arxiv.org/abs/2311.05348)]
    * **LVIS-Instruct4V**: "To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning", arXiv, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2311.07574)][[GitHub](https://github.com/X2FD/LVIS-INSTRUCT4V)]
    * **InfMLLM**: "InfMLLM: A Unified Framework for Visual-Language Tasks", arXiv, 2023 (*?*). [[Paper](https://arxiv.org/abs/2311.06791)][[PyTorch](https://github.com/mightyzau/InfMLLM)]
    * **Q-Instruct**: "Q-Instruct: Improving Low-level Visual Abilities for Multi-modality Foundation Models", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2311.06783)][[Website](https://q-future.github.io/Q-Instruct/)]
    * **DRESS**: "DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback", arXiv, 2023 (*SRI*). [[Paper](https://arxiv.org/abs/2311.10081)][[Dataset](https://huggingface.co/datasets/YangyiYY/LVLM_NLF)]
    * **LION**: "LION: Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge", arXiv, 2023 (*Harbin Institute of Technology*). [[Paper](https://arxiv.org/abs/2311.11860)][[Code (in construction)](https://github.com/rshaojimmy/JiuTian)][[Website](https://rshaojimmy.github.io/Projects/JiuTian-LION)]
    * **VCD**: "Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2311.16922)][[PyTorch](https://github.com/DAMO-NLP-SG/VCD)]
    * **OPERA**: "OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2311.17911)][[PyTorch](https://github.com/shikiw/OPERA)]
    * **CG-VLM**: "Contrastive Vision-Language Alignment Makes Efficient Instruction Learner", arXiv, 2023 (*South China University of Technology*). [[Paper](https://arxiv.org/abs/2311.17945)][[Code (in construction)](https://github.com/lizhaoliu-Lec/CG-VLM)]
    * **X-InstructBLIP**: "X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning", arXiv, 2023 (*Salesforce*). [[Paper](https://arxiv.org/abs/2311.18799)]
    * **ViP-LLaVA**: "Making Large Multimodal Models Understand Arbitrary Visual Prompts", arXiv, 2023 (*Cruise*). [[Paper](https://arxiv.org/abs/2312.00784)][[PyTorch](https://github.com/mu-cai/vip-llava)][[Website](https://vip-llava.github.io/)]
    * **Prompt-Highlighter**: "Prompt Highlighter: Interactive Control for Multi-Modal LLMs", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2312.04302)][[PyTorch](https://github.com/dvlab-research/Prompt-Highlighter/)][[Website](https://julianjuaner.github.io/projects/PromptHighlighter/)]
    * **Honeybee**: "Honeybee: Locality-enhanced Projector for Multimodal LLM", arXiv, 2023 (*Kakao*). [[Paper](https://arxiv.org/abs/2312.06742)][[Code (in construction)](https://github.com/kakaobrain/honeybee)]
    * **Osprey**: "Osprey: Pixel Understanding with Visual Instruction Tuning", arXiv, 2023 (*Zhejiang*). [[Paper](https://arxiv.org/abs/2312.10032)][[PyTorch](https://github.com/CircleRadon/Osprey)]
    * **Gemini**: "Gemini: A Family of Highly Capable Multimodal Models", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2312.11805)]
    * **V<sup>\*</sup>**: "V<sup>\*</sup>: Guided Visual Search as a Core Mechanism in Multimodal LLMs", arXiv, 2023 (*NYU*). [[Paper](https://arxiv.org/abs/2312.14135)][[PyTorch](https://github.com/penghao-wu/vstar)][[Website](https://vstar-seal.github.io/)]
    * **?**: "Gemini vs GPT-4V: A Preliminary Comparison and Combination of Vision-Language Models Through Qualitative Cases", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2312.15011)][[GitHub](https://github.com/Qi-Zhangyang/Gemini-vs-GPT4V)]
    * **TinyGPT-V**: "TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones", arXiv, 2023 (*Anhui Polytechnic University*). [[Paper](https://arxiv.org/abs/2312.16862)][[PyTorch](https://github.com/DLYuanGod/TinyGPT-V)]
    * **SNIFFER**: "SNIFFER: Multimodal Large Language Model for Explainable Out-of-Context Misinformation Detection", CVPR, 2024 (*NUS*). [[Paper](https://arxiv.org/abs/2403.03170)][[Code (in construction)](https://github.com/MischaQI/Sniffer)][[Website](https://pengqi.site/Sniffer/)]
    * **ChartAssisstant**: "ChartAssisstant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning", arXiv, 2024 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2401.02384)]
    * **LLaVA-**: "LLaVA-: Efficient Multi-Modal Assistant with Small Language Model", arXiv, 2024 (*Midea Group, China*). [[Paper](https://arxiv.org/abs/2401.02330)][[Code (in construction)](https://github.com/zhuyiche/llava-phi)]
    * **CaMML**: "CaMML: Context-Aware Multimodal Learner for Large Models", arXiv, 2024 (*Amazon*). [[Paper](https://arxiv.org/abs/2401.03149)]
    * **InternLM-XComposer2**: "InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model", arXiv, 2024 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2401.16420)][[PyTorch](https://github.com/InternLM/InternLM-XComposer)]
    * **MARINE**: "Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance", arXiv, 2024 (*UCLA*). [[Paper](https://arxiv.org/abs/2402.08680)]
    * **Prismatic-VLM**: "Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models", arXiv, 2024 (*Toyota*). [[Paper](https://arxiv.org/abs/2402.07865)][[PyTorch](https://github.com/TRI-ML/prismatic-vlms)]
    * **SPHINX-X**: "SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models", arXiv, 2024 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2402.05935)][[PyTorch](https://github.com/Alpha-VLLM/LLaMA2-Accessory)]
    * **ChartVLM**: "ChartX & ChartVLM: A Versatile Benchmark and Foundation Model for Complicated Chart Reasoning", arXiv, 2024 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2402.12185)][[PyTorch](https://github.com/UniModal4Reasoning/ChartVLM)]
    * **Vision-Flan**: "Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning", arXiv, 2024 (*Virginia Tech*). [[Paper](https://arxiv.org/abs/2402.11690)]
    * **CoLLaVO**: "CoLLaVO: Crayon Large Language and Vision mOdel", arXiv, 2024 (*KAIST*). [[Paper](https://arxiv.org/abs/2402.11248)]
    * **LLaVA-HR**: "Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models", arXiv, 2024 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2403.03003)][[PyTorch](https://github.com/luogen1996/LLaVA-HR)]
    * **InfiMM-HD**: "InfiMM-HD: A Leap Forward in High-Resolution Multimodal Understanding", arXiv, 2024 (*CAS*). [[Paper](https://arxiv.org/abs/2403.01487)][[PyTorch](https://github.com/InfiMM/infimm-hd/)]
    * **DeepSeek-VL**: "DeepSeek-VL: Towards Real-World Vision-Language Understanding", arXiv, 2024 (*DeepSeek, China*). [[Paper](https://arxiv.org/abs/2403.05525)][[PyTorch](https://github.com/deepseek-ai/DeepSeek-VL)]
    * **Gemini-1.5**: "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context", arXiv, 2024 (*Google*). [[Paper](https://arxiv.org/abs/2403.05530)]
    * **FastV**: "An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models", arXiv, 2024 (*Peking*). [[Paper](https://arxiv.org/abs/2403.06764)][[PyTorch (in construction)](https://github.com/pkunlp-icler/FastV)]
    * **MM1**: "MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training", arXiv, 2024 (*Apple*). [[Paper](https://arxiv.org/abs/2403.09611)]
* Conversation (Video):
    * **Video-ChatCaptioner**: "Video ChatCaptioner: Towards the Enriched Spatiotemporal Descriptions", arXiv, 2023 (*KAUST*). [[Paper](https://arxiv.org/abs/2304.04227)][[PyTorch](https://github.com/Vision-CAIR/ChatCaptioner)]
    * **ChatVideo**: "ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System", arXiv, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2304.14407)][[Website](https://www.wangjunke.info/ChatVideo/)]
    * **VideoChat**: "VideoChat: Chat-Centric Video Understanding", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2305.06355)][[PyTorch](https://github.com/OpenGVLab/Ask-Anything)]
    * **Video-LLaMA**: "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2306.02858)][[PyTorch](https://github.com/DAMO-NLP-SG/Video-LLaMA)]
    * **Video-ChatGPT**: "Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models", arXiv, 2023 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2306.05424)][[PyTorch](https://github.com/mbzuai-oryx/Video-ChatGPT)]
    * **MovieChat**: "MovieChat: From Dense Token to Sparse Memory for Long Video Understanding", arXiv, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2307.16449)][[PyTorch](https://github.com/rese1f/MovieChat)][[Website](https://rese1f.github.io/MovieChat/)]
    * **AntGPT**: "AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?", arXiv, 2023 (*Brown*). [[Paper](https://arxiv.org/abs/2307.16368)][[Website](https://brown-palm.github.io/AntGPT/)]
    * **Video-LLaVA**: "Video-LLaVA: Learning United Visual Representation by Alignment Before Projection", arXiv, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2311.10122)][[PyTorch](https://github.com/PKU-YuanGroup/Video-LLaVA)]
    * **PG-Video-LLaVA**: "PG-Video-LLaVA: Pixel Grounding Large Video-Language Models", arXiv, 2023 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2311.13435)][[Code (in construction)](https://github.com/mbzuai-oryx/Video-LLaVA)]
    * **MVBench**: "MVBench: A Comprehensive Multi-modal Video Understanding Benchmark", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2311.17005)][[PyTorch](https://github.com/OpenGVLab/Ask-Anything)]
    * **Valley**: "Valley: Video Assistant with Large Language model Enhanced abilitY", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2306.07207)]
    * **GPT4Video**: "GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2311.16511)][[Code (in construction)](https://github.com/gpt4video/GPT4Video)][[Website](https://gpt4video.github.io/)]
    * **Merlin**: "Merlin: Empowering Multimodal LLMs with Foresight Minds", arXiv, 2023 (*Huazhong University of Science and Technology*). [[Paper](https://arxiv.org/abs/2312.00589)]
    * **TimeChat**: "TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding", arXiv, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2312.02051)][[Code (in construction)](https://github.com/RenShuhuai-Andy/TimeChat)]
    * **VaQuitA**: "VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding", arXiv, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2312.02310)]
    * **?**: "Audio-Visual LLM for Video Understanding", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2312.06720)]
    * **Mementos**: "Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences", arXiv, 2024 (*Maryland*). [[Paper](https://arxiv.org/abs/2401.10529)][[GitHub](https://github.com/umd-huang-lab/Mementos)]
    * **LVChat**: "LVCHAT: Facilitating Long Video Comprehension", arXiv, 2024 (*UCSD*). [[Paper](https://arxiv.org/abs/2402.12079)][[PyTorch](https://github.com/wangyu-ustc/LVChat)]
    * **Momentor**: "Momentor: Advancing Video Large Language Model with Fine-Grained Temporal Reasoning", arXiv, 2024 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2402.11435)][[Code (in construction)](https://github.com/DCDmllm/Momentor)]
    * **IVA**: "LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs", arXiv, 2024 (*Harbin Institute of Technology*). [[Paper](https://arxiv.org/abs/2402.13546)]
    * **VLM-RLAIF**: "Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback", arXiv, 2024 (*Yonsei*). [[Paper](https://arxiv.org/abs/2402.03746)]
    * **Video-LaVIT**: "Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization", arXiv, 2024 (*Kuaishou*). [[Paper](https://arxiv.org/abs/2402.03161)][[PyTorch](https://github.com/jy0205/LaVIT)][[Website](https://video-lavit.github.io/)]
    * **MovieLLM**: "MovieLLM: Enhancing Long Video Understanding with AI-Generated Movies", arXiv, 2024 (*Tencent*). [[Paper](https://arxiv.org/abs/2403.01422)][[PyTorch](https://github.com/Deaddawn/MovieLLM-code)][[Website](https://deaddawn.github.io/MovieLLM/)]
* Conversation (3D):
    * **3D-LLM**: "3D-LLM: Injecting the 3D World into Large Language Models", NeurIPS, 2023 (*UCLA*). [[Paper](https://arxiv.org/abs/2307.12981)][[PyTorch](https://github.com/UMass-Foundation-Model/3D-LLM)][[Website](https://vis-www.cs.umass.edu/3dllm/)]
    * **Chat-3D**: "Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes", arXiv, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2308.08769)][[PyTorch](https://github.com/Chat-3D/Chat-3D)][[Website](https://chat-3d.github.io/)]
    * **Chat-3D-v2**: "Chat-3D v2: Bridging 3D Scene and Large Language Models with Object Identifiers", arXiv, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2312.08168)][[Code (in construction)](https://github.com/Chat-3D/Chat-3D-v2)]
    * **LiDAR-LLM**: "LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding", arXiv, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2312.14074)][[Code (in construction)](https://github.com/Yangsenqiao/LiDAR-LLM)][[Website](https://sites.google.com/view/lidar-llm)]
    * **Uni3D-LLM**: "Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models", arXiv, 2024 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2402.03327)]
    * **ShapeLLM**: "ShapeLLM: Universal 3D Object Understanding for Embodied Interaction", arXiv, 2024 (*Megvii*). [[Paper](https://arxiv.org/abs/2402.17766)][[Code (in construction)](https://github.com/qizekun/ShapeLLM)][[Website](https://qizekun.github.io/shapellm/)]
* Conversation (Multi):
    * **AnyMAL**: "AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2309.16058)]
    * **OneLLM**: "OneLLM: One Framework to Align All Modalities with Language", arXiv, 2023 (*CUHK + Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2312.03700)][[PyTorch](https://github.com/csuhan/OneLLM)]
    * **CREMA**: "CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion", arXiv, 2024 (*UNC*). [[Paper](https://arxiv.org/abs/2402.05889)][[PyTorch](https://github.com/Yui010206/CREMA)][[Website](https://crema-videollm.github.io/)]
    * **AnyGPT**: "AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling", arXiv, 2024 (*Fudan*). [[Paper](https://arxiv.org/abs/2402.12226)][[Code (in construction)](https://github.com/OpenMOSS/AnyGPT)][[Website](https://junzhan2000.github.io/AnyGPT.github.io/)]
* Visual Reasoning:
    * **BDC-Adapter**: "BDC-Adapter: Brownian Distance Covariance for Better Vision-Language Reasoning", BMVC, 2023 (*SUSTech*). [[Paper](https://arxiv.org/abs/2309.01256)]
    * **LSKD**: "Localized Symbolic Knowledge Distillation for Visual Commonsense Models", NeurIPS, 2023 (*UW*). [[Paper](https://arxiv.org/abs/2312.04837)][[Code (in construction)](https://github.com/jamespark3922/lskd)]
    * **RPT**: "Fine-Grained Regional Prompt Tuning for Visual Abductive Reasoning", arXiv, 2023 (*A\*STAR*). [[Paper](https://arxiv.org/abs/2303.10428)]
    * **LRR**: "Look, Remember and Reason: Visual Reasoning with Grounded Rationales", arXiv, 2023 (*Qualcomm*). [[Paper](https://arxiv.org/abs/2306.17778)]
    * **SDS-CLIP**: "Augmenting CLIP with Improved Visio-Linguistic Reasoning", arXiv, 2023 (*Maryland*). [[Paper](https://arxiv.org/abs/2307.09233)]
    * **?**: "Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models", arXiv, 2023 (*George Mason University*). [[Paper](https://arxiv.org/abs/2308.09778)]
    * **ViCor**: "ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models", arXiv, 2023 (*UC Santa Cruz*). [[Paper](https://arxiv.org/abs/2310.05872)]
    * **GENOME**: "GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and reusing ModulEs", arXiv, 2023 (*IBM*). [[Paper](https://arxiv.org/abs/2311.04901)][[PyTorch](https://github.com/UMass-Foundation-Model/genome)][[Website](https://vis-www.cs.umass.edu/genome/)]
    * **?**: "How Far Are We from Intelligent Visual Deductive Reasoning?", ICLRW, 2024 (*Apple*). [[Paper](https://arxiv.org/abs/2403.04732)]
    * **SpatialVLM**: "SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities", arXiv, 2024 (*DeepMind*). [[Paper](https://arxiv.org/abs/2401.12168)][[Website](https://spatial-vlm.github.io/)]
* Tracking:
    * **JointNLT**: "Joint Visual Grounding and Tracking with Natural Language Specification", CVPR, 2023 (*Harbin Institute of Technology*). [[Paper](https://arxiv.org/abs/2303.12027)][[PyTorch](https://github.com/lizhou-cs/JointNLT)]
    * **MMTrack**: "Towards Unified Token Learning for Vision-Language Tracking", arXiv, 2023 (*Guangxi Normal University*). [[Paper](https://arxiv.org/pdf/2308.14103.pdf)]
* Scene Graph:
    * **CaCao**: "Visually-Prompted Language Model for Fine-Grained Scene Graph Generation in an Open World", ICCV, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2303.13233)]
* Egocentric Video:
    * **MMG-Ego4D**: "MMG-Ego4D: Multi-Modal Generalization in Egocentric Action Recognition", CVPR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2305.07214)]
    * **EgoTV**: "EgoTV: Egocentric Task Verification from Natural Language Task Descriptions", ICCV, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2303.16975)]
* Dance Generation:
    * **TM2D**: "TM2D: Bimodality Driven 3D Dance Generation via Music-Text Integration", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2304.02419)][[Code (in construction)](https://github.com/Garfield-kh/TM2D)][[Website](https://garfield-kh.github.io/TM2D/)]
* Conceptual Understanding:
    * **?**: "Text-To-Concept (and Back) via Cross-Model Alignment", ICML, 2023 (*Maryland*). [[Paper](https://arxiv.org/abs/2305.06386)]
    * **EAC**: "Explain Any Concept: Segment Anything Meets Concept-Based Explanation", NeurIPS, 2023 (*HKUST*). [[Paper](https://arxiv.org/abs/2305.10289)]
    * **?**: "Probing Conceptual Understanding of Large Visual-Language Models", arXiv, 2023 (*UCF + SRI*). [[Paper](https://arxiv.org/abs/2304.03659)]
* Model Merging:
    * **VL-merging**: "An Empirical Study of Multimodal Model Merging", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2304.14933)][[PyTorch](https://github.com/ylsung/vl-merging)]
* Visual Word Sense Disambiguation (VWSD):
    * **CADG**: "Vision Meets Definitions: Unsupervised Visual Word Sense Disambiguation Incorporating Gloss Information", ACL, 2023 (*UMass*). [[Paper](https://arxiv.org/abs/2305.01788)]
* Object Hallucination:
    * **POPE**: "Evaluating Object Hallucination in Large Vision-Language Models", arXiv, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2305.10355)][[Code (in construction)](https://github.com/RUCAIBox/POPE)]
* Social Interaction:
    * **HIINT**: "HIINT: Historical, Intra- and Inter-personal Dynamics Modeling with Cross-person Memory Transformer", arXiv, 2023 (*MIT*). [[Paper](https://arxiv.org/abs/2305.12369)]
* Evaluation:
    * **HELM**: "Holistic Evaluation of Text-To-Image Models", NeurIPS (Datasets and Benchmarks), 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2311.04287)][[PyTorch](https://github.com/stanford-crfm/helm)][[Website](https://crfm.stanford.edu/heim/)]
    * **VisIT-Bench**: "VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use", NeurIPS (Datasets and Benchmarks), 2023 (*UW*). [[Paper](https://arxiv.org/abs/2308.06595)][[PyTorch](https://github.com/mlfoundations/VisIT-Bench/)][[Website](https://visit-bench.github.io/)]
    * **Perception-Test**: "Perception Test: A Diagnostic Benchmark for Multimodal Video Models", NeurIPS (Datasets and Benchmarks), 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2305.13786)][[GitHub](https://github.com/deepmind/perception_test)]
    * **VLM-Probing**: "Scalable Performance Analysis for Vision-Language Models", Joint Conference on Lexical and Computational Semantics (\*SEM), 2023 (*UMich*). [[Paper](https://arxiv.org/abs/2305.18786)][[PyTorch](https://github.com/MichiganNLP/Scalable-VLM-Probing)]
    * **VisualGPTScore**: "VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores", arXiv, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2306.01879)][[Code (in construction)](https://github.com/linzhiqiu/visual_gpt_score/)][[Website](https://linzhiqiu.github.io/papers/visual_gpt_score/)]
    * **LVLM-eHub**: "LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2306.09265)][[PyTorch (in construction)](https://github.com/OpenGVLab/Multi-Modality-Arena)]
    * **VisoGender**: "VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution", arXiv, 2023 (*Oxford*). [[Paper](https://arxiv.org/abs/2306.12424)][[PyTorch](https://github.com/oxai/visogender)]
    * **MME**: "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2306.13394)][[Code (in construction)](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)]
    * **MMBench**: "MMBench: Is Your Multi-modal Model an All-around Player?", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2307.06281)][[Website](https://opencompass.org.cn/mmbench)]
    * **Tiny-LVLM-eHub**: "Tiny LVLM-eHub: Early Multimodal Experiments with Bard", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2308.03729)][[PyTorch](https://github.com/OpenGVLab/Multi-Modality-Arena)][[Website](http://lvlm-ehub.opengvlab.com/)]
    * **MODE**: "An Examination of the Compositionality of Large Generative Vision-Language Models", arXiv, 2023 (*HKUST*). [[Paper](https://arxiv.org/abs/2308.10509)]
    * **TouchStone**: "TouchStone: Evaluating Vision-Language Models by Language Models", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2308.16890)]
    * **Q-Bench**: "Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2309.14181)]
    * **PCA-EVAL**: "Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond", arXiv, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2310.02071)][[Code (in construction)](https://github.com/pkunlp-icler/PCA-EVAL)]
    * **ReForm-Eval**: "ReForm-Eval: Evaluating Large Vision Language Models via Unified Re-Formulation of Task-Oriented Benchmarks", arXiv, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2310.02569)]
    * **?**: "Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models", arXiv, 2023 (*Zhejiang*). [[Paper](https://arxiv.org/abs/2310.13473)][[Code (in construction)](https://github.com/CoderJ-ONE/Giraffe-Bench)]
    * **HallusionBench**: "HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models", arXiv, 2023 (*Maryland*). [[Paper](https://arxiv.org/abs/2310.14566)][[Code (in construction)](https://github.com/tianyi-lab/HallusionBench)]
    * **?**: "GPT-4V(ision) as a Generalist Evaluator for Vision-Language Tasks", arXiv, 2023 (*UCSB*). [[Paper](https://arxiv.org/abs/2311.01361)]
    * **ChEF**: "ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2311.02692)][[PyTorch](https://github.com/OpenGVLab/LAMM)]
    * **ViLMA**: "ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models", arXiv, 2023 (*Ko University, Turkey*). [[Paper](https://arxiv.org/abs/2311.07022)][[Website](https://cyberiada.github.io/ViLMA/)]
    * **VLM-Eval**: "VLM-Eval: A General Evaluation on Video Large Language Models", arXiv, 2023 (*Megvii*). [[Paper](https://arxiv.org/abs/2311.11865)]
    * **Auto-Bench**: "Large Language Models as Automated Aligners for benchmarking Vision-Language Models", arXiv, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2311.14580)][[Website](https://jiyuanfeng.github.io/auto-bench.html)]
    * **AutoEval-Video**: "AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering", arXiv, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2311.14906)][[PyTorch](https://github.com/Xiuyuan-Chen/AutoEval-Video)]
    * **Video-Bench**: "Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models", arXiv, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2311.16103)][[PyTorch](https://github.com/PKU-YuanGroup/Video-Bench)]
    * **?**: "How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs", arXiv, 2023 (*UC Santa Cruz + UNC*). [[Paper](https://arxiv.org/abs/2311.16101)][[PyTorch](https://github.com/UCSC-VLAA/vllm-safety-benchmark)]
    * **VITATECS**: "VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models", arXiv, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2311.17404)][[PyTorch](https://github.com/lscpku/VITATECS)]
    * **SEED-Bench-2**: "SEED-Bench-2: Benchmarking Multimodal Large Language Models", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2311.17092)][[PyTorch](https://github.com/AILab-CVC/SEED-Bench)]
    * **VBench**: "VBench: Comprehensive Benchmark Suite for Video Generative Models", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2311.17982)][[PyTorch](https://github.com/Vchitect/VBench)][[Website](https://vchitect.github.io/VBench-project/)]
    * **MERLIM**: "Behind the Magic, MERLIM: Multi-modal Evaluation Benchmark for Large Image-Language Models", arXiv, 2023 (*KAUST*). [[Paper](https://arxiv.org/abs/2312.02219)]
    * **BenchLMM**: "BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2312.02896)][[PyTorch](https://github.com/AIFEG/BenchLMM)]
    * **M3DBench**: "M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts", arXiv, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2312.10763)][[Code (in construction)](https://github.com/OpenM3D/M3DBench/)][[Website](https://m3dbench.github.io/)]
    * **MM-SAP**: "MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception", arXiv, 2024 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2401.07529)][[Code (in construction)](https://github.com/YHWmz/MM-SAP)]
    * **MLLM-as-a-Judge**: "MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark", arXiv, 2024 (*Lehigh University, Pennsylvania*). [[Paper](https://arxiv.org/abs/2402.04788)][[GitHub](https://github.com/Dongping-Chen/MLLM-as-a-Judge)]
    * **TempCompass**: "TempCompass: Do Video LLMs Really Understand Videos?", arXiv, 2024 (*Peking*). [[Paper](https://arxiv.org/abs/2403.00476)][[Code (in construction)](https://github.com/llyx97/TempCompass)]
* Robustness:
    * **Hierarchy-CLIP**: "Improving Zero-shot Generalization and Robustness of Multi-modal Models", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2212.01758)][[JAX](https://github.com/gyhandy/Hierarchy-CLIP)][[Website](https://sites.google.com/usc.edu/hierarchy-clip/)]
    * **?**: "Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning", ICML, 2023 (*UCLA*). [[Paper](https://arxiv.org/abs/2304.03916)]
    * **SGA**: "Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models", ICCV, 2023 (*Southern University of Science and Technology*). [[Paper](https://arxiv.org/abs/2307.14061)]
    * **VLAttack**: "VLAttack: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models", NeurIPS, 2023 (*Pennsylvania State University*). [[Paper](https://arxiv.org/abs/2310.04655)]
    * **DAD**: "Distilling Out-of-Distribution Robustness from Vision-Language Foundation Models", NeurIPS, 2023 (*UIUC*). [[Paper](https://arxiv.org/abs/2311.01441)]
    * **AttackVLM**: "On Evaluating Adversarial Robustness of Large Vision-Language Models", NeurIPS, 2023 (*Singapore University of Technology and Design (SUTD)*). [[Paper](https://arxiv.org/abs/2305.16934)][[PyTorch](https://github.com/yunqing-me/AttackVLM)]
    * **RoCLIP**: "Robust Contrastive Language-Image Pre-training against Data Poisoning and Backdoor Attacks", NeurIPS, 2023 (*UCLA*). [[Paper](https://arxiv.org/abs/2303.06854)][[PyTorch](https://github.com/BigML-CS-UCLA/RoCLIP)]
    * **?**: "Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models", NeurIPS (Datasets and Benchmarks), 2023 (*LMU Munich*). [[Paper](https://arxiv.org/abs/2306.02080)][[PyTorch](https://github.com/adarobustness/adaptation_robustness)][[Website](https://adarobustness.github.io/)]
    * **OGEN**: "Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization", ICLR, 2024 (*Apple*). [[Paper](https://arxiv.org/abs/2401.15914)]
    * **CroPA**: "An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models", ICLR, 2024 (*Oxford*). [[Paper](https://arxiv.org/abs/2403.09766)][[PyTorch](https://github.com/Haochen-Luo/CroPA)]
    * **APT**: "One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models", CVPR, 2024 (*King's College London*). [[Paper](https://arxiv.org/abs/2403.01849)][[PyTorch](https://github.com/TreeLLi/APT)]
    * **Robust-CLIP**: "Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models", arXiv, 2024 (*University of Tubingen, Germany*). [[Paper](https://arxiv.org/abs/2402.12336)][[PyTorch](https://github.com/chs20/RobustVLM)]
* Compositional Reasoning:
    * **SugarCrepe**: "SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality", NeurIPS, 2023 (*AI2*). [[Paper](https://arxiv.org/abs/2306.14610)][[PyTorch](https://github.com/RAIVNLab/sugar-crepe)]
    * **DAC**: "Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models", NeurIPS, 2023 (*IBM*). [[Paper](https://arxiv.org/abs/2305.19595)]
    * **CoVLM**: "CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding", arXiv, 2023 (*UMass*). [[Paper](https://arxiv.org/abs/2311.03354)][[PyTorch](https://github.com/UMass-Foundation-Model/CoVLM)][[Website](https://vis-www.cs.umass.edu/CoVLM/)]
* Vocabulary-free Image Classification (VIC):
    * **CaSED**: "Vocabulary-free Image Classification", NeurIPS, 2023 (*University of Trento, Italy*). [[Paper](https://arxiv.org/abs/2306.00917)][[PyTorch](https://github.com/altndrr/vic)]
* Retrieval Augmentated Methods:
    * **?**: "Improving Image Recognition by Retrieving from Web-Scale Image-Text Data", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2304.05173)]
* NeRF:
    * **NeRDi**: "NeRDi: Single-View NeRF Synthesis with Language-Guided Diffusion as General Image Priors", CVPR, 2023 (*Waymo*). [[Paper](https://arxiv.org/abs/2212.03267)]
* Model Selection:
    * **LOVM**: "LOVM: Language-Only Vision Model Selection", NeurIPS, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2306.08893)]
    * **EMMS**: "Foundation Model is Efficient Multimodal Multitask Model Selector", NeurIPS, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2308.06262)][[PyTorch](https://github.com/OpenGVLab/Multitask-Model-Selector)]
* Multimodal Interaction:
    * **?**: "Learning Unseen Modality Interaction", arXiv, 2023 (*University of Amsterdam*). [[Paper](https://arxiv.org/abs/2306.12795)]
* Multimodal Translation:
    * **CLIPTrans**: "CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation", ICCV, 2023 (*Boston College*). [[Paper](https://arxiv.org/abs/2308.15226)][[PyTorch](https://github.com/devaansh100/CLIPTrans)]
* Noisy label detection:
    * **VDC**: "VDC: Versatile Data Cleanser for Detecting Dirty Samples via Visual-Linguistic Inconsistency", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2309.16211)]
* Model Compression:
    * **ECoFLaP**: "ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models", arXiv, 2023 (*UNC*). [[Paper](https://arxiv.org/abs/2310.02998)][[PyTorch](https://github.com/ylsung/ECoFLaP)][[Website](https://ecoflap.github.io/)]
    * **MoPE-CLIP**: "MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric", CVPR, 2024 (*CAS*). [[Paper](https://arxiv.org/abs/2403.07839)]
* Relation Extraction:
    * **OVRE**: "Open-Vocabulary Video Relation Extraction", AAAI, 2024 (*Fudan*). [[Paper](https://arxiv.org/abs/2312.15670)][[PyTorch (in construction)](https://github.com/Iriya99/OVRE)]
* Applications:
    * **MM-Navigator**: "GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2311.07562)][[Code (in construction)](https://github.com/zzxslp/MM-Navigator)]
    * **?**: "GPT-4V(ision) as A Social Media Analysis Engine", arXiv, 2023 (*University of Rochester*). [[Paper](https://arxiv.org/abs/2311.07547)][[GitHub](https://github.com/VIStA-H/GPT-4V_Social_Media)]
* X-Supervised:
    * **CAPro**: "CAPro: Webly Supervised Learning with Cross-Modality Aligned Prototypes", NeurIPS, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2310.09761)][[PyTorch](https://github.com/yuleiqin/capro)]
    * **MetaMAE**: "Modality-Agnostic Self-Supervised Learning with Meta-Learned Masked Auto-Encoder", NeurIPS, 2023 (*KAIST*). [[Paper](https://arxiv.org/abs/2310.16318)][[PyTorch](https://github.com/alinlab/MetaMAE)]

[[Back to Overview](#overview)]
