(back to [README.md](README.md) for other categories)
## Overview

- [Citation](#citation)
- [Other High-level Vision Tasks](#other-high-level-vision-tasks)
    - [Point Cloud / 3D](#point-cloud--3d)
    - [Pose Estimation](#pose-estimation)
    - [Tracking](#tracking)
    - [Re-ID](#re-id)
    - [Face](#face)
    - [Neural Architecture Search](#neural-architecture-search)
    - [Scene Graph](#scene-graph)
- [Transfer / X-Supervised / X-Shot / Continual Learning](#transfer--x-supervised--x-shot--continual-learning)
- [Low-level Vision Tasks](#low-level-vision-tasks)
    - [Image Restoration](#image-restoration)
    - [Video Restoration](#video-restoration)
    - [Inpainting / Completion / Outpainting](#inpainting--completion--outpainting)
    - [Image Generation](#image-generation)
    - [Video Generation](#video-generation)
    - [Transfer / Translation / Manipulation](#transfer--translation--manipulation)
    - [Other Low-Level Tasks](#other-low-level-tasks)
- [Reinforcement Learning](#reinforcement-learning)
    - [Navigation](#navigation)
    - [Other RL Tasks](#other-rl-tasks)
- [Medical](#medical)
    - [Medical Segmentation](#medical-segmentation)
    - [Medical Classification](#medical-classification)
    - [Medical Detection](#medical-detection)
    - [Medical Reconstruction](#medical-detection)
    - [Medical Low-Level Vision](#medical-low-level-vision)
    - [Medical Vision-Language](#medical-vision-language)
    - [Medical Others](#medical-others)
- [Other Tasks](#other-tasks)
- [Attention Mechanisms in Vision/NLP](#attention-mechanisms-in-visionnlp)
    - [Attention for Vision](#attention-for-vision)
    - [NLP](#attention-for-nlp)
    - [Both](#attention-for-both)
    - [Others](#attention-for-others)

---

## Citation
If you find this repository useful, please consider citing this list:
```
@misc{chen2022transformerpaperlist,
    title = {Ultimate awesome paper list: transformer and attention},
    author = {Chen, Min-Hung},
    journal = {GitHub repository},
    url = {https://github.com/cmhungsteve/Awesome-Transformer-Attention},
    year = {2022},
}
```

---

## Other High-level Vision Tasks
### Point Cloud / 3D
* **PCT**: "PCT: Point Cloud Transformer", arXiv, 2020 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2012.09688)][[Jittor](https://github.com/MenghaoGuo/PCT)][[PyTorch (uyzhang)](https://github.com/uyzhang/PCT_Pytorch)]
* **Point-Transformer**: "Point Transformer", arXiv, 2020 (*Ulm University*). [[Paper](https://arxiv.org/abs/2011.00931)]
* **NDT-Transformer**: "NDT-Transformer: Large-Scale 3D Point Cloud Localisation using the Normal Distribution Transform Representation", ICRA, 2021 (*University of Sheffield*). [[Paper](https://arxiv.org/abs/2103.12292)][[PyTorch](https://github.com/dachengxiaocheng/NDT-Transformer)]
* **P4Transformer**: "Point 4D Transformer Networks for Spatio-Temporal Modeling in Point Cloud Videos", CVPR, 2021 (*NUS*). [[Paper](https://openaccess.thecvf.com/content/CVPR2021/html/Fan_Point_4D_Transformer_Networks_for_Spatio-Temporal_Modeling_in_Point_Cloud_CVPR_2021_paper.html)]
* **SnowflakeNet**: "SnowflakeNet: Point Cloud Completion by Snowflake Point Deconvolution with Skip-Transformer", ICCV, 2021 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2108.04444)][[PyTorch](https://github.com/AllenXiangX/SnowflakeNet)]
* **PoinTr**: "PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers", ICCV, 2021 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2108.08839)][[PyTorch](https://github.com/yuxumin/PoinTr)]
* **Point-Transformer**: "Point Transformer", ICCV, 2021 (*Oxford + CUHK*). [[Paper](https://arxiv.org/abs/2012.09164)][[PyTorch (lucidrains)](https://github.com/lucidrains/point-transformer-pytorch)]
* **CT**: "Cloud Transformers: A Universal Approach To Point Cloud Processing Tasks", ICCV, 2021 (*Samsung*). [[Paper](https://arxiv.org/abs/2007.11679)]
* **3DVG-Transformer**: "3DVG-Transformer: Relation Modeling for Visual Grounding on Point Clouds", ICCV, 2021 (*Beihang University*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Zhao_3DVG-Transformer_Relation_Modeling_for_Visual_Grounding_on_Point_Clouds_ICCV_2021_paper.html)]
* **PPT-Net**: "Pyramid Point Cloud Transformer for Large-Scale Place Recognition", ICCV, 2021 (*Nanjing University of Science and Technology*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Hui_Pyramid_Point_Cloud_Transformer_for_Large-Scale_Place_Recognition_ICCV_2021_paper.html)]
* **?**: "Shape registration in the time of transformers", NeurIPS, 2021 (*Sapienza University of Rome*). [[Paper](https://arxiv.org/abs/2106.13679)]
* **YOGO**: "You Only Group Once: Efficient Point-Cloud Processing with Token Representation and Relation Inference Module", arXiv, 2021 (*Berkeley*). [[Paper](https://arxiv.org/abs/2103.09975)][[PyTorch](https://github.com/chenfengxu714/YOGO)]
* **DTNet**: "Dual Transformer for Point Cloud Analysis", arXiv, 2021 (*Southwest University*). [[Paper](https://arxiv.org/abs/2104.13044)]
* **MLMSPT**: "Point Cloud Learning with Transformer", arXiv, 2021 (*Southwest University*). [[Paper](https://arxiv.org/abs/2104.13636)]
* **PQ-Transformer**: "PQ-Transformer: Jointly Parsing 3D Objects and Layouts from Point Clouds", arXiv, 2021 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2109.05566)][[PyTorch](https://github.com/OPEN-AIR-SUN/PQ-Transformer)]
* **PST<sup>2</sup>**: "Spatial-Temporal Transformer for 3D Point Cloud Sequences", WACV, 2022 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2110.09783)]
* **SCTN**: "SCTN: Sparse Convolution-Transformer Network for Scene Flow Estimation", AAAI, 2022 (*KAUST*). [[Paper](https://arxiv.org/abs/2105.04447)]
* **AWT-Net**: "Adaptive Wavelet Transformer Network for 3D Shape Representation Learning", ICLR, 2022 (*NYU*). [[Paper](https://openreview.net/forum?id=5MLb3cLCJY)]
* **?**: "Deep Point Cloud Reconstruction", ICLR, 2022 (*KAIST*). [[Paper](https://arxiv.org/abs/2111.11704)]
* **PointMLP**: "Rethinking Network Design and Local Geometry in Point Cloud: A Simple Residual MLP Framework", ICLR, 2022 (*Northeastern*). [[Paper](https://arxiv.org/abs/2202.07123)][[PyTorch](https://github.com/ma-xu/pointMLP-pytorch)]
* **HiTPR**: "HiTPR: Hierarchical Transformer for Place Recognition in Point Cloud", ICRA, 2022 (*Nanjing University of Science and Technology*). [[Paper](https://arxiv.org/abs/2204.05481)]
* **FastPointTransformer**: "Fast Point Transformer", CVPR, 2022 (*POSTECH*). [[Paper](https://arxiv.org/abs/2112.04702)]
* **REGTR**: "REGTR: End-to-end Point Cloud Correspondences with Transformers", CVPR, 2022 (*NUS, Singapore*). [[Paper](https://arxiv.org/abs/2203.14517)][[PyTorch](https://github.com/yewzijian/RegTR)]
* **ShapeFormer**: "ShapeFormer: Transformer-based Shape Completion via Sparse Representation", CVPR, 2022 (*Shenzhen University*). [[Paper](https://arxiv.org/abs/2201.10326)][[Website](https://shapeformer.github.io/)]
* **PatchFormer**: "PatchFormer: An Efficient Point Transformer with Patch Attention", CVPR, 2022 (*Hangzhou Dianzi University*). [[Paper](https://arxiv.org/abs/2111.00207)]
* **?**: "An MIL-Derived Transformer for Weakly Supervised Point Cloud Segmentation", CVPR, 2022 (*NTU + NYCU*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Yang_An_MIL-Derived_Transformer_for_Weakly_Supervised_Point_Cloud_Segmentation_CVPR_2022_paper.html)][[Code (in construction)](https://github.com/jimmy15923/wspss_mil_transformer)]
* **Point-BERT**: "Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling", CVPR, 2022 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2111.14819)][[PyTorch](https://github.com/lulutang0608/Point-BERT)][[Website](https://point-bert.ivg-research.xyz/)]
* **GeoTransformer**: "Geometric Transformer for Fast and Robust Point Cloud Registration", CVPR, 2022 (*National University of Defense Technology, China*). [[Paper](https://arxiv.org/abs/2202.06688)][[PyTorch](https://github.com/qinzheng93/GeoTransformer)]
* **PointCLIP**: "PointCLIP: Point Cloud Understanding by CLIP", CVPR, 2022 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2112.02413)][[PyTorch](https://github.com/ZrrSkywalker/PointCLIP)]
* **?**: "3D Part Assembly Generation with Instance Encoded Transformer", IROS, 2022 (*Tongji University*). [[Paper](https://arxiv.org/abs/2207.01779)]
* **SeedFormer**: "SeedFormer: Patch Seeds based Point Cloud Completion with Upsample Transformer", ECCV, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2207.10315)][[PyTorch](https://github.com/hrzhou2/seedformer)]
* **MeshMAE**: "MeshMAE: Masked Autoencoders for 3D Mesh Data Analysis", ECCV, 2022 (*JD*). [[Paper](https://arxiv.org/abs/2207.10228)]
* **PPTr**: "Point Primitive Transformer for Long-Term 4D Point Cloud Video Understanding", ECCV, 2022 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2208.00281)]
* **Geodesic-Former**: "Geodesic-Former: a Geodesic-Guided Few-shot 3D Point Cloud Instance Segmenter", ECCV, 2022 (*VinAI Research, Vietnam*). [[Paper](https://arxiv.org/abs/2207.10859)]
* **LaplacianMesh-Transformer**: "Laplacian Mesh Transformer: Dual Attention and Topology Aware Network for 3D Mesh Classification and Segmentation", ECCV, 2022 (*CAS*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5047_ECCV_2022_paper.php)]
* **Point-MixSwap**: "Point MixSwap: Attentional Point Cloud Mixing via Swapping Matched Structural Divisions", ECCV, 2022 (*NYCU + NTU*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5484_ECCV_2022_paper.php)][[PyTorch](https://github.com/ardianumam/PointMixSwap)]
* **PointMixer**: "PointMixer: MLP-Mixer for Point Cloud Understanding", ECCV, 2022 (*KAIST*). [[Paper](https://arxiv.org/abs/2111.11187)]
* **Point-Transformer-V2**: "Point Transformer V2: Grouped Vector Attention and Partition-based Pooling", NeurIPS, 2022 (*HKU*). [[Paper](https://arxiv.org/abs/2210.05666)][[PyTorch (in construction)](https://github.com/Gofinge/PointTransformerV2)]
* **SPoVT**: "SPoVT: Semantic-Prototype Variational Transformer for Dense Point Cloud Semantic Completion", NeurIPS, 2022 (*NTU*). [[Paper](https://openreview.net/forum?id=JVoKzM_-lhz)][[PyTorch](https://github.com/haoyuhsu/spovt)][[Website](https://haoyuhsu.github.io/spovt/index.html)]
* **GSA**: "Geodesic Self-Attention for 3D Point Clouds", NeurIPS, 2022 (*East China Normal University*). [[Paper](https://openreview.net/forum?id=2ndfW2bw4mi)]
* **P2P**: "P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with Point-to-Pixel Prompting", NeurIPS, 2022 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2208.02812)][[PyTorch](https://github.com/wangzy22/P2P)][[Website](https://p2p.ivg-research.xyz/)]
* **3DTRL**: "Learning Viewpoint-Agnostic Visual Representations by Recovering Tokens in 3D Space", NeurIPS, 2022 (*Stony Brook*). [[Paper](https://arxiv.org/abs/2206.11895?context=cs.LG)][[PyTorch](https://github.com/elicassion/3DTRL)][[Website](https://elicassion.github.io/3dtrl/3dtrl.html)]
* **ShapeCrafter**: "ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model", NeurIPS, 2022 (*Brown*). [[Paper](https://arxiv.org/abs/2207.09446)]
* **XMFnet**: "Cross-modal Learning for Image-Guided Point Cloud Shape Completion", NeurIPS, 2022 (*Politecnico di Torino, Italy*). [[Paper](https://arxiv.org/abs/2209.09552)]
* **Point-M2AE**: "Point-M2AE: Multi-scale Masked Autoencoders for Hierarchical Point Cloud Pre-training", NeurIPS, 2022 (*CUHK*). [[Paper](https://arxiv.org/abs/2205.14401)][[PyTorch](https://github.com/ZrrSkywalker/Point-M2AE)]
* **LighTN**: "LighTN: Light-weight Transformer Network for Performance-overhead Tradeoff in Point Cloud Downsampling", arXiv, 2022 (*Beijing Jiaotong University*). [[Paper](https://arxiv.org/abs/2202.06263)]
* **PMP-Net++**: "PMP-Net++: Point Cloud Completion by Transformer-Enhanced Multi-step Point Moving Paths", arXiv, 2022 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2202.09507)]
* **SnowflakeNet**: "Snowflake Point Deconvolution for Point Cloud Completion and Generation with Skip-Transformer", arXiv, 2022 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2202.09367)][[PyTorch](https://github.com/AllenXiangX/SnowflakeNet)]
* **3DCTN**: "3DCTN: 3D Convolution-Transformer Network for Point Cloud Classification", arXiv, 2022 (*University of Waterloo, Canada*). [[Paper](https://arxiv.org/abs/2203.00828)]
* **VNT-Net**: "VNT-Net: Rotational Invariant Vector Neuron Transformers", arXiv, 2022 (*Ben-Gurion University of the Negev, Israel*). [[Paper](https://arxiv.org/abs/2205.09690)]
* **CompleteDT**: "CompleteDT: Point Cloud Completion with Dense Augment Inference Transformers", arXiv, 2022 (*Beijing Institute of Technology*). [[Paper](https://arxiv.org/abs/2205.14999)]
* **VN-Transformer**: "VN-Transformer: Rotation-Equivariant Attention for Vector Neurons", arXiv, 2022 (*Waymo*). [[Paper](https://arxiv.org/abs/2206.04176)]
* **Voxel-MAE**: "Masked Autoencoders for Self-Supervised Learning on Automotive Point Clouds", arXiv, 2022 (*Chalmers University of Technology, Sweden*). [[Paper](https://arxiv.org/abs/2207.00531)]
* **MAE3D**: "Masked Autoencoders in 3D Point Cloud Representation Learning", arXiv, 2022 (*Northwest A&F University, China*). [[Paper](https://arxiv.org/abs/2207.01545)]
* **Pix4Point**: "Pix4Point: Image Pretrained Transformers for 3D Point Cloud Understanding", arXiv, 2022 (*KAUST*). [[Paper](https://arxiv.org/abs/2208.12259)][[Code (in construction)](https://github.com/guochengqian/Pix4Point)]
* **MVP**: "Multiple View Performers for Shape Completion", arXiv, 2022 (*Columbia University*). [[Paper](https://arxiv.org/abs/2209.06291)]
* **Simple3D-Former**: "Can We Solve 3D Vision Tasks Starting from A 2D Vision Transformer?", arXiv, 2022 (*UT Austin*). [[Paper](https://arxiv.org/abs/2209.07026)][[PyTorch](https://github.com/Reimilia/Simple3D-Former)]
* **3DPCT**: "3DPCT: 3D Point Cloud Transformer with Dual Self-attention", arXiv, 2022 (*University of Waterloo, Canada*). [[Paper](https://arxiv.org/abs/2209.11255)]
* **PS-Former**: "Point Cloud Recognition with Position-to-Structure Attention Transformers", arXiv, 2022 (*UCSD*). [[Paper](https://arxiv.org/abs/2210.02030)]
* **LCPFormer**: "LCPFormer: Towards Effective 3D Point Cloud Analysis via Local Context Propagation in Transformers", arXiv, 2022 (*Aberystwyth University, UK*). [[Paper](https://arxiv.org/abs/2210.12755)]
* **R<sup>2</sup>-MLP**: "R<sup>2</sup>-MLP: Round-Roll MLP for Multi-View 3D Object Recognition", arXiv, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2211.11085)]
* **PVT3D**: "PVT3D: Point Voxel Transformers for Place Recognition from Sparse Lidar Scans", arXiv, 2022 (*TUM*). [[Paper](https://arxiv.org/abs/2211.12542)]
* **EPCL**: "Frozen CLIP Model is Efficient Point Cloud Backbone", arXiv, 2022 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2212.04098)]
* **CAT**: "Context-Aware Transformer for 3D Point Cloud Automatic Annotation", AAAI, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2303.14893)]
* **ACT**: "Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning?", ICLR, 2023 (*Megvii*). [[Paper](https://arxiv.org/abs/2212.08320)][[PyTorch](https://github.com/RunpeiDong/ACT)]
* **AnalogicalNets**: "Analogy-Forming Transformers for Few-Shot 3D Parsing", ICLR, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2304.14382)][[Website](https://analogicalnets.github.io/)]
* **ViPFormer**: "ViPFormer: Efficient Vision-and-Pointcloud Transformer for Unsupervised Pointcloud Understanding", ICRA, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2303.14376)][[PyTorch](https://github.com/auniquesun/ViPFormer)]
* **ProxyFormer**: "ProxyFormer: Proxy Alignment Assisted Point Cloud Completion with Missing Part Sensitive Transformer", CVPR, 2023 (*Nanjing University of Aeronautics and Astronautics*). [[Paper](https://arxiv.org/abs/2302.14435)][[PyTorch](https://github.com/I2-Multimedia-Lab/ProxyFormer)]
* **I2P-MAE.**: "Learning 3D Representations from 2D Pre-trained Models via Image-to-Point Masked Autoencoders", CVPR, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2212.06785)][[PyTorch](https://github.com/ZrrSkywalker/I2P-MAE)]
* **RoITr**: "Rotation-Invariant Transformer for Point Cloud Matching", CVPR, 2023 (*TUM*). [[Paper](https://arxiv.org/abs/2303.08231)]
* **SphereFormer**: "Spherical Transformer for LiDAR-based 3D Recognition", CVPR, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2303.12766)][[PyTorch](https://github.com/dvlab-research/SphereFormer)]
* **SPoTr**: "Self-positioning Point-based Transformer for Point Cloud Understanding", CVPR, 2023 (*Korea University*). [[Paper](https://arxiv.org/abs/2303.16450)][[PyTorch (in construction)](https://github.com/mlvlab/SPoTr)]
* **PointCMP**: "PointCMP: Contrastive Mask Prediction for Self-supervised Learning on Point Cloud Videos", CVPR, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2305.04075)]
* **GeoMAE**: "GeoMAE: Masked Geometric Target Prediction for Self-supervised Point Cloud Pre-Training", CVPR, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2305.08808)][[Code (in construction)](https://github.com/Tsinghua-MARS-Lab/GeoMAE)]
* **ULIP**: "ULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding", CVPR, 2023 (*Salesforce*). [[Paper](https://arxiv.org/abs/2212.05171)][[PyTorch](https://github.com/salesforce/ULIP)][[Website](https://tycho-xue.github.io/ULIP/)]
* **PointConvFormer**: "PointConvFormer: Revenge of the Point-based Convolution", CVPR, 2023 (*Apple*). [[Paper](https://arxiv.org/abs/2208.02879)]
* **AnchorFormer**: "AnchorFormer: Point Cloud Completion from Discriminative Nodes", CVPR, 2023 (*USTC*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Chen_AnchorFormer_Point_Cloud_Completion_From_Discriminative_Nodes_CVPR_2023_paper.html)][[PyTorch](https://github.com/chenzhik/AnchorFormer)]
* **FlatFormer**: "FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer", CVPR, 2023 (*MIT*). [[Paper](https://arxiv.org/abs/2301.08739)][[Website](https://flatformer.mit.edu/)]
* **PEAL**: "PEAL: Prior-Embedded Explicit Attention Learning for Low-Overlap Point Cloud Registration", CVPR, 2023 (*Hangzhou Dianzi University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Yu_PEAL_Prior-Embedded_Explicit_Attention_Learning_for_Low-Overlap_Point_Cloud_Registration_CVPR_2023_paper.html)]
* **APES**: "Attention-based Point Cloud Edge Sampling", CVPR, 2023 (*Karlsruhe Institute of Technology, Germany*). [[Paper](https://arxiv.org/abs/2302.14673)]
* **GD-MAE**: "GD-MAE: Generative Decoder for MAE Pre-training on LiDAR Point Clouds", CVPR, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2212.03010)][[PyTorch](https://github.com/Nightmare-n/GD-MAE)]
* **ShapeClipper**: "ShapeClipper: Scalable 3D Shape Learning from Single-View Images via Geometric and CLIP-based Consistency", CVPR, 2023 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2304.06247)][[Code (in construction)](https://github.com/zxhuang1698/ShapeClipper)][[Website](https://zixuanh.com/projects/shapeclipper.html)]
* **MSC**: "Masked Scene Contrast: A Scalable Framework for Unsupervised 3D Representation Learning", CVPR, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2303.14191)][[PyTorch](https://github.com/Pointcept/Pointcept)]
* **MSP**: "Self-supervised Pre-training with Masked Shape Prediction for 3D Scene Understanding", CVPR, 2023 (*MPI*). [[Paper](https://arxiv.org/abs/2305.05026)]
* **MM-3DScene**: "MM-3DScene: 3D Scene Understanding by Customizing Masked Modeling with Informative-Preserved Reconstruction and Self-Distilled Consistency", CVPR, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2212.09948)][[PyTorch](https://github.com/MingyeXu/mm-3dscene)][[Website](https://mingyexu.github.io/mm3dscene/)]
* **?**: "Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction", ICML, 2023 (*UT Austin*). [[Paper](https://arxiv.org/abs/2301.04791)]
* **ReCon**: "Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining", ICML, 2023 (*Megvii*). [[Paper](https://arxiv.org/abs/2302.02318)][[PyTorch](https://github.com/qizekun/ReCon)]
* **OctFormer**: "OctFormer: Octree-based Transformers for 3D Point Clouds", SIGGRAPH, 2023 (*Peking University*). [[Paper](https://arxiv.org/abs/2305.03045)][[Code (in construction)](https://github.com/octree-nn/octformer)][[Website](https://wang-ps.github.io/octformer)]
* **SVDFormer**: "SVDFormer: Complementing Point Cloud via Self-view Augmentation and Self-structure Dual-generator", ICCV, 2023 (*Nanjing University of Aeronautics and Astronautics*). [[Paper](https://arxiv.org/abs/2307.08492)][[PyTorch](https://github.com/czvvd/SVDFormer)]
* **TAP**: "Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models", ICCV, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2307.14971)][[PyTorch](https://github.com/wangzy22/TAP)]
* **MATE**: "MATE: Masked Autoencoders are Online 3D Test-Time Learners", ICCV, 2023 (*Graz University of Technology, Austria*). [[Paper](https://arxiv.org/abs/2211.11432)][[PyTorch](https://github.com/jmiemirza/MATE)]
* **DeFormer**: "DeFormer: Integrating Transformers with Deformable Models for 3D Shape Abstraction from a Single Image", ICCV, 2023 (*Rutgers*). [[Paper](https://arxiv.org/abs/2309.12594)]
* **RegFormer**: "RegFormer: An Efficient Projection-Aware Transformer Network for Large-Scale Point Cloud Registration", ICCV, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2303.12384)][[PyTorch](https://github.com/IRMVLab/RegFormer)]
* **PointCLIP-V2**: "PointCLIP V2: Adapting CLIP for Powerful 3D Open-world Learning", ICCV, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2211.11682)][[PyTorch](https://github.com/yangyangyang127/PointCLIP_V2)]
* **CLIP2Point**: "CLIP2Point: Transfer CLIP to Point Cloud Classification with Image-Depth Pre-training", ICCV, 2023 (*Harbin Institute of Technology*). [[Paper](https://arxiv.org/abs/2210.01055)][[PyTorch](https://github.com/tyhuang0428/CLIP2Point)]
* **IDPT**: "Instance-aware Dynamic Prompt Tuning for Pre-trained Point Cloud Models", ICCV, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2304.07221)][[PyTorch](https://github.com/zyh16143998882/IDPT)]
* **JM3D**: "Beyond First Impressions: Integrating Joint Multi-modal Cues for Comprehensive 3D Representation", ACMMM, 2023 (*NetEase, China*). [[Paper](https://arxiv.org/abs/2308.02982)][[PyTorch](https://github.com/Mr-Neko/JM3D)]
* **Bridge3D**: "Bridging the Domain Gap: Self-Supervised 3D Scene Understanding with Foundation Models", NeurIPS, 2023 (*Clemson*). [[Paper](https://arxiv.org/abs/2305.08776)][[Code (in construction)](https://github.com/Zhimin-C/Bridge3D)]
* **ConDaFormer**: "ConDaFormer: Disassembled Transformer with Local Structure Enhancement for 3D Point Cloud Understanding", NeurIPS, 2023 (*JD*). [[Paper](https://arxiv.org/abs/2312.11112)][[PyTorch](https://github.com/LHDuan/ConDaFormer)]
* **DiT-3D**: "DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation", NeurIPS, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2307.01831)][[PyTorch](https://github.com/DiT-3D/DiT-3D)][[Website](https://dit-3d.github.io/)]
* **OpenShape**: "OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding", NeurIPS, 2023 (*UCSD*). [[Paper](https://arxiv.org/abs/2305.10764)][[PyTorch](https://github.com/Colin97/OpenShape_code)][[Website](https://colin97.github.io/OpenShape/)]
* **PointGPT**: "PointGPT: Auto-regressively Generative Pre-training from Point Clouds", NeurIPS, 2023 (*Beijing Institute of Technology*). [[Paper](https://arxiv.org/abs/2305.11487)]
* **PIC**: "Explore In-Context Learning for 3D Point Cloud Understanding", NeurIPS, 2023 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2306.08659)][[PyTorch](https://github.com/fanglaosi/Point-In-Context)]
* **GeoTransformer**: "GeoTransformer: Fast and Robust Point Cloud Registration with Geometric Transformer", TPAMI, 2023 (*National University of Defense Technology, China*). [[Paper](https://arxiv.org/abs/2308.03768)][[PyTorch](https://github.com/qinzheng93/GeoTransformer)]
* **Text4Point**: "Joint Representation Learning for Text and 3D Point Cloud", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2301.07584)][[Code (in construction)](https://github.com/LeapLabTHU/Text4Point)]
* **FullFormer**: "FullFormer: Generating Shapes Inside Shapes", arXiv, 2023 (*University of Siegen, Germany*). [[Paper](https://arxiv.org/abs/2303.11235)]
* **Joint-MAE**: "Joint-MAE: 2D-3D Joint Masked Autoencoders for 3D Point Cloud Pre-training", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2302.14007)]
* **PointCAT**: "PointCAT: Cross-Attention Transformer for point cloud", arXiv, 2023 (*Nanjing University of Science and Technology*). [[Paper](https://arxiv.org/abs/2304.03012)][[PyTorch](https://github.com/xincheng-yang/PointCAT)]
* **MGT**: "Multi-scale Geometry-aware Transformer for 3D Point Cloud Classification", arXiv, 2023 (*TUM*). [[Paper](https://arxiv.org/abs/2304.05694)]
* **Swin3D**: "Swin3D: A Pretrained Transformer Backbone for 3D Indoor Scene Understanding", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2304.06906)]
* **ViewFormer**: "ViewFormer: View Set Attention for Multi-view 3D Shape Understanding", arXiv, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2305.00161)]
* **ULIP-2**: "ULIP-2: Towards Scalable Multimodal Pre-training For 3D Understanding", arXiv, 2023 (*Salesforce*). [[Paper](https://arxiv.org/abs/2305.08275)]
* **CDFormer**: "Collect-and-Distribute Transformer for 3D Point Cloud Analysis", arXiv, 2023 (*The University of Sydney*). [[Paper](https://arxiv.org/abs/2306.01257)][[PyTorch](https://github.com/haibo-qiu/CDFormer)]
* **PointCAM**: "Self-supervised adversarial masking for 3D point cloud representation learning", arXiv, 2023 (*Wrocław University of Science and Technology, Poland*). [[Paper](https://arxiv.org/abs/2307.05325)][[PyTorch](https://github.com/szacho/pointcam)]
* **PPT**: "Towards Large-scale 3D Representation Learning with Multi-dataset Point Prompt Training", arXiv, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2308.09718)][[PyTorch](https://github.com/Pointcept/Pointcept)]
* **Uni3D**: "Uni3D: Exploring Unified 3D Representation at Scale", arXiv, 2023 (*BAAI*). [[Paper](https://arxiv.org/abs/2310.06773)][[PyTorch](https://github.com/baaivision/Uni3D)]
* **JM3D**: "JM3D & JM3D-LLM: Elevating 3D Representation with Joint Multi-modal Cues", arXiv, 2023 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2310.09503)][[PyTorch](https://github.com/Mr-Neko/JM3D)]
* **PonderV2**: "PonderV2: Pave the Way for 3D Foundation Model with A Universal Pre-training Paradigm", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2310.08586)][[Code (in construction)](https://github.com/OpenGVLab/PonderV2)]
* **MeshGPT**: "MeshGPT: Generating Triangle Meshes with Decoder-Only Transformers", arXiv, 2023 (*TUM*). [[Paper](https://arxiv.org/abs/2311.15475)][[Website](https://nihalsid.github.io/mesh-gpt/)]
* **PTv3**: "Point Transformer V3: Simpler, Faster, Stronger", arXiv, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2312.10035)][[Code (in construction)](https://github.com/Pointcept/PointTransformerV3)]
* **3D-LFM**: "3D-LFM: Lifting Foundation Model", arXiv, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2312.11894)][[Code](https://github.com/mosamdabhi/3dlfm)][[Webite](https://3dlfm.github.io/)]
* **LAST-PCL**: "Language-Assisted 3D Scene Understanding", AAAI, 2024 (*Peking*). [[Paper](https://arxiv.org/abs/2312.11451)][[Code (in construction)](https://github.com/yanmin-wu/LAST-PCL)]
* **MM-Point**: "MM-Point: Multi-View Information-Enhanced Multi-Modal Self-Supervised 3D Point Cloud Understanding", AAAI, 2024 (*Southeast University, China*). [[Paper](https://arxiv.org/abs/2402.10002)]
* **PointMamba**: "PointMamba: A Simple State Space Model for Point Cloud Analysis", arXiv, 2024 (*Huazhong University of Science and Technology*). [[Paper](https://arxiv.org/abs/2402.10739)][[PyTorch](https://github.com/LMD0311/PointMamba)]

[[Back to Overview](#overview)]

### Pose Estimation
* Human-body: 
    * **HOT-Net**: "HOT-Net: Non-Autoregressive Transformer for 3D Hand-Object Pose Estimation", ACMMM. 2020 (*Kwai*). [[Paper](https://cse.buffalo.edu/~jmeng2/publications/hotnet_mm20)]
    * **TransPose**: "TransPose: Towards Explainable Human Pose Estimation by Transformer", arXiv, 2020 (*Southeast University*). [[Paper](https://arxiv.org/abs/2012.14214)][[PyTorch](https://github.com/yangsenius/TransPose)]
    * **PTF**: "Locally Aware Piecewise Transformation Fields for 3D Human Mesh Registration", CVPR, 2021 (*ETHZ*). [[Paper](https://arxiv.org/abs/2104.08160)][[Code (in construction)](https://github.com/taconite/PTF)][[Website](https://taconite.github.io/PTF/website/PTF.html)]
    * **METRO**: "End-to-End Human Pose and Mesh Reconstruction with Transformers", CVPR, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2012.09760)][[PyTorch](https://github.com/microsoft/MeshTransformer)] 
    * **PRTR**: "Pose Recognition with Cascade Transformers", CVPR, 2021 (*UCSD*). [[Paper](https://arxiv.org/abs/2104.06976)][[PyTorch](https://github.com/mlpc-ucsd/PRTR)]
    * **Mesh-Graphormer**: "Mesh Graphormer", ICCV, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2104.00272)][[PyTorch](https://github.com/microsoft/MeshGraphormer)]
    * **THUNDR**: "THUNDR: Transformer-based 3D HUmaN Reconstruction with Markers", ICCV, 2021 (*Google*). [[Paper](https://arxiv.org/abs/2106.09336)]
    * **PoseFormer**: "3D Human Pose Estimation with Spatial and Temporal Transformers", ICCV, 2021 (*UCF*). [[Paper](https://arxiv.org/abs/2103.10455)][[PyTorch](https://github.com/zczcwh/PoseFormer)]
    * **TransPose**: "TransPose: Keypoint Localization via Transformer", ICCV, 2021 (*Southeast University, China*). [[Paper](https://arxiv.org/abs/2012.14214)][[PyTorch](https://github.com/yangsenius/TransPose)]
    * **POTR**: "Pose Transformers (POTR): Human Motion Prediction With Non-Autoregressive Transformers", ICCVW, 2021 (*Idiap*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021W/SoMoF/html/Martinez-Gonzalez_Pose_Transformers_POTR_Human_Motion_Prediction_With_Non-Autoregressive_Transformers_ICCVW_2021_paper.html)]
    * **TransFusion**: "TransFusion: Cross-view Fusion with Transformer for 3D Human Pose Estimation", BMVC, 2021 (*UC Irvine*). [[Paper](https://arxiv.org/abs/2110.09554)][[PyTorch](https://github.com/HowieMa/TransFusion-Pose)]
    * **HRT**: "HRFormer: High-Resolution Transformer for Dense Prediction", NeurIPS, 2021 (*CAS*). [[Paper](https://arxiv.org/abs/2110.09408)][[PyTorch](https://github.com/HRNet/HRFormer)]
    * **POET**: "End-to-End Trainable Multi-Instance Pose Estimation with Transformers", arXiv, 2021 (*EPFL*). [[Paper](https://arxiv.org/abs/2103.12115)]
    * **Lifting-Transformer**: "Lifting Transformer for 3D Human Pose Estimation in Video", arXiv, 2021 (*Peking*). [[Paper](https://arxiv.org/abs/2103.14304)]
    * **TFPose**: "TFPose: Direct Human Pose Estimation with Transformers", arXiv, 2021 (*The University of Adelaide*). [[Paper](https://arxiv.org/abs/2103.15320)][[PyTorch](https://github.com/aim-uofa/AdelaiDet/)]
    * **Skeletor**: "Skeletor: Skeletal Transformers for Robust Body-Pose Estimation", arXiv, 2021 (*University of Surrey*). [[Paper](https://arxiv.org/abs/2104.11712)]
    * **HandsFormer**: "HandsFormer: Keypoint Transformer for Monocular 3D Pose Estimation of Hands and Object in Interaction", arXiv, 2021 (*Graz University of Technology*). [[Paper](https://arxiv.org/abs/2104.14639)]
    * **TTP**: "Test-Time Personalization with a Transformer for Human Pose Estimation", NeurIPS, 2021 (*UCSD*). [[Paper](https://arxiv.org/abs/2107.02133)][[PyTorch](https://github.com/harry11162/TTP)][[Website](https://liyz15.github.io/TTP/)]
    * **GraFormer**: "GraFormer: Graph Convolution Transformer for 3D Pose Estimation", arXiv, 2021 (*CAS*). [[Paper](https://arxiv.org/abs/2109.08364)]
    * **GCT**: "Geometry-Contrastive Transformer for Generalized 3D Pose Transfer", AAAI, 2022 (*University of Oulu*). [[Paper](https://arxiv.org/abs/2112.07374)][[PyTorch](https://github.com/mikecheninoulu/CGT)]
    * **MHFormer**: "MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation", CVPR, 2022 (*Peking*). [[Paper](https://arxiv.org/abs/2111.12707)][[PyTorch](https://github.com/Vegetebird/MHFormer)]
    * **PAHMT**: "Spatial-Temporal Parallel Transformer for Arm-Hand Dynamic Estimation", CVPR, 2022 (*NetEase*). [[Paper](https://arxiv.org/abs/2203.16202)]
    * **TCFormer**: "Not All Tokens Are Equal: Human-centric Visual Analysis via Token Clustering Transformer", CVPR, 2022 (*CUHK*). [[Paper](https://arxiv.org/abs/2204.08680)][[PyTorch](https://github.com/zengwang430521/TCFormer)]
    * **PETR**: "End-to-End Multi-Person Pose Estimation With Transformers", CVPR, 2022 (*Hikvision*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Shi_End-to-End_Multi-Person_Pose_Estimation_With_Transformers_CVPR_2022_paper.html)][[PyTorch](https://github.com/hikvision-research/opera)]
    * **GraFormer**: "GraFormer: Graph-Oriented Transformer for 3D Pose Estimation", CVPR, 2022 (*CAS*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Zhao_GraFormer_Graph-Oriented_Transformer_for_3D_Pose_Estimation_CVPR_2022_paper.html)]
    * **Keypoint-Transformer**: "Keypoint Transformer: Solving Joint Identification in Challenging Hands and Object Interactions for Accurate 3D Pose Estimation", CVPR, 2022 (*Graz University of Technology, Austria*). [[Paper](https://arxiv.org/abs/2104.14639)][[PyTorch](https://github.com/shreyashampali/kypt_transformer)][[Website](https://www.tugraz.at/index.php?id=57823)]
    * **MPS-Net**: "Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation from Monocular Video", CVPR, 2022 (*Academia Sinica*). [[Paper](https://arxiv.org/abs/2203.08534)][[Website](https://mps-net.github.io/MPS-Net/)]
    * **Ego-STAN**: "Building Spatio-temporal Transformers for Egocentric 3D Pose Estimation", CVPRW, 2022 (*University of Waterloo, Canada*). [[Paper](https://arxiv.org/abs/2206.04785)]
    * **AggPose**: "AggPose: Deep Aggregation Vision Transformer for Infant Pose Estimation", IJCAI, 2022 (*Shenzhen Baoan Women’s and Childiren’s Hospital*). [[Paper](https://arxiv.org/abs/2205.05277)][[Code (in construction)](https://github.com/SZAR-LAB/AggPose)]
    * **MotionMixer**: "MotionMixer: MLP-based 3D Human Body Pose Forecasting", IJCAI, 2022 (*Ulm University, Germany*). [[Paper](https://arxiv.org/abs/2207.00499)][[Code (in construction)](https://github.com/MotionMLP/MotionMixer)]
    * **Jointformer**: "Jointformer: Single-Frame Lifting Transformer with Error Prediction and Refinement for 3D Human Pose Estimation", ICPR, 2022 (*Trinity College Dublin, Ireland*). [[Paper](https://arxiv.org/abs/2208.03704)]
    * **IVT**: "IVT: An End-to-End Instance-guided Video Transformer for 3D Pose Estimation", ACMMM, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2208.03431)]
    * **FastMETRO**: "Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers", ECCV, 2022 (*POSTECH*). [[Paper](https://arxiv.org/abs/2207.13820)][[PyTorch](https://github.com/postech-ami/FastMETRO)][[Website](https://fastmetro.github.io)]
    * **PPT**: "PPT: token-Pruned Pose Transformer for monocular and multi-view human pose estimation", ECCV, 2022 (*UC Irvine*). [[Paper](https://arxiv.org/abs/2209.08194)][[PyTorch](https://github.com/HowieMa/PPT)]
    * **Poseur**: "Poseur: Direct Human Pose Regression with Transformers", ECCV, 2022 (*The University of Adelaide, Australia*). [[Paper](https://arxiv.org/abs/2201.07412)]
    * **ViTPose**: "ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation", NeurIPS, 2022 (*The University of Sydney*). [[Paper](https://arxiv.org/abs/2204.12484)][[PyTorch](https://github.com/ViTAE-Transformer/ViTPose)]
    * **Swin-Pose**: "Swin-Pose: Swin Transformer Based Human Pose Estimation", arXiv, 2022 (*UMass Lowell*) [[Paper](https://arxiv.org/abs/2201.07384)]
    * **HeadPosr**: "HeadPosr: End-to-end Trainable Head Pose Estimation using Transformer Encoders", arXiv, 2022 (*ETHZ*). [[Paper](https://arxiv.org/abs/2202.03548)]
    * **CrossFormer**: "CrossFormer: Cross Spatio-Temporal Transformer for 3D Human Pose Estimation", arXiv, 2022 (*Canberra University, Australia*). [[Paper](https://arxiv.org/abs/2203.13387)]
    * **VTP**: "VTP: Volumetric Transformer for Multi-view Multi-person 3D Pose Estimation", arXiv, 2022 (*Hangzhou Dianzi University*). [[Paper](https://arxiv.org/abs/2205.12602)]
    * **FeatER**: "FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER", CVPR, 2023 (*UCF*). [[Paper](https://arxiv.org/abs/2205.15448)][[Code (in construction)](https://github.com/zczcwh/FeatER)][[Website](https://zczcwh.github.io/feater_page/)]
    * **GraphMLP**: "GraphMLP: A Graph MLP-Like Architecture for 3D Human Pose Estimation", arXiv, 2022 (*Peking University*). [[Paper](https://arxiv.org/abs/2206.06420)]
    * **siMLPe**: "Back to MLP: A Simple Baseline for Human Motion Prediction", arXiv, 2022 (*INRIA*). [[Paper](https://arxiv.org/abs/2207.01567)][[Pytorch](https://github.com/dulucas/siMLPe)]
    * **Snipper**: "Snipper: A Spatiotemporal Transformer for Simultaneous Multi-Person 3D Pose Estimation Tracking and Forecasting on a Video Snippet", arXiv, 2022 (*University of Alberta, Canada*). [[Paper](https://arxiv.org/abs/2207.04320)][[PyTorch](https://github.com/JimmyZou/Snipper)]
    * **OTPose**: "OTPose: Occlusion-Aware Transformer for Pose Estimation in Sparsely-Labeled Videos", arXiv, 2022 (*Korea University*). [[Paper](https://arxiv.org/abs/2207.09725)]
    * **PoseBERT**: "PoseBERT: A Generic Transformer Module for Temporal 3D Human Modeling", arXiv, 2022 (*NAVER*). [[Paper](https://arxiv.org/abs/2208.10211)][[PyTorch](https://github.com/naver/posebert)]
    * **KOG-Transformer**: "K-Order Graph-oriented Transformer with GraAttention for 3D Pose and Shape Estimation", arXiv, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2208.11328)]
    * **SoMoFormer**: "SoMoFormer: Multi-Person Pose Forecasting with Transformers", arXiv, 2022 (*Stanford*). [[Paper](https://arxiv.org/abs/2208.14023)]
    * **DPIT**: "DPIT: Dual-Pipeline Integrated Transformer for Human Pose Estimation", arXiv, 2022 (*Shanghai University*). [[Paper](https://arxiv.org/abs/2209.02431)]
    * **Uplift-Upsample**: "Uplift and Upsample: Efficient 3D Human Pose Estimation with Uplifting Transformers", WACV, 2023 (*University of Augsburg, Germany*). [[Paper](https://arxiv.org/abs/2210.06110)][[Tensorflow](https://github.com/goldbricklemon/uplift-upsample-3dhpe)]
    * **TORE**: "TORE: Token Reduction for Efficient Human Mesh Recovery with Transformer", ICCV, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2211.10705)][[Code (in construction)](https://github.com/Frank-ZY-Dou/TORE)][[Website](https://frank-zy-dou.github.io/projects/Tore/index.html)]
    * **MPT**: "MPT: Mesh Pre-Training with Transformers for Human Pose and Mesh Reconstruction", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2211.13357)]
    * **ViTPose+**: "ViTPose+: Vision Transformer Foundation Model for Generic Body Pose Estimation", arXiv, 2022 (*The University of Sydney*). [[Paper](https://arxiv.org/abs/2212.04246)][[PyTorch](https://github.com/ViTAE-Transformer/ViTPose)]
    * **POT**: "Pose-Oriented Transformer with Uncertainty-Guided Refinement for 2D-to-3D Human Pose Estimation", AAAI, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2302.07408)]
    * **INT**: "Capturing the Motion of Every Joint: 3D Human Pose and Shape Estimation with Independent Tokens", ICLR, 2023 (*Southeast University*). [[Paper](https://openreview.net/forum?id=0Vv4H4Ch0la)]
    * **TBIFormer**: "Trajectory-Aware Body Interaction Transformer for Multi-Person Pose Forecasting", CVPR, 2023 (*Hangzhou Dianzi Universit*). [[Paper](https://arxiv.org/abs/2303.05095)]
    * **PSVT**: "PSVT: End-to-End Multi-person 3D Pose and Shape Estimation with Progressive Video Transformers", CVPR, 2023 (*Baidu*). [[Paper](https://arxiv.org/abs/2303.09187)]
    * **PCT**: "Human Pose as Compositional Tokens", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2303.11638)][[PyTorch](https://github.com/Gengzigang/PCT)][[Website](https://sites.google.com/view/pctpose)]
    * **OSX**: "One-Stage 3D Whole-Body Mesh Recovery with Component Aware Transformer", CVPR, 2023 (*IDEA*). [[Paper](https://arxiv.org/abs/2303.16160)][[PyTorch](https://github.com/IDEA-Research/OSX)][[Website](https://osx-ubody.github.io/)]
    * **PoseFormerV2**: "PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation", CVPR, 2023 (*UCF*). [[Paper](https://arxiv.org/abs/2303.17472)][[PyTorch](https://github.com/QitaoZhao/PoseFormerV2)][[Website](https://qitaozhao.github.io/PoseFormerV2)][[Website](https://qitaozhao.github.io/PoseFormerV2)]
    * **SA-HMR**: "Learning Human Mesh Recovery in 3D Scenes", CVPR, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2306.03847)][[Code (in construction)](https://github.com/zju3dv/SA-HMR/)][[Website](https://zju3dv.github.io/sahmr/)]
    * **DeFormer**: "Deformable Mesh Transformer for 3D Human Mesh Recovery", CVPR, 2023 (*National Institute of Advanced Industrial Science and Technology (AIST), Japan*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Yoshiyasu_Deformable_Mesh_Transformer_for_3D_Human_Mesh_Recovery_CVPR_2023_paper.html)]
    * **STCFormer**: "3D Human Pose Estimation With Spatio-Temporal Criss-Cross Attention", CVPR, 2023 (*Hefei University of Technology*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Tang_3D_Human_Pose_Estimation_With_Spatio-Temporal_Criss-Cross_Attention_CVPR_2023_paper.html)]
    * **DistilPose**: "DistilPose: Tokenized Pose Regression With Heatmap Distillation", CVPR, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2303.02455)][[PyTorch](https://github.com/yshMars/DistilPose)]
    * **LPFormer**: "LPFormer: LiDAR Pose Estimation Transformer with Multi-Task Network", CVPRW, 2023 (*UCF*). [[Paper](https://arxiv.org/abs/2306.12525)]
    * **LAMP**: "LAMP: Leveraging Language Prompts for Multi-person Pose Estimation", IROS, 2023 (*UCF*). [[Paper](https://arxiv.org/abs/2307.11934)][[PyTorch](https://github.com/shengnanh20/LAMP)]
    * **DiffPose**: "DiffPose: SpatioTemporal Diffusion Model for Video-Based Human Pose Estimation", ICCV, 2023 (*Jilin University*). [[Paper](https://arxiv.org/abs/2307.16687)]
    * **JOTR**: "JOTR: 3D Joint Contrastive Learning with Transformers for Occluded Human Mesh Recovery", ICCV, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2307.16377)]
    * **GroupPose**: "Group Pose: A Simple Baseline for End-to-End Multi-person Pose Estimation", ICCV, 2023 (*Baidu*). [[Paper](https://arxiv.org/abs/2308.07313)][[Paddle](https://github.com/Michel-liu/GroupPose-Paddle)][[PyTorch](https://github.com/Michel-liu/GroupPose)]
    * **CoordFormer**: "Coordinate Transformer: Achieving Single-stage Multi-person Mesh Recovery from Videos", ICCV, 2023 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2308.10334)][[Code (in construction)](https://github.com/Li-Hao-yuan/CoordFormer)]
    * **PoseFix**: "PoseFix: Correcting 3D Human Poses with Natural Language", ICCV, 2023 (*NAVER*). [[Paper](https://arxiv.org/abs/2309.08480)][[Website](https://europe.naverlabs.com/research/computer-vision/posefix/)]
    * **4D-Humans**: "Humans in 4D: Reconstructing and Tracking Humans with Transformers", ICCV, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2305.20091)][[PyTorch](https://github.com/shubham-goel/4D-Humans)][[Website](https://shubham-goel.github.io/4dhumans/)]
    * **HopFIR**: "HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation", ICCV, 2023 (*Hefei University of Technology*). [[Paper](https://arxiv.org/abs/2302.14581)]
    * **HumanMAC**: "HumanMAC: Masked Motion Completion for Human Motion Prediction", ICCV, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2302.03665)][[PyTorch](https://github.com/LinghaoChan/HumanMAC)][[Website](https://lhchen.top/Human-MAC/)]
    * **XFormer**: "XFormer: Fast and Accurate Monocular 3D Body Capture", arXiv, 2023 (*Huya Inc, China*). [[Paper](https://arxiv.org/abs/2305.11101)]
    * **PGformer**: "PGformer: Proxy-Bridged Game Transformer for Multi-Person Extremely Interactive Motion Prediction", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2306.03374)]
    * **?**: "Scene-aware Human Pose Generation using Transformer", arXiv, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2308.02177)]
    * **HoT**: "Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose Estimation", arXiv, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2311.12028)]
    * **Pose-Anything**: "Pose Anything: A Graph-Based Approach for Category-Agnostic Pose Estimation", arXiv, 2023 (*Tel Aviv*). [[Paper](https://arxiv.org/abs/2311.17891)][[Code (in construction)](https://github.com/orhir/PoseAnything)][[Website](https://orhir.github.io/pose-anything/)]
    * **PoseGPT**: "PoseGPT: Chatting about 3D Human Pose", arXiv, 2023 (*MPI*). [[Paper](https://arxiv.org/abs/2311.18836)][[Code (in construction)](https://github.com/yfeng95/PoseGPT)][[Website](https://yfeng95.github.io/posegpt/)]
    * **TEMP3D**: "TEMP3D: Temporally Continuous 3D Human Pose Estimation Under Occlusions", arXiv, 2023 (*UC Riverside*). [[Paper](https://arxiv.org/abs/2312.16221)][[Website](https://sites.google.com/ucr.edu/temp3d)]
* Hands:
    * **Hand-Transformer**: "Hand-Transformer: Non-Autoregressive Structured Modeling for 3D Hand Pose Estimation", ECCV, 2020 (*Kwai*). [[Paper](https://www.ecva.net/papers/eccv_2020/papers_ECCV/html/4836_ECCV_2020_paper.php)]
    * **SCAT**: "SCAT: Stride Consistency With Auto-Regressive Regressor and Transformer for Hand Pose Estimation", ICCVW, 2021 (*Alibaba*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021W/SoMoF/html/Gao_SCAT_Stride_Consistency_With_Auto-Regressive_Regressor_and_Transformer_for_Hand_ICCVW_2021_paper.html)]
    * **SeTHPose**: "Learning Sequential Contexts using Transformer for 3D Hand Pose Estimation", arXiv, 2022 (*Queen's University, Canada*). [[Paper](https://arxiv.org/abs/2206.00171)]
    * **HTT**: "Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition from Egocentric RGB Videos", CVPR, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2209.09484)][[PyTorch](https://github.com/fylwen/HTT)][[Website](https://fylwen.github.io/htt.html)]
    * **?**: "Image-free Domain Generalization via CLIP for 3D Hand Pose Estimation", arXiv, 2022 (*UNIST, Korea*). [[Paper](https://arxiv.org/abs/2210.16788)]
    * **A2J-Transformer**: "A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image", CVPR, 2023 (*Huazhong University of Science and Technology*). [[Paper](https://arxiv.org/abs/2304.03635)][[PyTorch](https://github.com/ChanglongJiangGit/A2J-Transformer)]
    * **H2OTR**: "Transformer-Based Unified Recognition of Two Hands Manipulating Objects", CVPR, 2023 (*Ulsan National Institute of Science & Technology (UNIST), Korea*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Cho_Transformer-Based_Unified_Recognition_of_Two_Hands_Manipulating_Objects_CVPR_2023_paper.html)]
    * **Deformer**: "Deformer: Dynamic Fusion Transformer for Robust Hand Pose Estimation", ICCV, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2303.04991)][[Code (in construction)](https://github.com/fuqichen1998/Deformer)][[Website](https://fuqichen1998.github.io/Deformer/)]
    * **CLIP-Hand3D**: "CLIP-Hand3D: Exploiting 3D Hand Pose Estimation via Context-Aware Prompting", ACMMM, 2023 (*Ocean University of China*). [[Paper](https://arxiv.org/abs/2309.16140)]
* Others:
    * **TAPE**: "Transformer Guided Geometry Model for Flow-Based Unsupervised Visual Odometry", arXiv, 2020 (*Tianjing University*). [[Paper](https://arxiv.org/abs/2101.02143)]
    * **T6D-Direct**: "T6D-Direct: Transformers for Multi-Object 6D Pose Direct Regression", GCPR, 2021 (*University of Bonn*). [[Paper](https://arxiv.org/abs/2109.10948)]
    * **6D-ViT**: "6D-ViT: Category-Level 6D Object Pose Estimation via Transformer-based Instance Representation Learning", arXiv, 2021 (*University of Science and Technology of China*). [[Paper](https://arxiv.org/abs/2110.04792)]
    * **RayTran**: "RayTran: 3D pose estimation and shape reconstruction of multiple objects from videos with ray-traced transformers", ECCV, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2203.13296)]
    * **DProST**: "DProST: Dynamic Projective Spatial Transformer Network for 6D Pose Estimation", ECCV, 2022 (*Seoul National University*). [[Paper](https://arxiv.org/abs/2112.08775)][[PyTorch](https://github.com/parkjaewoo0611/DProST)]
    * **AFT-VO**: "AFT-VO: Asynchronous Fusion Transformers for Multi-View Visual Odometry Estimation", arXiv, 2022 (*University of Surrey, UK*). [[Paper](https://arxiv.org/abs/2206.12946)]
    * **DPT-VO**: "Dense Prediction Transformer for Scale Estimation in Monocular Visual Odometry", arXiv, 2022 (*Aeronautics Institute of Technology, Brazil*). [[Paper](https://arxiv.org/abs/2210.01723)]
    * **?**: "Video based Object 6D Pose Estimation using Transformers", arXiv, 2022 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2210.13540)][[PyTorch](https://github.com/ApoorvaBeedu/VideoPose)]
    * **PoET**: "PoET: Pose Estimation Transformer for Single-View, Multi-Object 6D Pose Estimation", arXiv, 2022 (*Infineon Technologies Austria AG*). [[Paper](https://arxiv.org/abs/2211.14125)][[PyTorch](https://github.com/aau-cns/poet)]
    * **CRT-6D**: "CRT-6D: Fast 6D Object Pose Estimation with Cascaded Refinement Transformers", WACV, 2023 (*ICL, UK*). [[Paper](https://arxiv.org/abs/2210.11718)][[Code (in construction)](https://github.com/PedroCastro/CRT-6D)]
    * **TokenHPE**: "TokenHPE: Learning Orientation Tokens for Efficient Head Pose Estimation via Transformers", CVPR, 2023 (*Central China Normal University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_TokenHPE_Learning_Orientation_Tokens_for_Efficient_Head_Pose_Estimation_via_CVPR_2023_paper.html)][[Code (in construction)](https://github.com/zc2023/TokenHPE)]
    * **CLAMP**: "CLAMP: Prompt-based Contrastive Learning for Connecting Language and Animal Pose", CVPR, 2023 (*The University of Sydney*). [[Paper](https://arxiv.org/abs/2206.11752)][[Code (in construction)](https://github.com/xuzhang1199/CLAMP)]
    * **DFTr**: "Deep Fusion Transformer Network with Weighted Vector-Wise Keypoints Voting for Robust 6D Object Pose Estimation", ICCV, 2023 (*The Hong Kong Polytechnic University*). [[Paper](https://arxiv.org/abs/2308.05438)][[PyTorch](https://github.com/junzastar/DFTr_Voting)]
    * **c2f-MS-Trans**: "Coarse-to-Fine Multi-Scene Pose Regression with Transformers", TPAMI, 2023 (*Bar-Ilan University (BIU), Israel*). [[Paper](https://arxiv.org/abs/2308.11783)]
    * **TransPoser**: "TransPoser: Transformer as an Optimizer for Joint Object Shape and Pose Estimation", arXiv, 2023 (*Kyoto University*). [[Paper](https://arxiv.org/abs/2303.13477)]
    * **RelPose++**: "RelPose++: Recovering 6D Poses from Sparse-view Observations", arXiv, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2305.04926)][[PyTorch](https://github.com/amyxlase/relpose-plus-plus)][[Website](https://amyxlase.github.io/relpose-plus-plus/)]
    * **KDSM**: "Language-driven Open-Vocabulary Keypoint Detection for Animal Body and Face", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2310.05056)]
    * **UniPose**: "UniPose: Detecting Any Keypoints", arXiv, 2023 (*IDEA*). [[Paper](https://arxiv.org/abs/2310.08530)][[Code (in construction)](https://github.com/IDEA-Research/UniPose)][[Website](https://yangjie-cv.github.io/UniPose/)]
    * **SAM-6D**: "SAM-6D: Segment Anything Model Meets Zero-Shot 6D Object Pose Estimation", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2311.15707)][[PyTorch (in construction)](https://github.com/JiehongLin/SAM-6D)]
    * **?**: "Open-vocabulary object 6D pose estimation", arXiv, 2023 (*Fondazione Bruno Kessler (FBK), Italy*). [[Paper](https://arxiv.org/abs/2312.00690)]
    * **FoundationPose**: "FoundationPose: Unified 6D Pose Estimation and Tracking of Novel Objects", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2312.08344)][[Code (in construction)](https://github.com/NVlabs/FoundationPose)][[Website](https://nvlabs.github.io/FoundationPose/)]

[[Back to Overview](#overview)]

### Tracking
* General:
    * **TransTrack**: "TransTrack: Multiple-Object Tracking with Transformer",arXiv, 2020 (*HKU + ByteDance*). [[Paper](https://arxiv.org/abs/2012.15460)][[PyTorch](https://github.com/PeizeSun/TransTrack)]
    * **TransformerTrack**: "Transformer Meets Tracker: Exploiting Temporal Context for Robust Visual Tracking", CVPR, 2021 (*USTC*). [[Paper](https://arxiv.org/abs/2103.11681)][[PyTorch](https://github.com/594422814/TransformerTrack)]
    * **TransT**: "Transformer Tracking", CVPR, 2021 (*Dalian University of Technology*). [[Paper](https://arxiv.org/abs/2103.15436)][[PyTorch](https://github.com/chenxin-dlut/TransT)]
    * **STARK**: "Learning Spatio-Temporal Transformer for Visual Tracking", ICCV, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2103.17154)][[PyTorch](https://github.com/researchmm/Stark)]
    * **HiFT**: "HiFT: Hierarchical Feature Transformer for Aerial Tracking", ICCV, 2021 (*Tongji University*). [[Paper](https://arxiv.org/abs/2108.00202)][[PyTorch](https://github.com/vision4robotics/HiFT)]
    * **DTT**: "High-Performance Discriminative Tracking With Transformers", ICCV, 2021 (*CAS*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Yu_High-Performance_Discriminative_Tracking_With_Transformers_ICCV_2021_paper.html)]
    * **DualTFR**: "Learning Tracking Representations via Dual-Branch Fully Transformer Networks", ICCVW, 2021 (*Microsoft*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021W/VOT/html/Xie_Learning_Tracking_Representations_via_Dual-Branch_Fully_Transformer_Networks_ICCVW_2021_paper.html)][[PyTorch (in construction)](https://github.com/phiphiphi31/DualTFR)]
    * **TransCenter**: "TransCenter: Transformers with Dense Queries for Multiple-Object Tracking", arXiv, 2021 (*INRIA + MIT*). [[Paper](https://arxiv.org/abs/2103.15145)]
    * **TransMOT**: "TransMOT: Spatial-Temporal Graph Transformer for Multiple Object Tracking", arXiv, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2104.00194)]
    * **TREG**: "Target Transformed Regression for Accurate Tracking", arXiv, 2021 (*Nanjing University*). [[Paper](https://arxiv.org/abs/2104.00403)][[Code (in construction)](https://github.com/MCG-NJU/TREG)]
    * **TrTr**: "TrTr: Visual Tracking with Transformer", arXiv, 2021 (*University of Tokyo*). [[Paper](https://arxiv.org/abs/2105.03817)][[PyTorch](https://github.com/tongtybj/TrTr)]
    * **RelationTrack**: "RelationTrack: Relation-aware Multiple Object Tracking with Decoupled Representation", arXiv, 2021 (*Huazhong Univerisity of Science and Technology*). [[Paper](https://arxiv.org/abs/2105.04322)]
    * **SiamTPN**: "Siamese Transformer Pyramid Networks for Real-Time UAV Tracking", WACV, 2022 (*New York University*). [[Paper](https://arxiv.org/abs/2110.08822)]
    * **MixFormer**: "MixFormer: End-to-End Tracking with Iterative Mixed Attention", CVPR, 2022 (*Nanjing University*). [[Paper](https://arxiv.org/abs/2203.11082)][[PyTorch](https://github.com/MCG-NJU/MixFormer)]
    * **ToMP**: "Transforming Model Prediction for Tracking", CVPR, 2022 (*ETHZ*). [[Paper](https://arxiv.org/abs/2203.11192)][[PyTorch](https://github.com/visionml/pytracking)]
    * **GTR**: "Global Tracking Transformers", CVPR, 2022 (*UT Austin*). [[Paper](https://arxiv.org/abs/2203.13250)][[PyTorch](https://github.com/xingyizhou/GTR)]
    * **UTT**: "Unified Transformer Tracker for Object Tracking", CVPR, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2203.15175)][[Code (in construction)](https://github.com/Flowerfan/Trackron)]
    * **MeMOT**: "MeMOT: Multi-Object Tracking with Memory", CVPR, 2022 (*Amazon*). [[Paper](https://arxiv.org/abs/2203.16761)]
    * **CSwinTT**: "Transformer Tracking with Cyclic Shifting Window Attention", CVPR, 2022 (*Huazhong University of Science and Technology*). [[Paper](https://arxiv.org/abs/2205.03806)][[PyTorch](https://github.com/SkyeSong38/CSWinTT)]
    * **STNet**: "Spiking Transformers for Event-Based Single Object Tracking", CVPR, 2022 (*Dalian University of Technology*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Spiking_Transformers_for_Event-Based_Single_Object_Tracking_CVPR_2022_paper.html)]
    * **TrackFormer**: "TrackFormer: Multi-Object Tracking with Transformers", CVPR, 2022 (*Facebook*). [[Paper](https://arxiv.org/abs/2101.02702)][[PyTorch](https://github.com/timmeinhardt/trackformer)]
    * **SBT**: "Correlation-Aware Deep Tracking", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2203.01666)][[PyTorch](https://github.com/phiphiphi31/SBT)]
    * **SparseTT**: "SparseTT: Visual Tracking with Sparse Transformers", IJCAI, 2022 (*Beihang University*). [[Paper](https://arxiv.org/abs/2205.03776)][[Code (in construction)](https://github.com/fzh0917/SparseTT)]
    * **AiATrack**: "AiATrack: Attention in Attention for Transformer Visual Tracking", ECCV, 2022 (*Huazhong University of Science and Technology*). [[Paper](https://arxiv.org/abs/2207.09603)][[PyTorch](https://github.com/Little-Podi/AiATrack)]
    * **MOTR**: "MOTR: End-to-End Multiple-Object Tracking with TRansformer", ECCV, 2022 (*Megvii*). [[Paper](https://arxiv.org/abs/2105.03247)][[PyTorch](https://github.com/megvii-model/MOTR)]
    * **SwinTrack**: "SwinTrack: A Simple and Strong Baseline for Transformer Tracking", NeurIPS, 2022 (*South China University of Technology*). [[Paper](https://arxiv.org/abs/2112.00995)][[PyTorch](https://github.com/litinglin/swintrack)]
    * **ModaMixer**: "Divert More Attention to Vision-Language Tracking", NeurIPS, 2022 (*Beijing Jiaotong University*). [[Paper](https://arxiv.org/abs/2207.01076)][[PyTorch](https://github.com/JudasDie/SOTS)]
    * **TransMOT**: "Transformers for Multi-Object Tracking on Point Clouds", IV, 2022 (*Bosch*). [[Paper](https://arxiv.org/abs/2205.15730)]
    * **TransT-M**: "High-Performance Transformer Tracking", arXiv, 2022 (*Dalian University of Technology*). [[Paper](https://arxiv.org/abs/2203.13533)]
    * **HCAT**: "Efficient Visual Tracking via Hierarchical Cross-Attention Transformer", arXiv, 2022 (*Dalian University of Technology*). [[Paper](https://arxiv.org/abs/2203.13537)]
    * **?**: "Keypoints Tracking via Transformer Networks", arXiv, 2022 (*KAIST*). [[Paper](https://arxiv.org/abs/2203.12848)][[PyTorch](https://github.com/LexaNagiBator228/Keypoints-Tracking-via-Transformer-Networks/)]
    * **TranSTAM**: "Joint Spatial-Temporal and Appearance Modeling with Transformer for Multiple Object Tracking", arXiv, 2022 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2205.15495)][[PyTorch](https://github.com/icicle4/TranSTAM)]
    * **TransFiner**: "TransFiner: A Full-Scale Refinement Approach for Multiple Object Tracking", arXiv, 2022 (*China University of Geosciences*). [[Paper](https://arxiv.org/abs/2207.12967)]
    * **LPAT**: "Local Perception-Aware Transformer for Aerial Tracking", arXiv, 2022 (*Tongji University*). [[Paper](https://arxiv.org/abs/2208.00662)][[PyTorch](https://github.com/vision4robotics/LPAT)]
    * **TADN**: "Transformer-based assignment decision network for multiple object tracking", arXiv, 2022 (*National Technical University of Athens, Greece*). [[Paper](https://arxiv.org/abs/2208.03571)][[Code (in construction)](https://github.com/psaltaath/tadn-mot)]
    * **Strong-TransCenter**: "Strong-TransCenter: Improved Multi-Object Tracking based on Transformers with Dense Representations", arXiv, 2022 (*Tel-Aviv University*). [[Paper](https://arxiv.org/abs/2210.13570)][[PyTorch](https://github.com/amitgalor18/STC_Tracker)]
    * **MQT**: "End-to-end Tracking with a Multi-query Transformer", arXiv, 2022 (*Oxford*). [[Paper](https://arxiv.org/abs/2210.14601)]
    * **ProContEXT**: "ProContEXT: Exploring Progressive Context Transformer for Tracking", arXiv, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2210.15511)]
    * **?**: "Efficient Joint Detection and Multiple Object Tracking with Spatially Aware Transformer", arXiv, 2022 (*Sony*). [[Paper](https://arxiv.org/abs/2211.05654)]
    * **MOTRv2**: "MOTRv2: Bootstrapping End-to-End Multi-Object Tracking by Pretrained Object Detectors", CVPR, 2023 (*Megvii*). [[Paper](https://arxiv.org/abs/2211.09791)][[Pytorch](https://github.com/megvii-research/MOTRv2)]
    * **ViPT**: "Visual Prompt Multi-Modal Tracking", CVPR, 2023 (*Dalian University of Technology*). [[Paper](https://arxiv.org/abs/2303.10826)][[PyTorch](https://github.com/jiawen-zhu/ViPT)]
    * **GRM**: "Generalized Relation Modeling for Transformer Tracking", CVPR, 2023 (*HKUST*). [[Paper](https://arxiv.org/abs/2303.16580)][[PyTorch](https://github.com/Little-Podi/GRM)]
    * **DropMAE**: "DropMAE: Masked Autoencoders with Spatial-Attention Dropout for Tracking Tasks", CVPR, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2304.00571)][[PyTorch](https://github.com/jimmy-dq/DropMAE)]
    * **OVTrack**: "OVTrack: Open-Vocabulary Multiple Object Tracking", CVPR, 2023 (*ETHZ*). [[Paper](https://arxiv.org/abs/2304.08408)][[Website](https://www.vis.xyz/pub/ovtrack/)]
    * **SeqTrack**: "SeqTrack: Sequence to Sequence Learning for Visual Object Tracking", CVPR, 2023 (*Dalian University of Technology*). [[Paper](https://arxiv.org/abs/2304.14394)][[PyTorch](https://github.com/microsoft/VideoX/tree/master/SeqTrack)]
    * **TCOW**: "Tracking through Containers and Occluders in the Wild", CVPR, 2023 (*Columbia*). [[Paper](https://arxiv.org/abs/2305.03052)][[Code (in construction)](https://github.com/basilevh/tcow)][[Website](https://tcow.cs.columbia.edu/)]
    * **VideoTrack**: "VideoTrack: Learning to Track Objects via Video Transformer", CVPR, 2023 (*Microsoft*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Xie_VideoTrack_Learning_To_Track_Objects_via_Video_Transformer_CVPR_2023_paper.html)]
    * **MAT**: "Representation Learning for Visual Object Tracking by Masked Appearance Transfer", CVPR, 2023 (*Dalian University of Technology*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_Representation_Learning_for_Visual_Object_Tracking_by_Masked_Appearance_Transfer_CVPR_2023_paper.html)][[PyTorch](https://github.com/difhnp/MAT)]
    * **MeMOTR**: "MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking", ICCV, 2023 (*Nanjing University*). [[Paper](https://arxiv.org/abs/2307.15700)][[PyTorch](https://github.com/MCG-NJU/MeMOTR)]
    * **ROMTrack**: "Robust Object Modeling for Visual Tracking", ICCV, 2023 (*Nanjing University*). [[Paper](https://arxiv.org/abs/2308.05140)][[PyTorch](https://github.com/dawnyc/ROMTrack)]
    * **HiT**: "Exploring Lightweight Hierarchical Vision Transformers for Efficient Visual Tracking", ICCV, 2023 (*Dalian University of Technology*). [[Paper](https://arxiv.org/abs/2308.06904)][[PyTorch](https://github.com/kangben258/HiT)]
    * **OC-MOT**: "Object-Centric Multiple Object Tracking", ICCV, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2309.00233)]
    * **ColTrack**: "Collaborative Tracking Learning for Frame-Rate-Insensitive Multi-Object Tracking", ICCV, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2308.05911)][[PyTorch](https://github.com/bytedance/ColTrack)]
    * **MVT**: "Mobile Vision Transformer-based Visual Object Tracking", BMVC, 2023 (*Concordia University, Canada*). [[Paper](https://arxiv.org/abs/2309.05829)][[PyTorch](https://github.com/goutamyg/MVT)]
    * **MixFormerV2**: "MixFormerV2: Efficient Fully Transformer Tracking", NeurIPS, 2023 (*Nanjing University*). [[Paper](https://arxiv.org/abs/2305.15896)][[PyTorch](https://github.com/MCG-NJU/MixFormerV2)]
    * **MENDER**: "Type-to-Track: Retrieve Any Object via Prompt-based Tracking", NeurIPS, 2023 (*University of Arkansas*). [[Paper](https://arxiv.org/abs/2305.13495)][[Code](https://github.com/uark-cviu/Type-to-Track)][[Website](https://uark-cviu.github.io/Type-to-Track/)]
    * **MOTRv3**: "MOTRv3: Release-Fetch Supervision for End-to-End Multi-Object Tracking", arXiv, 2023 (*Megvii*). [[Paper](https://arxiv.org/abs/2305.14298)]
    * **OmniMotion**: "Tracking Everything Everywhere All at Once", arXiv, 2023 (*Cornell*). [[Paper](https://arxiv.org/abs/2306.05422)][[PyTorch](https://github.com/qianqianwang68/omnimotion)][[Website](https://omnimotion.github.io/)]
    * **?**: "A Dual-Source Attention Transformer for Multi-Person Pose Tracking", arXiv, 2023 (*University of Bonn, Germany*). [[Paper](https://arxiv.org/abs/2306.05807)]
    * **TAM**: "Track Anything: Segment Anything Meets Videos", arXiv, 2023 (*SUSTech*). [[Paper](https://arxiv.org/abs/2304.11968)][[PyTorch](https://github.com/gaomingqi/Track-Anything)]
    * **SAM-Track**: "Segment and Track Anything", arXiv, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2305.06558)][[PyTorch](https://github.com/z-x-yang/Segment-and-Track-Anything)]
    * **CoTracker**: "CoTracker: It is Better to Track Together", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2307.07635)][[PyTorch](https://github.com/facebookresearch/co-tracker)]
    * **OVTracktor**: "Zero-Shot Open-Vocabulary Tracking with Large Pre-Trained Models", arXiv, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2310.06992)][[Website](https://wenhsuanchu.github.io/ovtracktor/)]
    * **Un-Track**: "Single-Model and Any-Modality for Video Object Tracking", arXiv, 2023 (*University of Wurzburg (JMU), Germany*). [[Paper](https://arxiv.org/abs/2311.15851)][[Code (in construction)](https://github.com/Zongwei97/UnTrack)]
    * **TAO-Amodal**: "Tracking Any Object Amodally", arXiv, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2312.12433)][[Code (in construction)](https://github.com/WesleyHsieh0806/TAO-Amodal)][[Website](https://tao-amodal.github.io/)]
    * **ARTrackV2**: "ARTrackV2: Prompting Autoregressive Tracker Where to Look and How to Describe", arXiv, 2023 (*Xi'an Jiaotong University*). [[Paper](https://arxiv.org/abs/2312.17133)][[Website](https://artrackv2.github.io/)]
    * **TrackGPT**: "Tracking with Human-Intent Reasoning", arXiv, 2024 (*Alibaba*). [[Paper](https://arxiv.org/abs/2312.17448)][[Code (in construction)](https://github.com/jiawen-zhu/TrackGPT)]
    * **SMAT**: "Separable Self and Mixed Attention Transformers for Efficient Object Tracking", WACV, 2024 (*Concordia University, Canada*). [[Paper](https://arxiv.org/abs/2309.03979)][[PyTorch](https://github.com/goutamyg/SMAT)]
    * **ContrasTR**: "Contrastive Learning for Multi-Object Tracking with Transformers", WACV, 2024 (*KU Leuven*). [[Paper](https://arxiv.org/abs/2311.08043)]
    * **M3SOT**: "M3SOT: Multi-frame, Multi-field, Multi-space 3D Single Object Tracking", AAAI, 2024 (*Xidian University*). [[Paper](https://arxiv.org/abs/2312.06117)]
    * **EVPTrack**: "Explicit Visual Prompts for Visual Object Tracking", AAAI, 2024 (*Guangxi Normal University*). [[Paper](https://arxiv.org/abs/2401.03142)]
    * **SBT**: "Correlation-Embedded Transformer Tracking: A Single-Branch Framework", arXiv, 2024 (*SJTU*). [[Paper](https://arxiv.org/abs/2401.12743)][[PyTorch](https://github.com/phiphiphi31/SBT)]
* 3D:
    * **PTT**: "PTT: Point-Track-Transformer Module for 3D Single Object Tracking in Point Clouds", IROS, 2021 (*Northeastern University*). [[Paper](https://arxiv.org/abs/2108.06455)][[PyTorch (in construction)](https://github.com/shanjiayao/PTT)]
    * **LTTR**: "3D Object Tracking with Transformer", BMVC, 2021 (*Northeastern University, China*). [[Paper](https://arxiv.org/abs/2110.14921)][[Code (in construction)](https://github.com/3bobo/lttr)]
    * **PTTR**: "PTTR: Relational 3D Point Cloud Object Tracking with Transformer", CVPR, 2022 (*Sensetime*). [[Paper](https://arxiv.org/abs/2112.02857)][[PyTorch](https://github.com/Jasonkks/PTTR)]
    * **STNet**: "3D Siamese Transformer Network for Single Object Tracking on Point Clouds", ECCV, 2022 (*Nanjing University of Science and Technology*). [[Paper](https://arxiv.org/abs/2207.11995)][[PyTorch](https://github.com/fpthink/STNet)]
    * **CMT**: "CMT: Context-Matching-Guided Transformer for 3D Tracking in Point Clouds", ECCV, 2022 (*USTC*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1253_ECCV_2022_paper.php)]
    * **PTT**: "Real-time 3D Single Object Tracking with Transformer", TMM, 2022 (*Northeastern University, China*). [[Paper](https://arxiv.org/abs/2209.00860)][[PyTorch](https://github.com/shanjiayao/PTT)]
    * **InterTrack**: "InterTrack: Interaction Transformer for 3D Multi-Object Tracking", arXiv, 2022 (*University of Toronto*). [[Paper](https://arxiv.org/abs/2208.08041)]
    * **PTTR++**: "Exploring Point-BEV Fusion for 3D Point Cloud Object Tracking with Transformer", arXiv, 2022 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2208.05216)][[PyTorch](https://github.com/Jasonkks/PTTR)]
    * **GLT-T**: "GLT-T: Global-Local Transformer Voting for 3D Single Object Tracking in Point Clouds", AAAI, 2023 (*Hangzhou Dianzi University*). [[Paper](https://arxiv.org/abs/2211.10927)][[PyTorch](https://github.com/haooozi/GLT-T)]
    * **3DMOTFormer**: "3DMOTFormer: Graph Transformer for Online 3D Multi-Object Tracking", ICCV, 2023 (*University of Bonn, Germany*). [[Paper](https://arxiv.org/abs/2308.06635)][[PyTorch](https://github.com/dsx0511/3DMOTFormer)]
    * **CiteTracker**: "CiteTracker: Correlating Image and Text for Visual Tracking", ICCV, 2023 (*Peng Cheng Lab*). [[Paper](https://arxiv.org/abs/2308.11322)][[PyTorch)](https://github.com/NorahGreen/CiteTracker)]
    * **MoMA-M3T**: "Delving into Motion-Aware Matching for Monocular 3D Object Tracking", ICCV, 2023 (*UC Merced*). [[Paper](https://arxiv.org/abs/2308.11607)][[Code (in construction)](https://github.com/kuanchihhuang/MoMA-M3T)]
    * **TrajectoryFormer**: "TrajectoryFormer: 3D Object Tracking Transformer with Predictive Trajectory Hypotheses", ICCV, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2306.05888)][[Code (in construction)](https://github.com/poodarchu/EFG/blob/master/playground/tracking.3d/waymo/trajectoryformer/README.md)]
    * **SyncTrack**: "Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking", ICCV, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2308.12549)]
    * **MBPTrack**: "MBPTrack: Improving 3D Point Cloud Tracking with Memory Networks and Box Priors", ICCV, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2303.05071)]
    * **DQTrack**: "End-to-end 3D Tracking with Decoupled Queries", ICCV, 2023 (*NVIDIA*). [[Paper](https://openaccess.thecvf.com/content/ICCV2023/html/Li_End-to-end_3D_Tracking_with_Decoupled_Queries_ICCV_2023_paper.html)][[PyTorch](https://github.com/NVlabs/DQTrack)]
    * **GLT-T++**: "GLT-T++: Global-Local Transformer for 3D Siamese Tracking with Ranking Loss", arXiv, 2023 (*Hangzhou Dianzi University*). [[Paper](https://arxiv.org/abs/2304.00242)][[PyTorch](https://github.com/haooozi/GLT-T)]
    * **BOTT**: "BOTT: Box Only Transformer Tracker for 3D Object Tracking", arXiv, 2023 (*Motional*). [[Paper](https://arxiv.org/abs/2308.08753)]

[[Back to Overview](#overview)]

### Re-ID
* **PAT**: "Diverse Part Discovery: Occluded Person Re-Identification With Part-Aware Transformer", CVPR, 2021 (*University of Science and Technology of China*). [[Paper](https://arxiv.org/abs/2106.04095)]
* **HAT**: "HAT: Hierarchical Aggregation Transformers for Person Re-identification", ACMMM, 2021 (*Dalian University of Technology*). [[Paper](https://arxiv.org/abs/2107.05946)]
* **TransReID**: "TransReID: Transformer-based Object Re-Identification", ICCV, 2021 (*Alibaba*). [[Paper](https://arxiv.org/abs/2102.04378)][[PyTorch](https://github.com/heshuting555/TransReID)]
* **APD**: "Transformer Meets Part Model: Adaptive Part Division for Person Re-Identification", ICCVW, 2021 (*Meituan*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021W/HTCV/html/Lai_Transformer_Meets_Part_Model_Adaptive_Part_Division_for_Person_Re-Identification_ICCVW_2021_paper.html)]
* **Pirt**: "Pose-guided Inter- and Intra-part Relational Transformer for Occluded Person Re-Identification", ACMMM, 2021 (*Beihang University*). [[Paper](https://arxiv.org/abs/2109.03483)]
* **TransMatcher**: "Transformer-Based Deep Image Matching for Generalizable Person Re-identification", NeurIPS, 2021 (*IIAI*). [[Paper](https://arxiv.org/abs/2105.14432)][[PyTorch](https://github.com/ShengcaiLiao/QAConv)]
* **STT**: "Spatiotemporal Transformer for Video-based Person Re-identification", arXiv, 2021 (*Beihang University*). [[Paper](https://arxiv.org/abs/2103.16469)] 
* **AAformer**: "AAformer: Auto-Aligned Transformer for Person Re-Identification", arXiv, 2021 (*CAS*). [[Paper](https://arxiv.org/abs/2104.00921)]
* **TMT**: "A Video Is Worth Three Views: Trigeminal Transformers for Video-based Person Re-identification", arXiv, 2021 (*Dalian University of Technology*). [[Paper](https://arxiv.org/abs/2104.01745)]
* **LA-Transformer**: "Person Re-Identification with a Locally Aware Transformer", arXiv, 2021 (*University of Maryland Baltimore County*). [[Paper](https://arxiv.org/abs/2106.03720)]
* **DRL-Net**: "Learning Disentangled Representation Implicitly via Transformer for Occluded Person Re-Identification", arXiv, 2021 (*Peking University*). [[Paper](https://arxiv.org/abs/2107.02380)]
* **GiT**: "GiT: Graph Interactive Transformer for Vehicle Re-identification", arXiv, 2021 (*Huaqiao University*). [[Paper](https://arxiv.org/abs/2107.05475)]
* **OH-Former**: "OH-Former: Omni-Relational High-Order Transformer for Person Re-Identification", arXiv, 2021 (*Shanghaitech University*). [[Paper](https://arxiv.org/abs/2109.11159)]
* **CMTR**: "CMTR: Cross-modality Transformer for Visible-infrared Person Re-identification", arXiv, 2021 (*Beijing Jiaotong University*). [[Paper](https://arxiv.org/abs/2110.08994)]
* **PFD**: "Pose-guided Feature Disentangling for Occluded Person Re-identification Based on Transformer", AAAI, 2022 (*Peking*). [[Paper](https://arxiv.org/abs/2112.02466)][[PyTorch](https://github.com/WangTaoAs/PFD_Net)]
* **NFormer**: "NFormer: Robust Person Re-identification with Neighbor Transformer", CVPR, 2022 (*University of Amsterdam, Netherlands*). [[Paper](https://arxiv.org/abs/2204.09331)][[Code (in construction)](https://github.com/haochenheheda/NFormer)]
* **DCAL**: "Dual Cross-Attention Learning for Fine-Grained Visual Categorization and Object Re-Identification", CVPR, 2022 (*Advanced Micro Devices, China*). [[Paper](https://arxiv.org/abs/2205.02151)]
* **CMT**: " Cross-Modality Transformer for Visible-Infrared Person Re-identification", ECCV, 2022 (*USTC*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4836_ECCV_2022_paper.php)]
* **CAViT**: "CAViT: Contextual Alignment Vision Transformer for Video Object Re-identification", ECCV, 2022 (*CAS*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6694_ECCV_2022_paper.php)][[PyTorch](https://github.com/KimWu1994/CAViT)]
* **PiT**: "Multi-direction and Multi-scale Pyramid in Transformer for Video-based Pedestrian Retrieval", IEEE Transactions on Industrial Informatics, 2022 (* Peking*). [[Paper](https://arxiv.org/abs/2202.06014)]
* **?**: "Motion-Aware Transformer For Occluded Person Re-identification", arXiv, 2022 (*NetEase, China*). [[Paper](https://arxiv.org/abs/2202.04243)]
* **PFT**: "Short Range Correlation Transformer for Occluded Person Re-Identification", arXiv, 2022 (*Nanjing University of Posts and Telecommunications*). [[Paper](https://arxiv.org/abs/2201.01090)]
* **?**: "CLIP-Driven Fine-grained Text-Image Person Re-identification", arXiv, 2022 (*Nanjing University of Science and Technology*). [[Paper](https://arxiv.org/abs/2210.10276)]
* **SeqTR**: "Sequential Transformer for End-to-End Person Search", arXiv, 2022 (*East China Normal University*). [[Paper](https://arxiv.org/abs/2211.04323)]
* **CLIP-ReID**: "CLIP-ReID: Exploiting Vision-Language Model for Image Re-Identification without Concrete Text Labels", arXiv, 2022 (*East China Normal University*). [[Paper](https://arxiv.org/abs/2211.13977)]
* **TMGF**: "Transformer Based Multi-Grained Features for Unsupervised Person Re-Identification", WACVW, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2211.12280)][[Code (in construction)](https://github.com/RikoLi/WACV23-workshop-TMGF)]
* **PMT**: "Learning Progressive Modality-shared Transformers for Effective Visible-Infrared Person Re-identification", AAAI, 2023 (*Jiangsu University*). [[Paper](https://arxiv.org/abs/2212.00226)][[Code (in construction)](https://github.com/hulu88/PMT)]
* **DC-Former**: "DC-Former: Diverse and Compact Transformer for Person Re-Identification", AAAI, 2023 (*Ant Group*). [[Paper](https://arxiv.org/abs/2302.14335)][[PyTorch](https://github.com/ant-research/Diverse-and-Compact-Transformer)]
* **PHA**: "PHA: Patch-Wise High-Frequency Augmentation for Transformer-Based Person Re-Identification", CVPR, 2023 (*Beihang University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_PHA_Patch-Wise_High-Frequency_Augmentation_for_Transformer-Based_Person_Re-Identification_CVPR_2023_paper.html)]
* **TranSG**: "TranSG: Transformer-Based Skeleton Graph Prototype Contrastive Learning with Structure-Trajectory Prompted Reconstruction for Person Re-Identification", CVPR, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2303.06819)][[PyTorch](https://github.com/Kali-Hac/TranSG)]
* **UNIReID**: "Towards Modality-Agnostic Person Re-Identification With Descriptive Query", CVPR, 2023 (*Wuhan University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Towards_Modality-Agnostic_Person_Re-Identification_With_Descriptive_Query_CVPR_2023_paper.html)][[PyTorch](https://github.com/ccq195/UNIReID)]
* **UniPT**: "Unified Pre-training with Pseudo Texts for Text-To-Image Person Re-identification", ICCV, 2023 (*Baidu*). [[Paper](https://arxiv.org/abs/2309.01420)][[PyTorch](https://github.com/ZhiyinShao-H/UniPT)]
* **PAT**: "Part-Aware Transformer for Generalizable Person Re-identification", ICCV, 2023 (*UESTC*). [[Paper](https://arxiv.org/abs/2308.03322)][[PyTorch](https://github.com/liyuke65535/Part-Aware-Transformer)]
* **HAP**: "HAP: Structure-Aware Masked Image Modeling for Human-Centric Perception", NeurIPS, 2023 (*Baidu*). [[Paper](https://arxiv.org/abs/2310.20695)][[PyTorch](https://github.com/junkunyuan/HAP)][[Website](https://zhangxinyu-xyz.github.io/hap.github.io/)]
* **TP-TPS**: "Exploiting the Textual Potential from Vision-Language Pre-training for Text-based Person Search", arXiv, 2023 (*Tencent*). [[Paper](https://arxiv.org/abs/2303.04497)]
* **PLIP**: "PLIP: Language-Image Pre-training for Person Representation Learning", arXiv, 2023 (*Huazhong University of Science and Technology*). [[Paper](https://arxiv.org/abs/2305.08386)][[Code (in construction)](https://github.com/Zplusdragon/PLIP)]
* **SSCP**: "Selecting Learnable Training Samples is All DETRs Need in Crowded Pedestrian Detection", arXiv, 2023 (*Chongqing University of Posts and Telecommunications*). [[Paper](https://arxiv.org/abs/2305.10801)]
* **PI-VL**: "Exploring Part-Informed Visual-Language Learning for Person Re-Identification", arXiv, 2023 (*iFLYTEK, China*). [[Paper](https://arxiv.org/abs/2308.02738)]
* **TBPS-CLIP**: "An Empirical Study of CLIP for Text-based Person Search", arXiv, 2023 (*Soochow University, China*). [[Paper](https://arxiv.org/abs/2308.10045)][[PyTorch](https://github.com/Flame-Chasers/TBPS-CLIP)]
* **PersonMAE**: "PersonMAE: Person Re-Identification Pre-Training with Masked AutoEncoders", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2311.04496)]
* **TF-CLIP**: "TF-CLIP: Learning Text-free CLIP for Video-based Person Re-Identification", AAAI, 2024 (*Dalian University of Technology*). [[Paper](https://arxiv.org/abs/2312.09627)]
* **TOP-ReID**: "TOP-ReID: Multi-spectral Object Re-Identification with Token Permutation", AAAI, 2024 (*Dalian University of Technology*). [[Paper](https://arxiv.org/abs/2312.09612)][[PyTorch](https://github.com/924973292/TOP-ReID)]
* **MP-ReID**: "Multi-Prompts Learning with Cross-Modal Alignment for Attribute-based Person Re-Identification", AAAI, 2024 (*Eastern Institute of Technology, China*). [[Paper](https://arxiv.org/abs/2312.16797)]

[[Back to Overview](#overview)]

### Face
* General:
    * **FAU-Transformer**: "Facial Action Unit Detection With Transformers", CVPR, 2021 (*Rakuten Institute of Technology*). [[Paper](https://openaccess.thecvf.com/content/CVPR2021/html/Jacob_Facial_Action_Unit_Detection_With_Transformers_CVPR_2021_paper.html)]
    * **TADeT**: "Mitigating Bias in Visual Transformers via Targeted Alignment", BMVC, 2021 (*Gerogia Tech*). [[Paper](https://www.bmvc2021-virtualconference.com/assets/papers/0282.pdf)]
    * **ViT-Face**: "Face Transformer for Recognition", arXiv, 2021 (*Beijing University of Posts and Telecommunications*). [[Paper](https://arxiv.org/abs/2103.14803)]
    * **FaceT**: "Learning to Cluster Faces via Transformer", arXiv, 2021 (*Alibaba*). [[Paper](https://arxiv.org/abs/2104.11502)]
    * **VidFace**: "VidFace: A Full-Transformer Solver for Video Face Hallucination with Unaligned Tiny Snapshots", arXiv, 2021 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2105.14954)]
    * **FAA**: "Shuffle Transformer with Feature Alignment for Video Face Parsing", arXiv, 2021 (*Tencent*). [[Paper](https://arxiv.org/abs/2106.08650)]
    * **FaRL**: "General Facial Representation Learning in a Visual-Linguistic Manner", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2112.03109)][[PyTorch](https://github.com/faceperceiver/farl)]
    * **FaceFormer**: "FaceFormer: Speech-Driven 3D Facial Animation with Transformers", CVPR, 2022 (*HKU*). [[Paper](https://arxiv.org/abs/2112.05329)][[PyTorch](https://github.com/EvelynFan/FaceFormer)][[Website](https://evelynfan.github.io/audio2face/)]
    * **PhysFormer**: "PhysFormer: Facial Video-based Physiological Measurement with Temporal Difference Transformer", CVPR, 2022 (*University of Oulu, Finland*). [[Paper](https://arxiv.org/abs/2111.12082)][[PyTorch](https://github.com/ZitongYu/PhysFormer)]
    * **VTP**: "Sub-word Level Lip Reading With Visual Attention", CVPR, 2022 (*Oxford*). [[Paper](https://arxiv.org/abs/2110.07603)]
    * **Label2Label**: "Label2Label: A Language Modeling Framework for Multi-Attribute Learning", ECCV, 2022 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2207.08677)][[PyTorch](https://github.com/Li-Wanhua/Label2Label)]
    * **FPVT**: "Face Pyramid Vision Transformer", BMVC, 2022 (*FloppyDisk.AI, Pakistan*). [[Paper](https://arxiv.org/abs/2210.11974)][[PyTorch](https://github.com/khawar-islam/FPVT_BMVC22)][[Website](https://khawar-islam.github.io/fpvt/)]
    * **fViT**: "Part-based Face Recognition with Vision Transformers", BMVC, 2022 (*Queen Mary University of London*). [[Paper](https://arxiv.org/abs/2212.00057)]
    * **EventFormer**: "EventFormer: AU Event Transformer for Facial Action Unit Event Detection", arXiv, 2022 (*Peking*). [[Paper](https://arxiv.org/abs/2203.06355)]
    * **MFT**: "Multi-Modal Learning for AU Detection Based on Multi-Head Fused Transformers", arXiv, 2022 (*SUNY Binghamton*). [[Paper](https://arxiv.org/abs/2203.11441)]
    * **VC-TRSF**: "Self-supervised Video-centralised Transformer for Video Face Clustering", arXiv, 2022 (*ICL*). [[Paper](https://arxiv.org/abs/2203.13166)]
    * **MARLIN**: "MARLIN: Masked Autoencoder for facial video Representation LearnINg", CVPR, 2023 (*Monash University, Australia*). [[Paper](https://arxiv.org/abs/2211.06627)][[PyTorch](https://github.com/ControlNet/MARLIN)]
    * **TransFace**: "TransFace: Calibrating Transformer Training for Face Recognition from a Data-Centric Perspective", ICCV, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2308.10133)][[PyTorch](https://github.com/DanJun6737/TransFace)]
* Facial Landmark:
    * **Clusformer**: "Clusformer: A Transformer Based Clustering Approach to Unsupervised Large-Scale Face and Visual Landmark Recognition", CVPR, 2021 (*VinAI Research, Vietnam*). [[Paper](https://openaccess.thecvf.com/content/CVPR2021/html/Nguyen_Clusformer_A_Transformer_Based_Clustering_Approach_to_Unsupervised_Large-Scale_Face_CVPR_2021_paper.html)]
    * **LOTR**: "LOTR: Face Landmark Localization Using Localization Transformer", arXiv, 2021 (*Sertis, Thailand*). [[Paper](https://arxiv.org/abs/2109.10057)]
    * **SLPT**: "Sparse Local Patch Transformer for Robust Face Alignment and Landmarks Inherent Relation Learning", CVPR, 2022 (*University of Technology Sydney*). [[Paper](https://arxiv.org/abs/2203.06541)][[PyTorch](https://github.com/Jiahao-UTS/SLPT-master)]
    * **DTLD**: "Towards Accurate Facial Landmark Detection via Cascaded Transformers", CVPR, 2022 (*Samsung*). [[Paper](https://arxiv.org/abs/2208.10808)]
    * **RePFormer**: "RePFormer: Refinement Pyramid Transformer for Robust Facial Landmark Detection", arXiv, 2022 (*CUHK*). [[Paper](https://arxiv.org/abs/2207.03917)]
* Face Low-Level Vision:
    * **Latent-Transformer**: "A Latent Transformer for Disentangled Face Editing in Images and Videos", ICCV, 2021 (*Institut Polytechnique de Paris*). [[Paper](https://arxiv.org/abs/2106.11895)][[PyTorch](https://github.com/InterDigitalInc/latent-transformer)]
    * **TANet**: "TANet: A new Paradigm for Global Face Super-resolution via Transformer-CNN Aggregation Network", arXiv, 2021 (*Wuhan Institute of Technology*). [[Paper](https://arxiv.org/abs/2109.08174)]
    * **FAT**: "Facial Attribute Transformers for Precise and Robust Makeup Transfer", WACV, 2022 (*University of Rochester*). [[Paper](https://arxiv.org/abs/2104.02894)]
    * **SSAT**: "SSAT: A Symmetric Semantic-Aware Transformer Network for Makeup Transfer and Removal", AAAI, 2022 (*Wuhan University*). [[Paper](https://arxiv.org/abs/2112.03631)][[PyTorch](https://gitee.com/sunzhaoyang0304/ssat-msp)]
    * **TransEditor**: "TransEditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing", CVPR, 2022 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2203.17266)][[PyTorch](https://github.com/BillyXYB/TransEditor)][[Website](https://billyxyb.github.io/TransEditor/)]
    * **RestoreFormer**: "RestoreFormer: High-Quality Blind Face Restoration From Undegraded Key-Value Pairs", CVPR, 2022 (*HKU*). [[Paper](https://arxiv.org/abs/2201.06374)]
    * **HairCLIP**: "HairCLIP: Design Your Hair by Text and Reference Image", CVPR, 2022 (*USTC*). [[Paper](https://arxiv.org/abs/2112.05142)][[PyTorch](https://github.com/wty-ustc/HairCLIP)]
    * **AnyFace**: "AnyFace: Free-style Text-to-Face Synthesis and Manipulation", CVPR, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2203.15334)]
    * **CodeFormer**: "Towards Robust Blind Face Restoration with Codebook Lookup Transformer", NeurIPS, 2022 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2206.11253)][[PyTorch (in construction)](https://github.com/sczhou/CodeFormer)][[Website](https://shangchenzhou.com/projects/CodeFormer/)]
    * **Cycle-Text2Face**: "Cycle Text2Face: Cycle Text-to-face GAN via Transformers", arXiv, 2022 (*Shahed Univerisity, Iran*). [[Paper](https://arxiv.org/abs/2206.04503)]
    * **FaceFormer**: "FaceFormer: Scale-aware Blind Face Restoration with Transformers", arXiv, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2207.09790)]
    * **text2StyleGAN**: "Text-Free Learning of a Natural Language Interface for Pretrained Face Generators", arXiv, 2022 (*Toyota Technological Institute, Chicago*). [[Paper](https://arxiv.org/abs/2209.03953)][[PyTorch](https://github.com/duxiaodan/Fast_text2StyleGAN)]
    * **ManiCLIP**: "ManiCLIP: Multi-Attribute Face Manipulation from Text", arXiv, 2022 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2210.00445)][[PyTorch](https://github.com/hwang1996/ManiCLIP)]
    * **FEAT**: "FEAT: Face Editing with Attention", arXiv, 2022 (*Shenzhen University*). [[Paper](https://arxiv.org/abs/2202.02713)]
    * **CoralStyleCLIP**: "CoralStyleCLIP: Co-optimized Region and Layer Selection for Image Editing", CVPR, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2303.05031)]
    * **CLIP2Protect**: "CLIP2Protect: Protecting Facial Privacy Using Text-Guided Makeup via Adversarial Latent Search", CVPR, 2023 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2306.10008)][[Code (in construction)](https://github.com/fahadshamshad/Clip2Protect)]
    * **PATMAT**: "PATMAT: Person Aware Tuning of Mask-Aware Transformer for Face Inpainting", ICCV, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2304.06107)]
    * **HairCLIPv2**: "HairCLIPv2: Unifying Hair Editing via Proxy Feature Blending", ICCV, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2310.10651)][[Code (in construction)](https://github.com/wty-ustc/HairCLIPv2)]
    * **RestoreFormer++**: "RestoreFormer++: Towards Real-World Blind Face Restoration from Undegraded Key-Value Pairs", arXiv, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2308.07228)]
* Facial Expression:
    * **TransFER**: "TransFER: Learning Relation-aware Facial Expression Representations with Transformers", ICCV, 2021 (*CAS*). [[Paper](https://arxiv.org/abs/2108.11116)]
    * **CVT-Face**: "Robust Facial Expression Recognition with Convolutional Visual Transformers", arXiv, 2021 (*Hunan University*). [[Paper](https://arxiv.org/abs/2103.16854)]
    * **MViT**: "MViT: Mask Vision Transformer for Facial Expression Recognition in the wild", arXiv, 2021 (*University of Science and Technology of China*). [[Paper](https://arxiv.org/abs/2106.04520)]
    * **ViT-SE**: "Learning Vision Transformer with Squeeze and Excitation for Facial Expression Recognition", arXiv, 2021 (*CentraleSupélec, France*). [[Paper](https://arxiv.org/abs/2107.03107)]
    * **EST**: "Expression Snippet Transformer for Robust Video-based Facial Expression Recognition", arXiv, 2021 (*China University of Geosciences*). [[Paper](https://arxiv.org/abs/2109.08409)][[PyTorch](https://anonymous.4open.science/r/ATSE-C58B)]
    * **MFEViT**: "MFEViT: A Robust Lightweight Transformer-based Network for Multimodal 2D+3D Facial Expression Recognition", arXiv, 2021 (*University of Science and Technology of China*). [[Paper](https://arxiv.org/abs/2109.13086)]
    * **F-PDLS**: "Vision Transformer Equipped with Neural Resizer on Facial Expression Recognition Task", ICASSP, 2022 (*KAIST*). [[Paper](https://arxiv.org/abs/2204.02181)]
    * **?**: "Transformer-based Multimodal Information Fusion for Facial Expression Analysis", arXiv, 2022 (*Netease, China*). [[Paper](https://arxiv.org/abs/2203.12367)]
    * **?**: "Facial Expression Recognition with Swin Transformer", arXiv, 2022 (*Dongguk University, Korea*). [[Paper](https://arxiv.org/abs/2203.13472)]
    * **POSTER**: "POSTER: A Pyramid Cross-Fusion Transformer Network for Facial Expression Recognition", arXiv, 2022 (*UCF*). [[Paper](https://arxiv.org/abs/2204.04083)]
    * **STT**: "Spatio-Temporal Transformer for Dynamic Facial Expression Recognition in the Wild", arXiv, 2022 (*Hunan University*). [[Paper](https://arxiv.org/abs/2205.04749)]
    * **FaceMAE**: "FaceMAE: Privacy-Preserving Face Recognition via Masked Autoencoders", arXiv, 2022 (*NUS*). [[Paper](https://arxiv.org/abs/2205.11090)][[Code (in construction)](https://github.com/kaiwang960112/FaceMAE)]
    * **TransFA**: "TransFA: Transformer-based Representation for Face Attribute Evaluation", arXiv, 2022 (*Xidian University*). [[Paper](https://arxiv.org/abs/2207.05456)]
    * **AU-CVT**: "AU-Supervised Convolutional Vision Transformers for Synthetic Facial Expression Recognition", arXiv, 2022 (*Shenzhen Technology University*). [[Paper](https://arxiv.org/abs/2207.09777)][[PyTorch](https://github.com/msy1412/ABAW4)]
    * **?**: "Multi-Task Transformer with uncertainty modelling for Face Based Affective Computing", arXiv, 2022 (*Datakalab, France*). [[Paper](https://arxiv.org/abs/2208.03506)]
    * **APViT**: "Vision Transformer with Attentive Pooling for Robust Facial Expression Recognition", arXiv, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2212.05463)]
    * **Micron-BERT**: "Micron-BERT: BERT-based Facial Micro-Expression Recognition", CVPR, 2023 (*University of Arkansas*). [[Paper](https://arxiv.org/abs/2304.03195)][[PyTorch (in construction)](https://github.com/uark-cviu/Micron-BERT)]
    * **FRL-DGT**: "Feature Representation Learning with Adaptive Displacement Generation and Transformer Fusion for Micro-Expression Recognition", CVPR, 2023 (*Wuhan University*). [[Paper](https://arxiv.org/abs/2304.04420)]
    * **Text2Listen**: "Can Language Models Learn to Listen?", ICCV, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2308.10897)][[Website](https://people.eecs.berkeley.edu/~evonne_ng/projects/text2listen/)]
    * **CLEF**: "Weakly-Supervised Text-driven Contrastive Learning for Facial Behavior Understanding", ICCV, 2023 (*Binghamton University*). [[Paper](https://arxiv.org/abs/2304.00058)]
    * **EmoCLIP**: "EmoCLIP: A Vision-Language Method for Zero-Shot Video Facial Expression Recognition", arXiv, 2023 (*Queen Mary University of London*). [[Paper](https://arxiv.org/abs/2310.16640)][[PyTorch](https://github.com/NickyFot/EmoCLIP)]
* Attack-related:
    * **?**: "Video Transformer for Deepfake Detection with Incremental Learning", ACMMM, 2021 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2108.05307)]
    * **ViTranZFAS**: "On the Effectiveness of Vision Transformers for Zero-shot Face Anti-Spoofing", International Joint Conference on Biometrics (IJCB), 2021 (*Idiap*). [[Paper](https://arxiv.org/abs/2011.08019)]
    * **MTSS**: "Multi-Teacher Single-Student Visual Transformer with Multi-Level Attention for Face Spoofing Detection", BMVC, 2021 (*National Taiwan Ocean University*). [[Paper](https://www.bmvc2021-virtualconference.com/assets/papers/0113.pdf)]
    * **TransRPPG**: "TransRPPG: Remote Photoplethysmography Transformer for 3D Mask Face Presentation Attack Detection", arXiv, 2021 (*University of Oulu*). [[Paper](https://arxiv.org/abs/2104.07419)]
    * **CViT**: "Deepfake Video Detection Using Convolutional Vision Transformer", arXiv, 2021 (*Jimma University*). [[Paper](https://arxiv.org/abs/2102.11126)]
    * **ViT-Distill**: "Deepfake Detection Scheme Based on Vision Transformer and Distillation", arXiv, 2021 (*Sookmyung Women’s University*). [[Paper](https://arxiv.org/abs/2104.01353)]
    * **M2TR**: "M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection", arXiv, 2021 (*Fudan University*). [[Paper](https://arxiv.org/abs/2104.09770)]
    * **Cross-ViT**: "Combining EfficientNet and Vision Transformers for Video Deepfake Detection", arXiv, 2021 (*University of Pisa*). [[Paper](https://arxiv.org/abs/2107.02612)][[PyTorch](https://github.com/davide-coccomini/Combining-EfficientNet-and-Vision-Transformers-for-Video-Deepfake-Detection)]
    * **ICT**: "Protecting Celebrities from DeepFake with Identity Consistency Transformer", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2203.01318)][[PyTorch](https://github.com/LightDXY/ICT_DeepFake)]
    * **GGViT**: "GGViT: Multistream Vision Transformer Network in Face2Face Facial Reenactment Detection", ICPR, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2210.05990)]
    * **?**: "Hybrid Transformer Network for Deepfake Detection", International Conference on Content-Based Multimedia Indexing (CBMI), 2022 (*MediaFutures, Norway*). [[Paper](https://arxiv.org/abs/2208.05820)]
    * **ViTAF**: "Adaptive Transformers for Robust Few-shot Cross-domain Face Anti-spoofing", ECCV, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2203.12175)]
    * **UIA-ViT**: "UIA-ViT: Unsupervised Inconsistency-Aware Method Based on Vision Transformer for Face Forgery Detection", ECCV, 2022 (*USTC*). [[Paper](https://arxiv.org/abs/2210.12752)]
    * **?**: "Multi-Scale Wavelet Transformer for Face Forgery Detection", ACCV, 2022 (*Hikvision*). [[Paper](https://arxiv.org/abs/2210.03899)]
    * **?**: "Self-supervised Transformer for Deepfake Detection", arXiv, 2022 (*USTC, China*). [[Paper](https://arxiv.org/abs/2203.01265)]
    * **ViTransPAD**: "ViTransPAD: Video Transformer using convolution and self-attention for Face Presentation Attack Detection", arXiv, 2022 (*University of La Rochelle, France*). [[Paper](https://arxiv.org/abs/2203.01562)]
    * **?**: "Cross-Forgery Analysis of Vision Transformers and CNNs for Deepfake Image Detection", arXiv, 2022 (*National Research Council, Italy*). [[Paper](https://arxiv.org/abs/2206.13829)]
    * **STDT**: "Deepfake Video Detection with Spatiotemporal Dropout Transformer", arXiv, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2207.06612)]
    * **?**: "Deep Convolutional Pooling Transformer for Deepfake Detection", arXiv, 2022 (*HKU*). [[Paper](https://arxiv.org/abs/2209.05299)]
    * **DGM<sup>4</sup>**: "Detecting and Grounding Multi-Modal Media Manipulation", CVPR, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2304.02556)][[PyTorch](https://github.com/rshaojimmy/MultiModal-DeepFake)][[Website](https://rshaojimmy.github.io/Projects/MultiModal-DeepFake)]
    * **FLIP**: "FLIP: Cross-domain Face Anti-spoofing with Language Guidance", ICCV, 2023 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2309.16649)][[PyTorch](https://github.com/koushiksrivats/FLIP)][[Website](https://koushiksrivats.github.io/FLIP/)]
    * **Face-Transformer**: "Face Transformer: Towards High Fidelity and Accurate Face Swapping", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2304.02530)]
    * **DGM<sup>4</sup>**: "Detecting and Grounding Multi-Modal Media Manipulation and Beyond", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2309.14203)][[PyTorch](https://github.com/rshaojimmy/MultiModal-DeepFake)]
    * **AntifakePrompt**: "AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors", arXiv, 2023 (*NYCU*). [[Paper](https://arxiv.org/abs/2310.17419)]
* Fairness:
    * **TADeT**: "Mitigating Bias in Visual Transformers via Targeted Alignment", BMVC, 2021 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2302.04358)]
* Generation:
    * **Describe3D**: "High-Fidelity 3D Face Generation from Natural Language Descriptions", CVPR, 2023 (*Nanjing University*). [[Paper](https://arxiv.org/abs/2305.03302)][[PyTorch](https://github.com/zhuhao-nju/describe3d)]
    * **LipFormer**: "LipFormer: High-Fidelity and Generalizable Talking Face Generation With a Pre-Learned Facial Codebook", CVPR, 2023 (*Alibaba*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Wang_LipFormer_High-Fidelity_and_Generalizable_Talking_Face_Generation_With_a_Pre-Learned_CVPR_2023_paper.html)]
    * **?**: "High-fidelity Generalized Emotional Talking Face Generation with Multi-modal Emotion Space Learning", CVPR, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2305.02572)]
* 3D: 
    * **CodeTalker**: "CodeTalker: Speech-Driven 3D Facial Animation with Discrete Motion Prior", CVPR, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2301.02379)][[PyTorch](https://github.com/Doubiiu/CodeTalker)][[Website](https://doubiiu.github.io/projects/codetalker/)]
* Age:
    * **DAA**: "DAA: A Delta Age AdaIN operation for age estimation via binary code transformer", CVPR, 2023 (*Jiayu Intelligent Technology, China*). [[Paper](https://arxiv.org/abs/2303.07929)][[PyTorch](https://github.com/redcping/Delta_Age_AdaIN)]

[[Back to Overview](#overview)]

### Neural Architecture Search
* **HR-NAS**: "HR-NAS: Searching Efficient High-Resolution Neural Architectures with Lightweight Transformers", CVPR, 2021 (*HKU*). [[Paper](https://arxiv.org/abs/2106.06560)][[PyTorch](https://github.com/dingmyu/HR-NAS)]
* **CATE**: "CATE: Computation-aware Neural Architecture Encoding with Transformers", ICML, 2021 (*Michigan State*). [[Paper](https://arxiv.org/abs/2102.07108)]
* **AutoFormer**: "AutoFormer: Searching Transformers for Visual Recognition", ICCV, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2107.00651)][[PyTorch](https://github.com/microsoft/Cream/tree/main/AutoFormer)]
* **GLiT**: "GLiT: Neural Architecture Search for Global and Local Image Transformer", ICCV, 2021 (*The University of Sydney + SenseTime*). [[Paper](https://arxiv.org/abs/2107.02960)]
* **BossNAS**: "BossNAS: Exploring Hybrid CNN-transformers with Block-wisely Self-supervised Neural Architecture Search", ICCV, 2021 (*Monash University*). [[Paper](https://arxiv.org/abs/2103.12424)][[PyTorch](https://github.com/changlin31/BossNAS)]
* **ViT-ResNAS**: "Searching for Efficient Multi-Stage Vision Transformers", ICCVW, 2021 (*MIT*). [[Paper](https://arxiv.org/abs/2109.00642)][[PyTorch](https://github.com/yilunliao/vit-search)]
* **AutoformerV2**: "Searching the Search Space of Vision Transformer", NeurIPS, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.14725)][[PyTorch](https://github.com/microsoft/Cream/tree/main/AutoFormerV2)]
* **TNASP**: "TNASP: A Transformer-based NAS Predictor with a Self-evolution Framework", NeurIPS, 2021 (*CAS + Kuaishou*). [[Paper](https://proceedings.neurips.cc/paper/2021/hash/7fa1575cbd7027c9a799983a485c3c2f-Abstract.html)]
* **PSViT**: "PSViT: Better Vision Transformer via Token Pooling and Attention Sharing", arXiv, 2021 (*The University of Sydney + SenseTime*). [[Paper](https://arxiv.org/abs/2108.03428)]
* **As-ViT**: "Auto-scaling Vision Transformers without Training", ICLR, 2022 (*UT Austin*). [[Paper](https://arxiv.org/abs/2202.11921)][[PyTorch](https://github.com/VITA-Group/AsViT)]
* **NASViT**: "NASViT: Neural Architecture Search for Efficient Vision Transformers with Gradient Conflict aware Supernet Training", ICLR, 2022 (*Facebook*). [[Paper](https://openreview.net/forum?id=Qaw16njk6L)]
* **TF-TAS**: "Training-free Transformer Architecture Search", CVPR, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2203.12217)]
* **ViT-Slim**: "Vision Transformer Slimming: Multi-Dimension Searching in Continuous Optimization Space", CVPR, 2022 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2201.00814)][[PyTorch](https://github.com/Arnav0400/ViT-Slim)]
* **BurgerFormer**: "Searching for BurgerFormer with Micro-Meso-Macro Space Design", ICML, 2022 (*CAS*). [[Paper](https://proceedings.mlr.press/v162/yang22f.html)][[Code (in construction)](https://github.com/xingxing-123/BurgerFormer)]
* **UniNet**: "UniNet: Unified Architecture Search with Convolution, Transformer, and MLP", ECCV, 2022 (*CUHK + SenseTime*). [[Paper](https://arxiv.org/abs/2207.05420)]
* **ViTAS**: "Vision Transformer Architecture Search", ECCV, 2022 (*The University of Sydney + SenseTime*). [[Paper](https://arxiv.org/abs/2106.13700)]
* **VTCAS**: "Vision Transformer with Convolutions Architecture Search", arXiv, 2022 (*Donghua University*). [[Paper](https://arxiv.org/abs/2203.10435)]
* **NOAH**: "Neural Prompt Search", arXiv, 2022 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2206.04673)][[PyTorch](https://github.com/Davidzhangyuanhan/NOAH)]
* **FocusFormer**: "FocusFormer: Focusing on What We Need via Architecture Sampler", arXiv, 2022 (*Monash University, Australia*). [[Paper](https://arxiv.org/abs/2208.10861)]
* **NAR-Former**: "NAR-Former: Neural Architecture Representation Learning towards Holistic Attributes Prediction", CVPR, 2023 (*Xidian University, China*). [[Paper](https://arxiv.org/abs/2211.08024)][[PyTorch](https://github.com/yuny220/NAR-Former)]
* **MDL-NAS**: "MDL-NAS: A Joint Multi-Domain Learning Framework for Vision Transformer", CVPR, 2023 (*SenseTime*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Wang_MDL-NAS_A_Joint_Multi-Domain_Learning_Framework_for_Vision_Transformer_CVPR_2023_paper.html)]
* **AutoTaskFormer**: "AutoTaskFormer: Searching Vision Transformers for Multi-task Learning", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2304.08756)]
* **GPT-NAS**: "GPT-NAS: Neural Architecture Search with the Generative Pre-Trained Model", arXiv, 2023 (*Sichuan University*). [[Paper](https://arxiv.org/abs/2305.05351)]
* **NAR-Former-V2**: "NAR-Former V2: Rethinking Transformer for Universal Neural Network Representation Learning", arXiv, 2023 (*Intellifusion, China*). [[Paper](https://arxiv.org/abs/2306.10792)]
* **AutoST**: "AutoST: Training-free Neural Architecture Search for Spiking Transformers", arXiv, 2023 (*NC State*). [[Paper](https://arxiv.org/abs/2307.00293)]
* **TurboViT**: "TurboViT: Generating Fast Vision Transformers via Generative Architecture Search", arXiv, 2023 (*University of Waterloo*). [[Paper](https://arxiv.org/abs/2308.11421)]
* **FLORA**: "FLORA: Fine-grained Low-Rank Architecture Search for Vision Transformer", WACV, 2024 (*NYCU*). [[Paper](https://arxiv.org/abs/2311.03912)][[PyTorch](https://github.com/shadowpa0327/FLORA)]
* **Auto-Prox**: "Auto-Prox: Training-Free Vision Transformer Architecture Search via Automatic Proxy Discovery", AAAI, 2024 (*National University of Defense Technology, China*). [[Paper](https://arxiv.org/abs/2312.09059)][[Code (in construction)](https://github.com/lilujunai/Auto-Prox-AAAI24)]

[[Back to Overview](#overview)]

### Scene Graph
* **BGT-Net**: "BGT-Net: Bidirectional GRU Transformer Network for Scene Graph Generation", CVPRW, 2021 (*ETHZ*). [[Paper](https://arxiv.org/abs/2109.05346)]
* **STTran**: "Spatial-Temporal Transformer for Dynamic Scene Graph Generation", ICCV, 2021 (*Leibniz University Hannover, Germany*). [[Paper](https://arxiv.org/abs/2107.12309)][[PyTorch](https://github.com/yrcong/STTran)]
* **SGG-NLS**: "Learning to Generate Scene Graph from Natural Language Supervision", ICCV, 2021 (*University of Wisconsin-Madison*). [[Paper](https://arxiv.org/abs/2109.02227)][[PyTorch](https://github.com/YiwuZhong/SGG_from_NLS)]
* **SGG-Seq2Seq**: "Context-Aware Scene Graph Generation With Seq2Seq Transformers", ICCV, 2021 (*Layer 6 AI, Canada*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Lu_Context-Aware_Scene_Graph_Generation_With_Seq2Seq_Transformers_ICCV_2021_paper.html)][[PyTorch](https://github.com/layer6ai-labs/SGG-Seq2Seq)]
* **RELAX**: "Image-Text Alignment using Adaptive Cross-attention with Transformer Encoder for Scene Graphs", BMVC, 2021 (*Samsung*). [[Paper](https://www.bmvc2021-virtualconference.com/assets/papers/0117.pdf)]
* **Relation-Transformer**: "Scenes and Surroundings: Scene Graph Generation using Relation Transformer", arXiv, 2021 (*LMU Munich*). [[Paper](https://arxiv.org/abs/2107.05448)]
* **SGTR**: "SGTR: End-to-end Scene Graph Generation with Transformer", CVPR, 2022 (*ShanghaiTech*). [[Paper](https://arxiv.org/abs/2112.12970)][[Code (in construction)](https://github.com/Scarecrow0/SGTR)]
* **GCL**: "Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased Scene Graph Generation", CVPR, 2022 (*Shandong University*). [[Paper](https://arxiv.org/abs/2203.09811)][[PyTorch](https://github.com/dongxingning/SHA-GCL-for-SGG)]
* **Relationformer**: "Relationformer: A Unified Framework for Image-to-Graph Generation", ECCV, 2022 (*TUM*). [[Paper](https://arxiv.org/abs/2203.10202)][[Code (in construction)](https://github.com/suprosanna/relationformer)]
* **SVRP**: "Towards Open-vocabulary Scene Graph Generation with Prompt-based Finetuning", ECCV, 2022 (*Monash University*). [[Paper](https://arxiv.org/abs/2208.08165)]
* **RelTR**: "RelTR: Relation Transformer for Scene Graph Generation", arXiv, 2022 (*Leibniz University Hannover, Germany*). [[Paper](https://arxiv.org/abs/2201.11460)][[PyTorch](https://github.com/yrcong/RelTR)]
* **SG-Shuffle**: "SG-Shuffle: Multi-aspect Shuffle Transformer for Scene Graph Generation", arXiv, 2022 (*The University of Sydney*). [[Paper](https://arxiv.org/abs/2211.04773)]
* **IS-GGT**: "Iterative Scene Graph Generation with Generative Transformers", CVPR, 2023 (*Oklahoma State University*). [[Paper](https://arxiv.org/abs/2211.16636)]
* **SQUAT**: "Devil's on the Edges: Selective Quad Attention for Scene Graph Generation", CVPR, 2023 (*POSTECH*). [[Paper](https://arxiv.org/abs/2304.03495)][[PyTorch](https://github.com/hesedjds/SQUAT)][[Website](https://cvlab.postech.ac.kr/research/SQUAT/)]
* **VS<sup>3</sup>**: "Learning to Generate Language-supervised and Open-vocabulary Scene Graph using Pre-trained Visual-Semantic Space", CVPR, 2023 (*CUHK*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Learning_To_Generate_Language-Supervised_and_Open-Vocabulary_Scene_Graph_Using_Pre-Trained_CVPR_2023_paper.html)]
* **PVSG**: "Panoptic Video Scene Graph Generation", CVPR, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2311.17058)][[PyTorch](https://github.com/LilyDaytoy/OpenPVSG)][[Website](https://jingkang50.github.io/PVSG/)]
* **VETO-MEET**: "Vision Relation Transformer for Unbiased Scene Graph Generation", ICCV, 2023 (*Technical University of Darmstadt, Germany*). [[Paper](https://arxiv.org/abs/2308.09472)][[PyTorch](https://github.com/visinf/veto)]
* **TextPSG**: "TextPSG: Panoptic Scene Graph Generation from Textual Descriptions", ICCV, 2023 (*IBM*). [[Paper](https://arxiv.org/abs/2310.07056)][[Code (in construction)](https://github.com/chengyzhao/TextPSG)][[Website](https://vis-www.cs.umass.edu/TextPSG/)]
* **HiLo**: "HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation", ICCV, 2023 (*King's College London*). [[Paper](https://arxiv.org/abs/2303.15994)][[PyTorch](https://github.com/franciszzj/HiLo)]
* **SGTR+**: "SGTR+: End-to-end Scene Graph Generation with Transformer", TPAMI, 2023 (*ShanghaiTech*). [[Paper](https://arxiv.org/abs/2401.12835)][[PyTorch](https://github.com/Scarecrow0/SGTR)]
* **SGT**: "Revisiting Transformer for Point Cloud-based 3D Scene Graph Generation", arXiv, 2023 (*Beijing University of Posts and Telecommunications*). [[Paper](https://arxiv.org/abs/2303.11048)]

[[Back to Overview](#overview)]


## Transfer / X-Supervised / X-Shot / Continual Learning
* Transfer Learning/Adapter:
    * **AdaptFormer**: "AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition", NeurIPS, 2022 (*HKU*). [[Paper](https://arxiv.org/abs/2205.13535)][[PyTorch](https://github.com/ShoufaChen/AdaptFormer)][[Website](http://www.shoufachen.com/adaptformer-page/)]
    * **Convpass**: "Convolutional Bypasses Are Better Vision Transformer Adapters", arXiv, 2022 (*Peking University*). [[Paper](https://arxiv.org/abs/2207.07039)][[Pytorch](https://github.com/JieShibo/PETL-ViT)]
    * **FacT**: "FacT: Factor-Tuning for Lightweight Adaptation on Vision Transformer", AAAI, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2212.03145)][[Pytorch](https://github.com/JieShibo/PETL-ViT)]
    * **Consolidator**: "Consolidator: Mergable Adapter with Group Connections for Vision Transformer", ICLR, 2023 (*Tsinghua*). [[Paper](https://openreview.net/forum?id=J_Cja7cpgW)]
    * **REACT**: "Learning Customized Visual Models with Retrieval-Augmented Knowledge", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2301.07094)][[Code (in construction)](https://github.com/microsoft/react)][[Website](https://react-vl.github.io/)]
    * **MP**: "Tuning Pre-trained Model via Moment Probing", ICCV, 2023 (*Tianjin University*). [[Paper](https://arxiv.org/abs/2307.11342)][[PyTorch](https://github.com/mingzeG/Moment-Probing)]
    * **ARC**: "Efficient Adaptation of Large Vision Transformer via Adapter Re-Composing", NeurIPS, 2023 (*Xi'an University of Architecture and Technology*). [[Paper](https://arxiv.org/abs/2310.06234)][[PyTorch](https://github.com/DavidYanAnDe/ARC)]
    * **Res-Tuning**: "Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone", NeurIPS, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2310.19859)][[Website](https://res-tuning.github.io/)]
    * **E<sup>3</sup>VA**: "Parameter-efficient is not sufficient: Exploring Parameter, Memory, and Time Efficient Adapter Tuning for Dense Predictions", arXiv, 2023 (*Alibaba + Microsoft*). [[Paper](https://arxiv.org/abs/2306.09729)]
    * **Minimax**: "Task-Robust Pre-Training for Worst-Case Downstream Adaptation", arXiv, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2306.12070)]
    * **HST**: "Hierarchical Side-Tuning for Vision Transformers", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2310.05393)][[Code (in construction)](https://github.com/AFeng-x/HST)]
    * **PELA**: "PELA: Learning Parameter-Efficient Models with Low-Rank Approximation", arXiv, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2310.10700)][[PyTorch](https://github.com/guoyang9/PELA)]
    * **Mona**: "Adapter is All You Need for Tuning Visual Tasks", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2311.15010)][[PyTorch](https://github.com/Leiyi-Hu/mona)]
    * **?**: "Label-efficient Training of Small Task-specific Models by Leveraging Vision Foundation Models", arXiv, 2023 (*Apple*). [[Paper](https://arxiv.org/abs/2311.18237)]
    * **GIFT**: "GIFT: Generative Interpretable Fine-Tuning Transformers", arXiv, 2023 (*NC State*). [[Paper](https://arxiv.org/abs/2312.00700)][[Code (in construction)](https://github.com/savadikarc/gift)]
    * **FAPFT**: "Partial Fine-Tuning: A Successor to Full Fine-Tuning for Vision Transformers", arXiv, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2312.15681)]
    * **VMT-Adapter**: "VMT-Adapter: Parameter-Efficient Transfer Learning for Multi-Task Dense", AAAI, 2024 (*Tencent*). [[Paper](https://arxiv.org/abs/2312.08733)]
    * **Dr<sup>2</sup>Net**: "Dr<sup>2</sup>Net: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning", arXiv, 2024 (*KAUST*). [[Paper](https://arxiv.org/abs/2401.04105)]
    * **ViSFT**: "Supervised Fine-tuning in turn Improves Visual Foundation Models", arXiv, 2024 (*Tencent*). [[Paper](https://arxiv.org/abs/2401.10222)][[PyTorch](https://github.com/TencentARC/ViSFT)]
* Domain Adaptation/Domain Generalization/Federated Learning:
    * **TransDA**: "Transformer-Based Source-Free Domain Adaptation", arXiv, 2021 (*Haerbin Institute of Technology*). [[Paper](https://arxiv.org/abs/2105.14138)][[PyTorch](https://github.com/ygjwd12345/TransDA)]
    * **TVT**: "TVT: Transferable Vision Transformer for Unsupervised Domain Adaptation", arXiv, 2021 (*UT Arlington + Kuaishou*). [[Paper](https://arxiv.org/abs/2108.05988)]
    * **ResTran**: "Discovering Spatial Relationships by Transformers for Domain Generalization", arXiv, 2021 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2108.10046)]
    * **WinTR**: "Exploiting Both Domain-specific and Invariant Knowledge via a Win-win Transformer for Unsupervised Domain Adaptation", arXiv, 2021 (*Beijing Institute of Technology*). [[Paper](https://arxiv.org/abs/2111.12941)]
    * **CDTrans**: "CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation", ICLR, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2109.06165)][[PyTorch](https://github.com/CDTrans/CDTrans)]
    * **SSRT**: "Safe Self-Refinement for Transformer-based Domain Adaptation", CVPR, 2022 (*Stony Brook*). [[Paper](https://arxiv.org/abs/2204.07683)]
    * **DOT**: "Making the Best of Both Worlds: A Domain-Oriented Transformer for Unsupervised Domain Adaptation", ACMMM, 2022 (*Beijing Institute of Technology*). [[Paper](https://arxiv.org/abs/2208.01195)]
    * **GVRT**: "Grounding Visual Representations with Texts for Domain Generalization", ECCV, 2022 (*LG*). [[Paper](https://arxiv.org/abs/2207.10285)][[PyTorch](https://github.com/mswzeus/GVRT)]
    * **PACMAC**: "Adapting Self-Supervised Vision Transformers by Probing Attention-Conditioned Masking Consistency", NeurIPS, 2022 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2206.08222)][[PyTorch](https://github.com/virajprabhu/PACMAC)]
    * **BCAT**: "Domain Adaptation via Bidirectional Cross-Attention Transformer", arXiv, 2022 (*Southern University of Science and Technology*). [[Paper](https://arxiv.org/abs/2201.05887)]
    * **DoTNet**: "Towards Unsupervised Domain Adaptation via Domain-Transformer", arXiv, 2022 (*Sun Yat-Sen University*). [[Paper](https://arxiv.org/abs/2202.13777)]
    * **TransDA**: "Smoothing Matters: Momentum Transformer for Domain Adaptive Semantic Segmentation", arXiv, 2022 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2203.07988)][[Code (in construction)](https://github.com/alpc91/TransDA)]
    * **FAMLP**: "FAMLP: A Frequency-Aware MLP-Like Architecture For Domain Generalization", arXiv, 2022 (*University of Science and Technology of China*). [[Paper](https://arxiv.org/abs/2203.12893)]
    * **ERM-ViT**: "Self-Distilled Vision Transformer for Domain Generalization", arXiv, 2022 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2207.12392)][[PyTorch](https://github.com/maryam089/SDViT)]
    * **MPA**: "Multi-Prompt Alignment for Multi-source Unsupervised Domain Adaptation", arXiv, 2022 (*Fudan University*). [[Paper](https://arxiv.org/abs/2209.15210)]
    * **DePT**: "Visual Prompt Tuning for Test-time Domain Adaptation", arXiv, 2022 (*Amazon*). [[Paper](https://arxiv.org/abs/2210.04831)]
    * **LADS**: "Using Language to Extend to Unseen Domains", arXiv, 2022 (*Berkeley*). [[Paper](https://arxiv.org/abs/2210.09520)]
    * **FedAPT**: "Cross-domain Federated Adaptive Prompt Tuning for CLIP", arXiv, 2022 (*Fudan University*). [[Paper](https://arxiv.org/abs/2211.07864)]
    * **MetaPrompt**: "Learning Domain Invariant Prompt for Vision-Language Models", arXiv, 2022 (*Tongji University + Microsoft*). [[Paper](https://arxiv.org/abs/2212.04196)]
    * **GMoE**: "Sparse Mixture-of-Experts are Domain Generalizable Learners", ICLR, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2206.04046)]
    * **PMTrans**: "Patch-Mix Transformer for Unsupervised Domain Adaptation: A Game Perspective", CVPR, 2023 (*HKUST*). [[Paper](https://arxiv.org/abs/2303.13434)][[Website (in construction)](https://vlis2022.github.io/cvpr23/PMTrans)]
    * **ALOFT**: "ALOFT: A Lightweight MLP-like Architecture with Dynamic Low-frequency Transform for Domain Generalization", CVPR, 2023 (*Nanjing University*). [[Paper](https://arxiv.org/abs/2303.11674)][[PyTorch](https://github.com/lingeringlight/ALOFT/)]
    * **PromptStyler**: "PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization", ICCV, 2023 (*Agency for Defense Development, Korea*). [[Paper](https://arxiv.org/abs/2307.15199)][[Website](https://promptstyler.github.io/)]
    * **DSiT**: "Domain-Specificity Inducing Transformers for Source-Free Domain Adaptation", ICCV, 2023 (*Indian Institute of Science*). [[Paper](https://arxiv.org/abs/2308.14023)][[Website](http://val.cds.iisc.ac.in/DSiT-SFDA/)]
    * **pFedPG**: "Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation", ICCV, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2308.15367)]
    * **FedPerfix**: "FedPerfix: Towards Partial Model Personalization of Vision Transformers in Federated Learning", ICCV, 2023 (*UCF*). [[Paper](https://arxiv.org/abs/2308.09160)][[Code (in construction)](https://github.com/imguangyu/FedPerfix)]
    * **RISE**: "A Sentence Speaks a Thousand Images: Domain Generalization through Distilling CLIP with Language Guidance", ICCV, 2023 (*UW-Madison*). [[Paper](https://arxiv.org/abs/2309.12530)][[PyTorch](https://github.com/OoDBag/RISE)]
    * **PØDA**: "PØDA: Prompt-driven Zero-shot Domain Adaptation", ICCV, 2023 (*INRIA*). [[Paper](https://arxiv.org/abs/2212.03241)][[PyTorch](https://github.com/astra-vision/PODA)][[Website](https://astra-vision.github.io/PODA/)]
    * **AD-CLIP**: "AD-CLIP: Adapting Domains in Prompt Space Using CLIP", ICCVW, 2023 (*IIT Bombay*). [[Paper](https://arxiv.org/abs/2308.05659)]
    * **StyLIP**: "StyLIP: Multi-Scale Style-Conditioned Prompt Learning for CLIP-based Domain Generalization", arXiv, 2023 (*TUM*). [[Paper](https://arxiv.org/abs/2302.09251)]
    * **FedCLIP**: "FedCLIP: Fast Generalization and Personalization for CLIP in Federated Learning", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2302.13485)][[PyTorch](https://github.com/microsoft/PersonalizedFL)]
    * **UniOOD**: "Universal Domain Adaptation from Foundation Models", arXiv, 2023 (*South China University of Technology*). [[Paper](https://arxiv.org/abs/2305.11092)][[Code (in construction)](https://github.com/szubing/uniood)]
    * **PEST**: "Prompt Ensemble Self-training for Open-Vocabulary Domain Adaptation", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2306.16658)]
    * **?**: "Open-Set Domain Adaptation with Visual-Language Foundation Models", arXiv, 2023 (*The University of Tokyo*). [[Paper](https://arxiv.org/abs/2307.16204)]
    * **ReCLIP**: "ReCLIP: Refine Contrastive Language Image Pre-Training with Source Free Domain Adaptation", arXiv, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2308.03793)]
    * **VPA**: "VPA: Fully Test-Time Visual Prompt Adaptation", arXiv, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2309.15251)]
    * **FedTPG**: "Text-driven Prompt Generation for Vision-Language Models in Federated Learning", arXiv, 2023 (*Bosch*). [[Paper](https://arxiv.org/abs/2310.06123)]
    * **FedLGT**: "Language-Guided Transformer for Federated Multi-Label Classification", AAAI, 2024 (*NTU*). [[Paper](https://arxiv.org/abs/2312.07165)][[Code (in construction)](https://github.com/Jack24658735/FedLGT)]
    * **LLaVO**: "Large Language Models as Visual Cross-Domain Learners", arXiv, 2024 (*Southern University of Science and Technology*). [[Paper](https://arxiv.org/abs/2401.03253)][[Website](https://ll-a-vo.github.io/)][[PyTorch](https://github.com/LL-a-VO/LLaVO)]
* X-Supervised:
    * **Semiformer**: "Semi-Supervised Vision Transformers", ECCV, 2022 (*Fudan University*). [[Paper](https://arxiv.org/abs/2111.11067)][[PyTorch](https://github.com/wengzejia1/Semiformer)]
    * **SVL-Adapter**: "SVL-Adapter: Self-Supervised Adapter for Vision-Language Pretrained Models", BMVC, 2022 (*UCL*). [[Paper](https://arxiv.org/abs/2210.03794)][[Code (in construction)](https://github.com/omipan/svl_adapter)]
    * **Semi-ViT**: "Semi-supervised Vision Transformers at Scale", NeurIPS, 2022 (*Amazon*). [[Paper](https://arxiv.org/abs/2208.05688)]
    * **DPT**: "Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few Labels", NeurIPS, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2302.10586)][[PyTorch](https://github.com/ML-GSAI/DPT)]
* Zero-Shot:
    * **ViT-ZSL**: "Multi-Head Self-Attention via Vision Transformer for Zero-Shot Learning", IMVIP, 2021 (*University of Exeter, UK*). [[Paper](https://arxiv.org/abs/2108.00045)]
    * **TransZero**: "TransZero: Attribute-guided Transformer for Zero-Shot Learning", AAAI, 2022 (*Huazhong University of Science and Technology*). [[Paper](https://arxiv.org/abs/2112.01683)][[PyTorch](https://github.com/shiming-chen/TransZero)]
    * **?**: "Zero-shot Visual Commonsense Immorality Prediction", BMVC, 2022 (*Korea University*). [[Paper](https://arxiv.org/abs/2211.05521)][[PyTorch](https://github.com/ku-vai/Zero-shot-Visual-Commonsense-Immorality-Prediction)]
    * **I2DFormer**: "I2DFormer: Learning Image to Document Attention for Zero-Shot Image Classification", NeurIPS, 2022 (*ETHZ*). [[Paper](https://arxiv.org/abs/2209.10304)]
    * **HRT**: "Hybrid Routing Transformer for Zero-Shot Learning", arXiv, 2022 (*Xidian University*). [[Paper](https://arxiv.org/abs/2203.15310)]
    * **CuPL**: "What does a platypus look like? Generating customized prompts for zero-shot image classification", arXiv, 2022 (*University of Washington*). [[Paper](https://arxiv.org/abs/2209.03320)][[PyTorch](https://github.com/sarahpratt/CuPL)]
    * **VL-Taboo**: "VL-Taboo: An Analysis of Attribute-based Zero-shot Capabilities of Vision-Language Models", arXiv, 2022 (*Goethe University Frankfurt, Germany*). [[Paper](https://arxiv.org/abs/2209.06103)][[Code (in construction)](https://github.com/felixVogel02/VL-Taboo)]
    * **CALIP**: "CALIP: Zero-Shot Enhancement of CLIP with Parameter-free Attention", arXiv, 2022 (*Peking University*). [[Paper](https://arxiv.org/abs/2209.14169)]
    * **PromptCompVL**: "Prompting Large Pre-trained Vision-Language Models For Compositional Concept Learning", arXiv, 2022 (*Michigan State*). [[Paper](https://arxiv.org/abs/2211.05077)]
    * **MUST**: "Masked Unsupervised Self-training for Zero-shot Image Classification", ICLR, 2023 (*Salesforce*). [[Paper](https://arxiv.org/abs/2206.02967)][[PyTorch](https://github.com/salesforce/MUST)]
    * **I2MVFormer**: "I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification", CVPR, 2023 (*ETHZ*). [[Paper](https://arxiv.org/abs/2212.02291)]
    * **ADE**: "Learning Attention as Disentangler for Compositional Zero-shot Learning", CVPR, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2303.15111)][[PyTorch](https://github.com/haoosz/ade-czsl)][[Website](https://haoosz.github.io/ade-czsl/)]
    * **CHiLS**: "CHiLS: Zero-Shot Image Classification with Hierarchical Label Sets", ICML, 2023 (*UCSD*). [[Paper](https://arxiv.org/abs/2302.02551)][[PyTorch](https://github.com/acmi-lab/CHILS)]
    * **CoT**: "Hierarchical Visual Primitive Experts for Compositional Zero-Shot Learning", ICCV, 2023 (*Yonsei*). [[Paper](https://arxiv.org/abs/2308.04016)][[PyTorch](https://github.com/HanjaeKim98/CoT)]
    * **diffusion-classifier**: "Your Diffusion Model is Secretly a Zero-Shot Classifier", ICCV, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2303.16203)][[PyTorch](https://github.com/diffusion-classifier/diffusion-classifier)][[Website](https://diffusion-classifier.github.io/)]
    * **SuS-X**: "SuS-X: Training-Free Name-Only Transfer of Vision-Language Models", ICCV, 2022 (*Cambridge*). [[Paper](https://arxiv.org/abs/2211.16198)][[PyTorch](https://github.com/vishaal27/SuS-X)]
    * **?**: "Text-to-Image Diffusion Models are Zero-Shot Classifiers", NeurIPS, 2023 (*DeepMind*). [[Paper](https://arxiv.org/abs/2303.15233)]
    * **AutoCLIP**: "AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models", arXiv, 2023 (*Bosch*). [[Paper](https://arxiv.org/abs/2309.16414)]
    * **MMPT**: "Prompt Tuning for Zero-shot Compositional Learning", arXiv, 2023 (*Samsung*). [[Paper](https://arxiv.org/abs/2312.02191)]
* X-Shot:
    * **CrossTransformer**: "CrossTransformers: spatially-aware few-shot transfer", NeurIPS, 2020 (*DeepMind*). [[Paper](https://arxiv.org/abs/2007.11498)][[Tensorflow](https://github.com/google-research/meta-dataset)]
    * **URT**: "A Universal Representation Transformer Layer for Few-Shot Image Classification", ICLR, 2021 (*Mila*). [[Paper](https://openreview.net/forum?id=04cII6MumYV)][[PyTorch](https://github.com/liulu112601/URT)]
    * **TRX**: "Temporal-Relational CrossTransformers for Few-Shot Action Recognition", CVPR, 2021 (*University of Bristol*). [[Paper](https://arxiv.org/abs/2101.06184)][[PyTorch](https://github.com/tobyperrett/TRX)]
    * **Few-shot-Transformer**: "Few-Shot Transformation of Common Actions into Time and Space", arXiv, 2021 (*University of Amsterdam*). [[Paper](https://arxiv.org/abs/2104.02439)]
    * **HCTransformers**: "Attribute Surrogates Learning and Spectral Tokens Pooling in Transformers for Few-shot Learning", CVPR, 2022 (*Fudan University*). [[Paper](https://arxiv.org/abs/2203.09064)][[PyTorch](https://github.com/StomachCold/HCTransformers)]
    * **HyperTransformer**: "HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning", CVPR, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2201.04182)][[PyTorch](https://github.com/wgcban/HyperTransformer)][[Website](https://www.wgcban.com/research#h.ar24vwqlm021)]
    * **STRM**: "Spatio-temporal Relation Modeling for Few-shot Action Recognition", CVPR, 2022 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2112.05132)][[PyTorch](https://github.com/Anirudh257/strm)][[Website](https://anirudh257.github.io/strm/)]
    * **HyperTransformer**: "HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning", ICML, 2022 (*Google*). [[Paper](https://proceedings.mlr.press/v162/zhmoginov22a.html)]
    * **CPM**: "Compound Prototype Matching for Few-shot Action Recognition", ECCV, 2022 (*The University of Tokyo*). [[Paper](https://arxiv.org/abs/2207.05515)]
    * **SUN**: "Self-Promoted Supervision for Few-Shot Transformer", ECCV, 2022 (*Harbin Institute of Technology + NUS*). [[Paper](https://arxiv.org/abs/2203.07057)][[PyTorch](https://github.com/DongSky/few-shot-vit)]
    * **tSF**: "tSF: Transformer-Based Semantic Filter for Few-Shot Learning", ECCV, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2211.00868)]
    * **TransVLAD**: "TransVLAD: Focusing on Locally Aggregated Descriptors for Few-Shot Learning", ECCV, 2022 (*Southern University of Science and Technology, China*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/7722_ECCV_2022_paper.php)]
    * **BaseTransformers**: "BaseTransformers: Attention over base data-points for One Shot Learning", BMVC, 2022 (*Dublin City University, Ireland*). [[Paper](https://arxiv.org/abs/2210.02476)][[PyTorch](https://github.com/mayug/BaseTransformers)]
    * **FPTrans**: "Feature-Proxy Transformer for Few-Shot Segmentation", NeurIPS, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2210.06908)][[Code (in construction)](https://github.com/Jarvis73/FPTrans)]
    * **MM-Former**: "Mask Matching Transformer for Few-Shot Segmentation", NeurIPS, 2022 (*Picsart*). [[Paper](https://openreview.net/forum?id=zt4xNo0lF8W)][[PyTorch](https://github.com/Picsart-AI-Research/Mask-Matching-Transformer)]
    * **MG-ViT**: "Mask-guided Vision Transformer (MG-ViT) for Few-Shot Learning", arXiv, 2022 (*University of Electronic Science and Technology of China*). [[Paper](https://arxiv.org/abs/2205.09995)]
    * **QSFormer**: "Few-Shot Learning Meets Transformer: Unified Query-Support Transformers for Few-Shot Classification", arXiv, 2022 (*Anhui University*). [[Paper](https://arxiv.org/abs/2208.12398)]
    * **FS-CT**: "Enhancing Few-shot Image Classification with Cosine Transformer", arXiv, 2022 (*VinUniversity, Vietnam*). [[Paper](https://arxiv.org/abs/2211.06828)][[PyTorch](https://github.com/vinuni-vishc/Few-Shot-Cosine-Transformer)]
    * **CoCa-CNI**: "Exploiting Category Names for Few-Shot Classification with Vision-Language Models", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2211.16594)]
    * **SP**: "Semantic Prompt for Few-Shot Image Recognition", CVPR, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2303.14123)]
    * **SMKD**: "Supervised Masked Knowledge Distillation for Few-Shot Transformers", CVPR, 2023 (*Columbia*). [[Paper](https://arxiv.org/abs/2303.15466)][[PyTorch](https://github.com/HL-hanlin/SMKD)]
    * **CST**: "Distilling Self-Supervised Vision Transformers for Weakly-Supervised Few-Shot Classification & Segmentation", CVPR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2307.03407)]
    * **Hint-Aug**: "Hint-Aug: Drawing Hints from Foundation Vision Transformers Towards Boosted Few-Shot Parameter-Efficient Tuning", CVPR, 2023 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2304.12520)]
    * **ProD**: "ProD: Prompting-To-Disentangle Domain Knowledge for Cross-Domain Few-Shot Image Classification", CVPR, 2023 (*University of Technology Sydney*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Ma_ProD_Prompting-To-Disentangle_Domain_Knowledge_for_Cross-Domain_Few-Shot_Image_Classification_CVPR_2023_paper.html)]
    * **PVP**: "PVP: Pre-trained Visual Parameter-Efficient Tuning", arXiv, 2023 (*Defense Innovation Institute, China*). [[Paper](https://arxiv.org/abs/2304.13639)]
* Continual Learning:
    * **MEAT**: "Meta-attention for ViT-backed Continual Learning", CVPR, 2022 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2203.11684)][[Code (in construction)](https://github.com/zju-vipa/MEAT-TIL)]
    * **DyTox**: "DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion", CVPR, 2022 (*Sorbonne Universite, France*). [[Paper](https://arxiv.org/abs/2111.11326)][[PyTorch](https://github.com/arthurdouillard/dytox)]
    * **LVT**: "Continual Learning With Lifelong Vision Transformer", CVPR, 2022 (*The University of Sydney*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Continual_Learning_With_Lifelong_Vision_Transformer_CVPR_2022_paper.html)]
    * **L2P**: "Learning to Prompt for Continual Learning", CVPR, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2112.08654)][[Tensorflow](https://github.com/google-research/l2p)]
    * **?**: "Simpler is Better: off-the-shelf Continual Learning Through Pretrained Backbones", CVPRW, 2022 (*Ca' Foscari University, Italy*). [[Paper](https://arxiv.org/abs/2205.01586)][[PyTorch](https://github.com/francesco-p/off-the-shelf-cl)]
    * **ADA**: "Continual Learning with Transformers for Image Classification", CVPRW, 2022 (*Amazon*). [[Paper](https://arxiv.org/abs/2206.14085)]
    * **?**: "Towards Exemplar-Free Continual Learning in Vision Transformers: an Account of Attention, Functional and Weight Regularization", CVPRW, 2022 (*Ca' Foscari University, Italy*). [[Paper](https://arxiv.org/abs/2203.13167)]
    * **DualPrompt**: "DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning", ECCV, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2204.04799)][[Tensorflow](https://github.com/google-research/l2p)]
    * **CVT**: "Online Continual Learning with Contrastive Vision Transformer", ECCV, 2022 (*The University of Sydney*). [[Paper](https://arxiv.org/abs/2207.13516)]
    * **IncCLIP**: "Generative Negative Text Replay for Continual Vision-Language Pretraining", ECCV, 2022 (*ShanghaiTech*). [[Paper](https://arxiv.org/abs/2210.17322)]
    * **S-Prompts**: "S-Prompts Learning with Pre-trained Transformers: An Occam's Razor for Domain Incremental Learning", NeurIPS, 2022 (*Singapore Management University*). [[Paper](https://arxiv.org/abs/2207.12819)]
    * **ADA**: "Memory Efficient Continual Learning with Transformers", NeurIPS, 2022 (*Amazon*). [[Paper](https://openreview.net/forum?id=U07d1Y-x2E)]
    * **BMU-MoCo**: "BMU-MoCo: Bidirectional Momentum Update for Continual Video-Language Modeling", NeurIPS, 2022 (*Renmin University of China*). [[Paper](https://openreview.net/forum?id=H5z5Q--YdYd)]
    * **CLiMB**: "CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks", NeurIPS (Datasets and Benchmarks), 2022 (*USC*). [[Paper](https://arxiv.org/abs/2206.09059)][[PyTorch](https://github.com/GLAMOR-USC/CLiMB)]
    * **COLT**: "Transformers Are Better Continual Learners", arXiv, 2022 (*Hikvision*). [[Paper](https://arxiv.org/abs/2201.04924)]
    * **D<sup>3</sup>Former**: "D<sup>3</sup>Former: Debiased Dual Distilled Transformer for Incremental Learning", arXiv, 2022 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2208.00777)][[PyTorch](https://github.com/abdohelmy/D-3Former)]
    * **Continual-CLIP**: "CLIP model is an Efficient Continual Learner", arXiv, 2022 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2210.03114)][[Code (in construction)](https://github.com/vgthengane/Continual-CLIP)]
    * **GCAB-CFDC**: "Gated Class-Attention with Cascaded Feature Drift Compensation for Exemplar-free Continual Learning of Vision Transformers", arXiv, 2022 (*University of Pavia, Italy*). [[Paper](https://arxiv.org/abs/2211.12292)][[Code (in construction)](https://github.com/OcraM17/GCAB-CFDC)]
    * **CODA-Prompt**: "CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning", arXiv, 2022 (*IBM*). [[Paper](https://arxiv.org/abs/2211.13218)]
    * **PIVOT**: "PIVOT: Prompting for Video Continual Learning", arXiv, 2022 (*KAUST*). [[Paper](https://arxiv.org/abs/2212.04842)]
    * **AttriCLIP**: "AttriCLIP: A Non-Incremental Learner for Incremental Knowledge Learning", CVPR, 2023 (*Beihang University*). [[Paper](https://arxiv.org/abs/2305.11488)][[PyTorch](https://github.com/bhrqw/AttriCLIP)]
    * **DKT**: "DKT: Diverse Knowledge Transfer Transformer for Class Incremental Learning", CVPR, 2023 (*Xi'an Jiaotong*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Gao_DKT_Diverse_Knowledge_Transfer_Transformer_for_Class_Incremental_Learning_CVPR_2023_paper.html)][[Code (in construction)](https://github.com/MIV-XJTU/DKT)]
    * **CODA-Prompt**: "CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning", CVPR, 2023 (*IBM*). [[Paper](https://arxiv.org/abs/2211.13218)][[PyTorch](https://github.com/GT-RIPL/CODA-Prompt)]
    * **BiRT**: "BiRT: Bio-inspired Replay in Vision Transformers for Continual Learning", ICML, 2023 (*NavInfo, Netherlands*). [[Paper](https://arxiv.org/abs/2305.04769)]
    * **CLR**: "CLR: Channel-wise Lightweight Reprogramming for Continual Learning", ICCV, 2023 (*USC*). [[Paper](https://arxiv.org/abs/2307.11386)][[Code (in construction)](https://github.com/gyhandy/Channel-wise-Lightweight-Reprogramming)]
    * **CTP**: "CTP: Towards Vision-Language Continual Pretraining via Compatible Momentum Contrast and Topology Preservation", ICCV, 2023 (*Peng Cheng Lab*). [[Paper](https://arxiv.org/abs/2308.07146)][[PyTorch](https://github.com/KevinLight831/CTP)]
    * **APG**: "When Prompt-based Incremental Learning Does Not Meet Strong Pretraining", ICCV, 2023 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2308.10445)][[PyTorch](https://github.com/TOM-tym/APG)]
    * **MAE-CIL**: "Masked Autoencoders are Efficient Class Incremental Learners", ICCV, 2023 (*Nankai University*). [[Paper](https://arxiv.org/abs/2308.12510)][[PyTorch (in construction)](https://github.com/scok30/MAE-CIL)]
    * **ConTraCon**: "Exemplar-Free Continual Transformer with Convolutions", ICCV, 2023 (*IIT Kharagpur*). [[Paper](https://arxiv.org/abs/2308.11357)][[PyTorch](https://github.com/CVIR/contracon)][[Website](https://cvir.github.io/projects/contracon)]
    * **LGCL**: "Introducing Language Guidance in Prompt-based Continual Learning", ICCV, 2023 (*RPTU Kaiserslautern, Germany*). [[Paper](https://arxiv.org/abs/2308.15827)]
    * **MVP**: "Online Class Incremental Learning on Stochastic Blurry Task Boundary via Mask and Visual Prompt Tuning", ICCV, 2023 (*Kyung Hee University, Korea*). [[Paper](https://arxiv.org/abs/2308.09303)][[PyTorch](https://github.com/moonjunyyy/Si-Blurry)]
    * **ZSCL**: "Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models", ICCV, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2303.06628)][[PyTorch](https://github.com/Thunderbeee/ZSCL)]
    * **C-LN**: "On the Effectiveness of LayerNorm Tuning for Continual Learning in Vision Transformers", ICCVW, 2023 (*University of Trento*). [[Paper](https://arxiv.org/abs/2308.09610)][[PyTorch](https://github.com/tdemin16/Continual-LayerNorm-Tuning)]
    * **PromptFusion**: "PromptFusion: Decoupling Stability and Plasticity for Continual Learning", arXiv, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2303.07223)]
    * **MSc-iNCD**: "Large-scale Pre-trained Models are Surprisingly Strong in Incremental Novel Class Discovery", arXiv, 2023 (*University of Trento, Italy*). [[Paper](https://arxiv.org/abs/2303.15975)][[PyTorch (in construction)](https://github.com/OatmealLiu/MSc-iNCD)]
    * **PROOF**: "Learning without Forgetting for Vision-Language Models", arXiv, 2023 (*Nanjing University*). [[Paper](https://arxiv.org/abs/2305.19270)]
    * **HePCo**: "HePCo: Data-Free Heterogeneous Prompt Consolidation for Continual Federated Learning", arXiv, 2023 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2306.09970)]
    * **?**: "Continual Learning in Open-vocabulary Classification with Complementary Memory Systems", arXiv, 2023 (*UIUC*). [[Paper](https://arxiv.org/abs/2307.01430)]
    * **MoP-CLIP**: "MoP-CLIP: A Mixture of Prompt-Tuned CLIP Models for Domain Incremental Learning", arXiv, 2023 (*ETS Montreal, Canada*). [[Paper](https://arxiv.org/abs/2307.05707)]
    * **TiC-CLIP**: "TiC-CLIP: Continual Training of CLIP Models", arXiv, 2023 (*Apple*). [[Paper](https://arxiv.org/abs/2310.16226)]
    * **TIER**: "Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning", AAAI, 2024 (**). [[Paper](https://arxiv.org/abs/2401.17186)]
* Long-tail/Imbalanced:
    * **BatchFormer**: "BatchFormer: Learning to Explore Sample Relationships for Robust Representation Learning", CVPR, 2022 (*The University of Sydney*). [[Paper](https://arxiv.org/abs/2203.01522)][[PyTorch](https://github.com/zhihou7/BatchFormer)]
    * **BatchFormerV2**: "BatchFormerV2: Exploring Sample Relationships for Dense Representation Learning", arXiv, 2022 (*The University of Sydney*). [[Paper](https://arxiv.org/abs/2204.01254)]
    * **LPT**: "LPT: Long-tailed Prompt Tuning for Image Classification", ICLR, 2023 (*Harbin Institute of Technology*). [[Paper](https://arxiv.org/abs/2210.01033)]
    * **PDC**: "Rethink Long-tailed Recognition with Vision Transforms", ICASSP, 2023 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2302.14284)]
    * **?**: "Exploring Vision-Language Models for Imbalanced Learning", arXiv, 2023 (*Peking University*). [[Paper](https://arxiv.org/abs/2304.01457)][[PyTorch](https://github.com/Imbalance-VLM/Imbalance-VLM)]
    * **LMPT**: "LMPT: Prompt Tuning with Class-Specific Embedding Loss for Long-tailed Multi-Label Visual Recognition", arXiv, 2023 (*Monash University, Australia*). [[Paper](https://arxiv.org/abs/2305.04536)][[PyTorch](https://github.com/richard-peng-xia/LMPT)]
* Knowledge Distillation:
    * **?**: "Knowledge Distillation via the Target-aware Transformer", CVPR, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2205.10793)]
    * **DearKD**: "DearKD: Data-Efficient Early Knowledge Distillation for Vision Transformers", CVPR, 2022 (*JD*). [[Paper](https://arxiv.org/abs/2204.12997)]
    * **AttnDistill**: "Attention Distillation: self-supervised vision transformer students need more guidance", BMVC, 2022 (*UAB, Spain*). [[Paper](https://arxiv.org/abs/2210.00944)][[PyTorch](https://github.com/wangkai930418/attndistill)]
    * **ViTKD**: "ViTKD: Practical Guidelines for ViT feature knowledge distillation", arXiv, 2022 (*IDEA*). [[Paper](https://arxiv.org/abs/2209.02432)][[PyTorch (in construction)](https://github.com/yzd-v/cls_KD)]
    * **?**: "Adaptive Attention Link-based Regularization for Vision Transformers", arXiv, 2022 (*Chung-Ang University, Korea*). [[Paper](https://arxiv.org/abs/2211.13852)]
    * **LiVT**: "Learning Imbalanced Data with Vision Transformers", CVPR, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2212.02015)][[PyTorch](https://github.com/XuZhengzhuo/LiVT)]
    * **G2SD**: "Generic-to-Specific Distillation of Masked Autoencoders", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2302.14771)][[PyTorch](https://github.com/pengzhiliang/G2SD)]
    * **SLaK**: "Are Large Kernels Better Teachers than Transformers for ConvNets?", ICML, 2023 (*Eindhoven University of Technology, Netherlands*). [[Paper] (https://arxiv.org/abs/2305.19412)][[PyTorch](https://github.com/VITA-Group/SLaK)]
    * **CSKD**: "Cumulative Spatial Knowledge Distillation for Vision Transformers", ICCV, 2023 (*Megvii*). [[Paper](https://arxiv.org/abs/2307.08500)]
    * **TinyCLIP**: "TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance", ICCV, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2309.12314)][[PyTorch](https://github.com/microsoft/Cream/tree/main/TinyCLIP)]
    * **DIME-FM**: "DIME-FM: DIstilling Multimodal and Efficient Foundation Models", ICCV, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2303.18232)]
    * **MaskedKD**: "MaskedKD: Efficient Distillation of Vision Transformers with Masked Images", arXiv, 2023 (*POSTECH*). [[Paper](https://arxiv.org/abs/2302.10494)]
    * **AM-RADIO**: "AM-RADIO: Agglomerative Model -- Reduce All Domains Into One", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2312.06709)]
* Clustering:
    * **VTCC**: "Vision Transformer for Contrastive Clustering", arXiv, 2022 (*Sun Yat-sen University, China*). [[Paper](https://arxiv.org/abs/2206.12925)]
* Novel Category Discovery:
    * **PromptCAL**: "PromptCAL: Contrastive Affinity Learning via Auxiliary Prompts for Generalized Novel Category Discovery", CVPR, 2023 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2212.05590)][[PyTorch](https://github.com/sheng-eatamath/PromptCAL)]
    * **CLIP-GCD**: "CLIP-GCD: Simple Language Guided Generalized Category Discovery", arXiv, 2023 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2305.10420)]

[[Back to Overview](#overview)]


## Low-level Vision Tasks
### Image Restoration 
* General:
    * **NLRN**: "Non-Local Recurrent Network for Image Restoration", NeurIPS, 2018 (*UIUC*). [[Paper](https://arxiv.org/abs/1806.02919)][[Tensorflow](https://github.com/Ding-Liu/NLRN)]
    * **RNAN**: "Residual Non-local Attention Networks for Image Restoration", ICLR, 2019 (*Northeastern University*). [[Paper](https://openreview.net/forum?id=HkeGhoA5FX)][[PyTorch](https://github.com/yulunzhang/RNAN)]
    * **PANet**: "Pyramid Attention Networks for Image Restoration", arXiv, 2020 (*UIUC*). [[Paper](https://arxiv.org/abs/2004.13824)][[PyTorch](https://github.com/SHI-Labs/Pyramid-Attention-Networks)]
    * **IPT**: "Pre-Trained Image Processing Transformer", CVPR, 2021 (*Huawei*). [[Paper](https://arxiv.org/abs/2012.00364)][[PyTorch (in construction)](https://github.com/huawei-noah/Pretrained-IPT)]
    * **SwinIR**: "SwinIR: Image Restoration Using Swin Transformer", ICCVW, 2021 (*ETHZ*). [[Paper](https://arxiv.org/abs/2108.10257)][[PyTorch](https://github.com/JingyunLiang/SwinIR)]
    * **SiamTrans**: "SiamTrans: Zero-Shot Multi-Frame Image Restoration with Pre-Trained Siamese Transformers", AAAI, 2022 (*Huawei*). [[Paper](https://arxiv.org/abs/2112.09426)]
    * **Uformer**: "Uformer: A General U-Shaped Transformer for Image Restoration", CVPR, 2022 (*University of Science and Technology of China*). [[Paper](https://arxiv.org/abs/2106.03106)][[PyTorch](https://github.com/ZhendongWang6/Uformer)]
    * **MAXIM**: "MAXIM: Multi-Axis MLP for Image Processing", CVPR, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2201.02973)][[Tensorflow](https://github.com/google-research/maxim)]
    * **Restormer**: "Restormer: Efficient Transformer for High-Resolution Image Restoration", CVPR, 2022 (*IIAI, UAE*). [[Paper](https://arxiv.org/abs/2111.09881)][[PyTorch](https://github.com/swz30/Restormer)]
    * **TransWeather**: "TransWeather: Transformer-based Restoration of Images Degraded by Adverse Weather Conditions", CVPR, 2022 (*JHU*). [[Paper](https://arxiv.org/abs/2111.14813)][[PyTorch](https://github.com/jeya-maria-jose/TransWeather)][[Website](https://jeya-maria-jose.github.io/transweather-web/)]
    * **KiT**: "KNN Local Attention for Image Restoration", CVPR, 2022 (*Yonsei University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Lee_KNN_Local_Attention_for_Image_Restoration_CVPR_2022_paper.html)]
    * **ELMformer**: "ELMformer: Efficient Raw Image Restoration with a Locally Multiplicative Transformer", ACMMM, 2022 (*Horizon Robotics*). [[Paper](https://arxiv.org/abs/2208.14704)][[Code (in construction)](https://github.com/leonmakise/ELMformer)]
    * **EDT**: "On Efficient Transformer-Based Image Pre-training for Low-Level Vision", arXiv, 2022 (*CUHK*). [[Paper](https://arxiv.org/abs/2112.10175)][[PyTorch](https://github.com/fenglinglwb/EDT)]
    * **?**: "Transform your Smartphone into a DSLR Camera: Learning the ISP in the Wild", arXiv, 2022 (*ETHZ*). [[Paper](https://arxiv.org/abs/2203.10636)]
    * **TMT**: "Imaging through the Atmosphere using Turbulence Mitigation Transformer", arXiv, 2022 (*Purdue*). [[Paper](https://arxiv.org/abs/2207.06465)][[Code (in construction)](https://github.com/xg416/TMT)][[Website](https://xg416.github.io/TMT/)]
    * **LRT**: "LRT: An Efficient Low-Light Restoration Transformer for Dark Light Field Images", arXiv, 2022 (*HKU*). [[Paper](https://arxiv.org/abs/2209.02197)]
    * **ART**: "Accurate Image Restoration with Attention Retractable Transformer", ICLR, 2023 (*Shanghai Jiao Tong University*). [[Paper](https://arxiv.org/abs/2210.01427)][[PyTorch](https://github.com/gladzhang/ART)]
    * **Burstormer**: "Burstormer: Burst Image Restoration and Enhancement Transformer", CVPR, 2023 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2304.01194)][[Code (in construction)](https://github.com/akshaydudhane16/Burstormer)]
    * **?**: "Comprehensive and Delicate: An Efficient Transformer for Image Restoration", CVPR, 2023 (*Sichuan University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_Comprehensive_and_Delicate_An_Efficient_Transformer_for_Image_Restoration_CVPR_2023_paper.html)]
    * **ShuffleFormer**: "Random Shuffle Transformer for Image Restoration", ICML, 2023 (*USTC*). [[Paper](https://openreview.net/forum?id=7RIjvZfceF)][[PyTorch (in construction)](https://github.com/jiexiaou/ShuffleFormer)]
    * **PromptIR**: "PromptIR: Prompting for All-in-One Blind Image Restoration", NeurIPS, 2023 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2306.13090)][[PyTorch](https://github.com/va1shn9v/PromptIR)]
    * **UCDIR**: "A Unified Conditional Framework for Diffusion-based Image Restoration", NeurIPS, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2305.20049)][[Code (in construction)](https://github.com/zhangyi-3/UCDIR)][[Website](https://zhangyi-3.github.io/project/UCDIR/)]
    * **MAEIP**: "Masked Autoencoders as Image Processors", arXiv, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2303.17316)][[Code (in construction)](https://github.com/DuanHuiyu/MAEIP_CSformer)]
    * **RAMiT**: "RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image Restoration", arXiv, 2023 (*Sogang University, Korea*). [[Paper](https://arxiv.org/abs/2305.11474)]
    * **RAP**: "Restore Anything Pipeline: Segment Anything Meets Image Restoration", arXiv, 2023 (*ETHZ*). [[Paper](https://arxiv.org/abs/2305.13093)][[Code (in construction)](https://github.com/eth-siplab/RAP)]
    * **ProRes**: "ProRes: Exploring Degradation-aware Visual Prompt for Universal Image Restoration", arXiv, 2023 (*Horizon Robotics*). [[Paper](https://arxiv.org/abs/2306.13653)][[PyTorch (in construction)](https://github.com/leonmakise/ProRes)]
    * **C2F-DFT**: "Learning A Coarse-to-Fine Diffusion Transformer for Image Restoration", arXiv, 2023 (*Dalian University of Technology*). [[Paper](https://arxiv.org/abs/2308.08730)][[PyTorch (in construction)](https://github.com/wlydlut/C2F-DFT)]
    * **DA-CLIP**: "Controlling Vision-Language Models for Universal Image Restoration", arXiv, 2023 (*Uppsala University, Sweden*). [[Paper](https://arxiv.org/abs/2310.01018)][[PyTorch](https://github.com/Algolzw/daclip-uir)][[Website](https://algolzw.github.io/daclip-uir/index.html)]
    * **AutoDIR**: "AutoDIR: Automatic All-in-One Image Restoration with Latent Diffusion", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2310.10123)]
    * **MPerceiver**: "Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and Fidelity for All-in-One Image Restoration", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2312.02918)]
    * **TIP**: "TIP: Text-Driven Image Processing with Semantic and Restoration Instructions", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2312.11595)][[Website](https://chenyangqiqi.github.io/tip/)]
* Super-Resolution:
    * **SAN**: "Second-Order Attention Network for Single Image Super-Resolution", CVPR, 2019 (*Tsinghua*). [[Paper](https://openaccess.thecvf.com/content_CVPR_2019/html/Dai_Second-Order_Attention_Network_for_Single_Image_Super-Resolution_CVPR_2019_paper.html)][[PyTorch](https://github.com/daitao/SAN)]
    * **CS-NL**: "Image Super-Resolution with Cross-Scale Non-Local Attention and Exhaustive Self-Exemplars Mining", CVPR, 2020 (*UIUC*). [[Paper](https://arxiv.org/abs/2006.01424)][[PyTorch](https://github.com/SHI-Labs/Cross-Scale-Non-Local-Attention)]
    * **TTSR**: "Learning Texture Transformer Network for Image Super-Resolution", CVPR, 2020 (*Microsoft*). [[Paper](https://arxiv.org/abs/2006.04139)][[PyTorch](https://github.com/researchmm/TTSR)]
    * **HAN**: "Single Image Super-Resolution via a Holistic Attention Network", ECCV, 2020 (*Northeastern University*). [[Paper](https://arxiv.org/abs/2008.08767)][[PyTorch](https://github.com/wwlCape/HAN)]
    * **NLSN**: "Image Super-Resolution With Non-Local Sparse Attention", CVPR, 2021 (*UIUC*). [[Paper](https://openaccess.thecvf.com/content/CVPR2021/html/Mei_Image_Super-Resolution_With_Non-Local_Sparse_Attention_CVPR_2021_paper.html)]
    * **ITSRN**: "Implicit Transformer Network for Screen Content Image Continuous Super-Resolution", NeurIPS, 2021 (*Tianjin University*). [[Paper](https://arxiv.org/abs/2112.06174)][[PyTorch](https://github.com/codyshen0000/ITSRN)]
    * **FPAN**: "Feedback Pyramid Attention Networks for Single Image Super-Resolution", arXiv, 2021 (*Nanjing University of Science and Technology*). [[Paper](https://arxiv.org/abs/2106.06966)]
    * **ESRT**: "Efficient Transformer for Single Image Super-Resolution", arXiv, 2021 (*Peking University*). [[Paper](https://arxiv.org/abs/2108.11084)]
    * **Fusformer**: "Fusformer: A Transformer-based Fusion Approach for Hyperspectral Image Super-resolution", arXiv, 2021 (*University of Electronic Science and Technology of China*). [[Paper](https://arxiv.org/abs/2109.02079)]
    * **DPT**: "Detail-Preserving Transformer for Light Field Image Super-Resolution", AAAI, 2022 (*Beijing Institute of Technology*). [[Paper](https://arxiv.org/abs/2201.00346)][[PyTorch](https://github.com/BITszwang/DPT)]
    * **BSRT**: "BSRT: Improving Burst Super-Resolution with Swin Transformer and Flow-Guided Deformable Alignment", CVPRW, 2022 (*Megvii*). [[Paper](https://arxiv.org/abs/2204.08332)][[PyTorch](https://github.com/Algolzw/BSRT)]
    * **TATT**: "A Text Attention Network for Spatial Deformation Robust Scene Text Image Super-resolution", CVPR, 2022 (*The Hong Kong Polytechnic University*). [[Paper](https://arxiv.org/abs/2203.09388)][[PyTorch](https://github.com/mjq11302010044/TATT)]
    * **LBNet**: "Lightweight Bimodal Network for Single-Image Super-Resolution via Symmetric CNN and Recursive Transformer", IJCAI, 2022 (*Nanjing University of Posts and Telecommunications*). [[Paper](https://arxiv.org/abs/2204.13286)][[PyTorch (in construction)](https://github.com/IVIPLab/LBNet)]
    * **DATSR**: "Reference-based Image Super-Resolution with Deformable Attention Transformer", ECCV, 2022 (*ETHZ*). [[Paper](https://arxiv.org/abs/2207.11938)][[Code (in construction)](https://github.com/caojiezhang/DATSR)]
    * **ELAN**: "Efficient Long-Range Attention Network for Image Super-resolution", ECCV, 2022 (*The Hong Kong Polytechnic University*). [[Paper](https://arxiv.org/abs/2203.06697)][[PyTorch](https://github.com/xindongzhang/ELAN)]
    * **Swin2SR**: "Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration", ECCVW, 2022 (*University of Wurzburg, Germany*). [[Paper](https://arxiv.org/abs/2209.11345)]
    * **CAT**: "Cross Aggregation Transformer for Image Restoration", NeurIPS, 2022 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2211.13654)][[PyTorch](https://github.com/zhengchen1999/CAT)]
    * **Stoformer**: "Stochastic Window Transformer for Image Restoration", NeurIPS, 2022 (*USTC*). [[Paper](https://openreview.net/forum?id=0ucMtEKCihU)][[PyTorch](https://github.com/jiexiaou/Stoformer)]
    * **LFT**: "Light Field Image Super-Resolution with Transformers", IEEE Signal Processing Letters, 2022 (*National University of Defense Technology, China*). [[Paper](https://arxiv.org/abs/2108.07597)][[PyTorch](https://github.com/ZhengyuLiang24/LFT)]
    * **ELAN**: "Efficient Long-Range Attention Network for Image Super-resolution", arXiv, 2022 (*The Hong Kong Polytechnic University*). [[Paper](https://arxiv.org/abs/2203.06697)][[Code (in construction)](https://github.com/xindongzhang/ELAN)]
    * **ACT**: "Rich CNN-Transformer Feature Aggregation Networks for Super-Resolution", arXiv, 2022 (*LG*). [[Paper](https://arxiv.org/abs/2203.07682)]
    * **HIPA**: "HIPA: Hierarchical Patch Transformer for Single Image Super Resolution", arXiv, 2022 (*CUHK*). [[Paper](https://arxiv.org/abs/2203.10247)]
    * **CTCNet**: "CTCNet: A CNN-Transformer Cooperation Network for Face Image Super-Resolution", arXiv, 2022 (*Nanjing University of Posts and Telecommunications*). [[Paper](https://arxiv.org/abs/2204.08696)]
    * **ShuffleMixer**: "ShuffleMixer: An Efficient ConvNet for Image Super-Resolution", arXiv, 2022 (*Nanjing University of Science and Technology*). [[Paper](https://arxiv.org/abs/2205.15175)][[PyTorch](https://github.com/sunny2109/MobileSR-NTIRE2022)]
    * **HST**: "HST: Hierarchical Swin Transformer for Compressed Image Super-resolution", ECCVW, 2022 (*USTC*). [[Paper](https://arxiv.org/abs/2208.09885)]
    * **SwinFIR**: "SwinFIR: Revisiting the SwinIR with Fast Fourier Convolution and Improved Training for Image Super-Resolution", arXiv, 2022 (*Samsung*). [[Paper](https://arxiv.org/abs/2208.11247)]
    * **ITSRN++**: "ITSRN++: Stronger and Better Implicit Transformer Network for Continuous Screen Content Image Super-Resolution", arXiv, 2022 (*Tianjin University*). [[Paper](https://arxiv.org/abs/2210.08812)]
    * **NGswin**: "N-Gram in Swin Transformers for Efficient Lightweight Image Super-Resolution", CVPR, 2023 (*Sogang University, Korea*). [[Paper](https://arxiv.org/abs/2211.11436)][[PyTorch](https://github.com/rami0205/NGramSwin)]
    * **OSRT**: "OSRT: Omnidirectional Image Super-Resolution with Distortion-aware Transformer", CVPR, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2302.03453)]
    * **HAT**: "Activating More Pixels in Image Super-Resolution Transformer", CVPR, 2023 (*University of Macau*). [[Paper](https://arxiv.org/abs/2205.04437)][[PyTorch](https://github.com/chxy95/HAT)]
    * **CLIT**: "Cascaded Local Implicit Transformer for Arbitrary-Scale Super-Resolution", CVPR, 2023 (*MediaTek*). [[Paper](https://arxiv.org/abs/2303.16513)]
    * **CiaoSR**: "CiaoSR: Continuous Implicit Attention-in-Attention Network for Arbitrary-Scale Image Super-Resolution", CVPR, 2023 (*ETHZ*). [[Paper](https://arxiv.org/abs/2212.04362)][[PyTorch](https://github.com/caojiezhang/CiaoSR)]
    * **HTCAN**: "Hybrid Transformer and CNN Attention Network for Stereo Image Super-resolution", CVPRW, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2305.05177)]
    * **DAT**: "Dual Aggregation Transformer for Image Super-Resolution", ICCV, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2308.03364)][[PyTorch](https://github.com/zhengchen1999/DAT)]
    * **CRAFT**: "Feature Modulation Transformer: Cross-Refinement of Global Representation via High-Frequency Prior for Image Super-Resolution", ICCV, 2023 (*UESTC*). [[Paper](https://arxiv.org/abs/2308.05022)][[Code (in construction)](https://github.com/AVC2-UESTC/CRAFT-SR)]
    * **ESSAformer**: "ESSAformer: Efficient Transformer for Hyperspectral Image Super-resolution", ICCV, 2023 (*Xidian University*). [[Paper](https://arxiv.org/abs/2307.14010)][[PyTorch](https://github.com/Rexzhan/ESSAformer)]
    * **SRFormer**: "SRFormer: Permuted Self-Attention for Single Image Super-Resolution", ICCV, 2023 (*Nankai University*). [[Paper](https://arxiv.org/abs/2303.09735)]
    * **ResShift**: "ResShift: Efficient Diffusion Model for Image Super-resolution by Residual Shifting", NeurIPS, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2307.12348)][[PyTorch](https://github.com/zsyOAOA/ResShift)][[Website](https://zsyoaoa.github.io/projects/resshift/)]
    * **RGT**: "Recursive Generalization Transformer for Image Super-Resolution", arXiv, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2303.06373)]
    * **SOSR**: "SOSR: Source-Free Image Super-Resolution with Wavelet Augmentation Transformer", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2303.17783)]
    * **HAT**: "HAT: Hybrid Attention Transformer for Image Restoration", arXiv, 2023 (*University of Macau*). [[Paper](https://arxiv.org/abs/2309.05239)][[PyTorch](https://github.com/XPixelGroup/HAT)]
    * **PromptSR**: "Image Super-Resolution with Text Prompt Diffusion", arXiv, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2311.14282)][[Code (in construction)](https://github.com/zhengchen1999/PromptSR)]
* Others:
    * **SDNet**: "SDNet: multi-branch for single image deraining using swin", arXiv, 2021 (*Xinjiang University*). [[Paper](https://arxiv.org/abs/2105.15077)][[Code (in construction)](https://github.com/H-tfx/SDNet)]
    * **ATTSF**: "Attention! Stay Focus!", arXiv, 2021 (*BridgeAI, Seoul*). [[Paper](https://arxiv.org/abs/2104.07925)][[Tensorflow](https://github.com/tuvovan/ATTSF)]
    * **HyLoG-ViT**: "Hybrid Local-Global Transformer for Image Dehazing", arXiv, 2021 (*Beihang University*). [[Paper](https://arxiv.org/abs/2109.07100)]
    * **HyperTransformer**: "HyperTransformer: A Textural and Spectral Feature Fusion Transformer for Pansharpening", CVPR, 2022 (*JHU*). [[Paper](https://arxiv.org/abs/2203.02503)][[PyTorch](https://github.com/wgcban/HyperTransformer)]
    * **DeHamer**: "Image Dehazing Transformer With Transmission-Aware 3D Position Embedding", CVPR, 2022 (*Nankai University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Guo_Image_Dehazing_Transformer_With_Transmission-Aware_3D_Position_Embedding_CVPR_2022_paper.html)][[Website](https://li-chongyi.github.io/Proj_DeHamer.html)]
    * **PTNet**: "Learning Parallax Transformer Network for Stereo Image JPEG Artifacts Removal", ACMMM, 2022 (*Fudan University*). [[Paper](https://arxiv.org/abs/2207.07335)]
    * **CharFormer**: "CharFormer: A Glyph Fusion based Attentive Framework for High-precision Character Image Denoising", ACMMM, 2022 (*Jilin University*). [[Paper](https://arxiv.org/abs/2207.07798)][[PyTorch (in construction)](https://github.com/daqians/CharFormer)]
    * **TurbNet**: "Single Frame Atmospheric Turbulence Mitigation: A Benchmark Study and A New Physics-Inspired Transformer Model", ECCV, 2022 (*Purdue + UT Austin*). [[Paper](https://arxiv.org/abs/2207.10040)][[PyTorch](https://github.com/VITA-Group/TurbNet)]
    * **Stripformer**: "Stripformer: Strip Transformer for Fast Image Deblurring", ECCV, 2022 (*NTHU*). [[Paper](https://arxiv.org/abs/2204.04627)]
    * **DehazeFormer**: "Vision Transformers for Single Image Dehazing", arXiv, 2022 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2204.03883)][[PyTorch](https://github.com/IDKiro/DehazeFormer)]
    * **RSTCANet**: "Residual Swin Transformer Channel Attention Network for Image Demosaicing", arXiv, 2022 (*Tampere University, Finland*). [[Paper](https://arxiv.org/abs/2204.07098)]
    * **DRT**: "DRT: A Lightweight Single Image Deraining Recursive Transformer", arXiv, 2022 (*ANU, Australia*). [[Paper](https://arxiv.org/abs/2204.11385)][[PyTorch (in construction)](https://github.com/YC-Liang/DRT)]
    * **DenSformer**: "Dense residual Transformer for image denoising", arXiv, 2022 (*University of Science and Technology Beijing*). [[Paper](https://arxiv.org/abs/2205.06944)]
    * **Cubic-Mixer**: "UHD Image Deblurring via Multi-scale Cubic-Mixer", arXiv, 2022 (*Nanjing University of Science and Technology*). [[Paper](https://arxiv.org/abs/2206.03678)]
    * **PoCoformer**: "Polarized Color Image Denoising using Pocoformer", arXiv, 2022 (*The University of Tokyo*). [[Paper](https://arxiv.org/abs/2207.00215)]
    * **MSP-Former**: "MSP-Former: Multi-Scale Projection Transformer for Single Image Desnowing", arXiv, 2022 (*Jimei University*). [[Paper](https://arxiv.org/abs/2207.05621)]
    * **ELF**: "Magic ELF: Image Deraining Meets Association Learning and Transformer", arXiv, 2022 (*Wuhan University*). [[Paper](https://arxiv.org/abs/2207.10455)][[PyTorch (in construction)](https://github.com/kuijiang94/Magic-ELF)]
    * **DnSwin**: "DnSwin: Toward Real-World Denoising via Continuous Wavelet Sliding-Transformer", arXiv, 2022 (*Guangdong University of Technology*). [[Paper](https://arxiv.org/abs/2207.13861)]
    * **SnowFormer**: "SnowFormer: Scale-aware Transformer via Context Interaction for Single Image Desnowing", arXiv, 2022 (*Jimei University, China*). [[Paper](https://arxiv.org/abs/2208.09703)]
    * **DMTNet**: "DMTNet: Dynamic Multi-scale Network for Dual-pixel Images Defocus Deblurring with Transformer", arXiv, 2022 (*Samsung*). [[Paper](https://arxiv.org/abs/2209.06040)]
    * **LMQFormer**: "LMQFormer: A Laplace-Prior-Guided Mask Query Transformer for Lightweight Snow Removal", arXiv, 2022 (*Fuzhou University*). [[Paper](https://arxiv.org/abs/2210.04787)]
    * **Semi-UFormer**: "Semi-UFormer: Semi-supervised Uncertainty-aware Transformer for Image Dehazing", arXiv, 2022 (*Nanjing University of Aeronautics and Astronautics*). [[Paper](https://arxiv.org/abs/2210.16057)]
    * **WITT**: "WITT: A Wireless Image Transmission Transformer for Semantic Communications", arXiv, 2022 (*Beijing University of Posts and Telecommunications*). [[Paper](https://arxiv.org/abs/2211.00937)][[Code (in construction)](https://github.com/KeYang8/WITT)]
    * **BiT**: "Blur Interpolation Transformer for Real-World Motion from Blur", CVPR, 2023 (*The University of Tokyo*). [[Paper](https://arxiv.org/abs/2211.11423)][[PyTorch](https://github.com/zzh-tech/BiT)][[Website](https://zzh-tech.github.io/BiT/)]
    * **SST**: "Spatial-Spectral Transformer for Hyperspectral Image Denoising", arXiv, 2022 (*Beijing Institute of Technology*). [[Paper](https://arxiv.org/abs/2211.14090)][[PyTorch](https://github.com/MyuLi/SST)]
    * **DRSformer**: "Learning A Sparse Transformer Network for Effective Image Deraining", CVPR, 2023 (*Nanjing University of Science and Technology*). [[Paper](https://arxiv.org/abs/2303.11950)][[PyTorch](https://github.com/cschenxiang/DRSformer)]
    * **MaskedDenoising**: "Masked Image Training for Generalizable Deep Image Denoising", CVPR, 2023 (*HKUST*). [[Paper](https://arxiv.org/abs/2303.13132)][[Code (in construction)](https://github.com/haoyuc/MaskedDenoising)]
    * **SERT**: "Spectral Enhanced Rectangle Transformer for Hyperspectral Image Denoising", CVPR, 2023 (*Beijing Institute of Technology*). [[Paper](https://arxiv.org/abs/2304.00844)][[PyTorch](https://github.com/MyuLi/SERT)]
    * **FFTformer**: "Efficient Frequency Domain-based Transformers for High-Quality Image Deblurring", CVPR, 2023 (*Nanjing University of Science and Technology*). [[Paper](https://arxiv.org/abs/2211.12250)][[PyTorch](https://github.com/kkkls/FFTformer)]
    * **MB-TaylorFormer**: "MB-TaylorFormer: Multi-branch Efficient Transformer Expanded by Taylor Formula for Image Dehazing", ICCV, 2023 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2308.14036)][[PyTorch](https://github.com/FVL2020/ICCV-2023-MB-TaylorFormer)]
    * **UDR-S<sup>2</sup>Former**: "Sparse Sampling Transformer with Uncertainty-Driven Ranking for Unified Removal of Raindrops and Rain Streaks", ICCV, 2023 (*HKUST*). [[Paper](https://arxiv.org/abs/2308.14153)][[PyTorch](https://github.com/Ephemeral182/UDR-S2Former_deraining)][[Website](https://ephemeral182.github.io/UDR_S2Former_deraining/)]
    * **HSDT**: "Hybrid Spectral Denoising Transformer with Guided Attention", ICCV, 2023 (*Beijing Institute of Technology*). [[Paper](https://arxiv.org/abs/2303.09040)][[PyTorch](https://github.com/Zeqiang-Lai/HSDT)]
    * **HI-Diff**: "Hierarchical Integration Diffusion Model for Realistic Image Deblurring", NeurIPS, 2023 (*SJTU*). [[Paper](https://arxiv.org/abs/2305.12966)][[PyTorch](https://github.com/zhengchen1999/HI-Diff)]
    * **SelfPromer**: "SelfPromer: Self-Prompt Dehazing Transformers with Depth-Consistency", arXiv, 2023 (*The Hong Kong Polytechnic University*). [[Paper](https://arxiv.org/abs/2303.07033)]
    * **Xformer**: "Xformer: Hybrid X-Shaped Transformer for Image Denoising", arXiv, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2303.06440)]
    * **?**: "A Data-Centric Solution to NonHomogeneous Dehazing via Vision Transformer", arXiv, 2023 (*McMaster University, Canada*). [[Paper](https://arxiv.org/abs/2304.07874)][[PyTorch](https://github.com/yangyiliu21/ntire2023_ITBdehaze)]

[[Back to Overview](#overview)]

### Video Restoration
* **VSR-Transformer**: "Video Super-Resolution Transformer", arXiv, 2021 (*ETHZ*). [[Paper](https://arxiv.org/abs/2106.06847)][[PyTorch](https://github.com/caojiezhang/VSR-Transformer)]
* **MANA**: "Memory-Augmented Non-Local Attention for Video Super-Resolution", CVPR, 2022 (*JD*). [[Paper](https://arxiv.org/abs/2108.11048)]
* **?**: "Bringing Old Films Back to Life", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2203.17276)][[Code (in construction)](https://github.com/raywzy/Bringing-Old-Films-Back-to-Life)]
* **TTVSR**: "Learning Trajectory-Aware Transformer for Video Super-Resolution", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2204.04216)][[PyTorch](https://github.com/researchmm/TTVSR)]
* **Trans-SVSR**: "A New Dataset and Transformer for Stereoscopic Video Super-Resolution", CVPR, 2022 (*Bahcesehir University, Turkey*). [[Paper](https://arxiv.org/abs/2204.10039)][[PyTorch](https://github.com/H-deep/Trans-SVSR/)]
* **STDAN**: "STDAN: Deformable Attention Network for Space-Time Video Super-Resolution", CVPRW, 2022 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2203.06841)]
* **VRT**: "VRT: A Video Restoration Transformer", arXiv, 2022 (*ETHZ*). [[Paper](https://arxiv.org/abs/2201.12288)][[PyTorch](https://github.com/JingyunLiang/VRT)]
* **FGST**: "Flow-Guided Sparse Transformer for Video Deblurring", ICML, 2022 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2201.01893)][[Code (in construction)](https://github.com/linjing7/VR-Baseline)]
* **RSTT**: "RSTT: Real-time Spatial Temporal Transformer for Space-Time Video Super-Resolution", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2203.14186)][[PyTorch](https://github.com/llmpass/RSTT)]
* **FTVSR**: "Learning Spatiotemporal Frequency-Transformer for Compressed Video Super-Resolution", ECCV, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2208.03012)][[PyTorch](https://github.com/researchmm/FTVSR)]
* **EFNet**: "Event-Based Fusion for Motion Deblurring with Cross-modal Attention", ECCV, 2022 (*ETHZ*). [[Paper](https://arxiv.org/abs/2112.00167)]
* **TempFormer**: "TempFormer: Temporally Consistent Transformer for Video Denoising", ECCV, 2022 (*Disney*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6092_ECCV_2022_paper.php)]
* **RVRT**: "Recurrent Video Restoration Transformer with Guided Deformable Attention", NeurIPS, 2022 (*ETHZ*). [[Paper](https://arxiv.org/abs/2206.02146)][[PyTorch](https://github.com/JingyunLiang/RVRT)]
* **?**: "Rethinking Alignment in Video Super-Resolution Transformers", NeurIPS, 2022 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2207.08494)][[PyTorch](https://github.com/XPixelGroup/RethinkVSRAlignment)]
* **VDTR**: "VDTR: Video Deblurring with Transformer", arXiv, 2022 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2204.08023)][[Code (in construction)](https://github.com/ljzycmd/VDTR)]
* **DSCT**: "Coarse-to-Fine Video Denoising with Dual-Stage Spatial-Channel Transformer", arXiv, 2022 (*Beijing University of Posts and Telecommunications*). [[Paper](https://arxiv.org/abs/2205.00214)]
* **Group-ShiftNet**: "No Attention is Needed: Grouped Spatial-temporal Shift for Simple and Efficient Video Restorers", arXiv, 2022 (*CUHK*). [[Paper](https://arxiv.org/abs/2206.10810)][[Code (in construction)](https://github.com/dasongli1/grouped_shift_net)][[Website](https://github.com/dasongli1/grouped_shift_net)]

[[Back to Overview](#overview)]

### Inpainting / Completion / Outpainting
* **Contexual-Attention**: "Generative Image Inpainting with Contextual Attention", CVPR, 2018 (*UIUC*). [[Paper](https://arxiv.org/abs/1801.07892)][[Tensorflow](https://github.com/JiahuiYu/generative_inpainting)]
* **PEN-Net**: "Learning Pyramid-Context Encoder Network for High-Quality Image Inpainting", CVPR, 2019 (*Microsoft*). [[Paper](https://arxiv.org/abs/1904.07475)][[PyTorch](https://github.com/researchmm/PEN-Net-for-Inpainting)]
* **Copy-Paste**: "Copy-and-Paste Networks for Deep Video Inpainting", ICCV, 2019 (*Yonsei University*). [[Paper](https://arxiv.org/abs/1908.11587)][[PyTorch](https://github.com/shleecs/Copy-and-Paste-Networks-for-Deep-Video-Inpainting)]
* **Onion-Peel**: "Onion-Peel Networks for Deep Video Completion", ICCV, 2019 (*Yonsei University*). [[Paper](https://arxiv.org/abs/1908.08718)][[PyTorch](https://github.com/seoungwugoh/opn-demo)]
* **STTN**: "Learning Joint Spatial-Temporal Transformations for Video Inpainting", ECCV, 2020 (*Microsoft*). [[Paper](https://arxiv.org/abs/2007.10247)][[PyTorch](https://github.com/researchmm/STTN)]
* **FuseFormer**: "FuseFormer: Fusing Fine-Grained Information in Transformers for Video Inpainting", ICCV, 2021 (*CUHK + SenseTime*). [[Paper](https://arxiv.org/abs/2109.02974)][[PyTorch](https://github.com/ruiliu-ai/FuseFormer)]
* **ICT**: "High-Fidelity Pluralistic Image Completion with Transformers", ICCV, 2021 (*CUHK*). [[Paper](https://arxiv.org/abs/2103.14031)][[PyTorch](https://github.com/raywzy/ICT)][[Website](http://raywzy.com/ICT/)]
* **DSTT**: "Decoupled Spatial-Temporal Transformer for Video Inpainting", arXiv, 2021 (*CUHK + SenseTime*). [[Paper](https://arxiv.org/abs/2104.06637)][[Code (in construction)](https://github.com/ruiliu-ai/DSTT)]
* **TFill**: "TFill: Image Completion via a Transformer-Based Architecture", arXiv, 2021 (*NTU Singapore*). [[Paper](https://arxiv.org/abs/2104.00845)][[Code (in construction)](https://github.com/lyndonzheng/TFill)]
* **BAT-Fill**: "Diverse Image Inpainting with Bidirectional and Autoregressive Transformers", arXiv, 2021 (*NTU Singapore*). [[Paper](https://arxiv.org/abs/2104.12335)]
* **?**: "Image-Adaptive Hint Generation via Vision Transformer for Outpainting", WACV, 2022 (*Sogang University, Korea*). [[Paper](https://openaccess.thecvf.com/content/WACV2022/html/Kong_Image-Adaptive_Hint_Generation_via_Vision_Transformer_for_Outpainting_WACV_2022_paper.html)]
* **ZITS**: "Incremental Transformer Structure Enhanced Image Inpainting with Masking Positional Encoding", CVPR, 2022 (*Fudan*). [[Paper](https://arxiv.org/abs/2203.00867)][[PyTorch](https://github.com/DQiaole/ZITS_inpainting)][[Website](https://dqiaole.github.io/ZITS_inpainting/)]
* **MAT**: "MAT: Mask-Aware Transformer for Large Hole Image Inpainting", CVPR, 2022 (*CUHK*). [[Paper](https://arxiv.org/abs/2203.15270)][[PyTorch](https://github.com/fenglinglwb/MAT)]
* **PUT**: "Reduce Information Loss in Transformers for Pluralistic Image Inpainting", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2205.05076)][[PyTorch](https://github.com/liuqk3/PUT)]
* **DLFormer**: "DLFormer: Discrete Latent Transformer for Video Inpainting", CVPR, 2022 (*Tencent*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Ren_DLFormer_Discrete_Latent_Transformer_for_Video_Inpainting_CVPR_2022_paper.html)][[Code (in construction)](https://github.com/JingjingRenabc/dlformer)]
* **T-former**: "T-former: An Efficient Transformer for Image Inpainting", ACMMM, 2022 (*Xi'an Jiaotong*). [[Paper](https://arxiv.org/abs/2305.07239)][[PyTorch](https://github.com/dengyecode/T-former_image_inpainting)]
* **QueryOTR**: "Outpainting by Queries", ECCV, 2022 (*University of Liverpool, UK*). [[Paper](https://arxiv.org/abs/2207.05312)][[PyTorch (in construction)](https://github.com/Kaiseem/QueryOTR)]
* **FGT**: "Flow-Guided Transformer for Video Inpainting", ECCV, 2022 (*USTC*). [[Paper](https://arxiv.org/abs/2208.06768)][[PyTorch](https://github.com/hitachinsk/FGT)]
* **MAE-FAR**: "Learning Prior Feature and Attention Enhanced Image Inpainting", ECCV, 2022 (*Fudan University*). [[Paper](https://arxiv.org/abs/2208.01837)][[PyTorch (in construction)](https://github.com/ewrfcas/MAE-FAR)][[Website](https://ewrfcas.github.io/MAE-FAR/)]
* **?**: "Visual Prompting via Image Inpainting", NeurIPS, 2022 (*Berkeley*). [[Paper](https://arxiv.org/abs/2209.00647)][[PyTorch](https://github.com/amirbar/visual_prompting)][[Website](https://yossigandelsman.github.io/visual_prompt/)]
* **U-Transformer**: "Generalised Image Outpainting with U-Transformer", arXiv, 2022 (*Xi'an Jiaotong-Liverpool University*). [[Paper](https://arxiv.org/abs/2201.11403)]
* **SpA-Former**: "SpA-Former: Transformer image shadow detection and removal via spatial attention", arXiv, 2022 (*Shanghai Jiao Tong University*). [[Paper](https://arxiv.org/abs/2206.10910)][[PyTorch](https://github.com/zhangbaijin/SpA-Former-shadow-removal)]
* **CRFormer**: "CRFormer: A Cross-Region Transformer for Shadow Removal", arXiv, 2022 (*Beijing Jiaotong University*). [[Paper](https://arxiv.org/abs/2207.01600)]
* **DeViT**: "DeViT: Deformed Vision Transformers in Video Inpainting", arXiv, 2022 (*Kuaishou*). [[Paper](https://arxiv.org/abs/2209.13925)]
* **ZITS++**: "ZITS++: Image Inpainting by Improving the Incremental Transformer on Structural Priors", arXiv, 2022 (*Fudan*). [[Paper](https://arxiv.org/abs/2210.05950)]
* **TPFNet**: "TPFNet: A Novel Text In-painting Transformer for Text Removal", arXiv, 2022 (*?*). [[Paper](https://arxiv.org/abs/2210.14461)][[Code (in construction)](https://github.com/CandleLabAI/TPFNet)]
* **FlowLens**: "FlowLens: Seeing Beyond the FoV via Flow-guided Clip-Recurrent Transformer", arXiv, 2022 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2211.11293)][[Code (in construction)](https://github.com/MasterHow/FlowLens)]
* **?**: "Putting People in Their Place: Affordance-Aware Human Insertion into Scenes", CVPR, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2304.14406)][[PyTorch (in construction)](https://github.com/adobe-research/affordance-insertion)][[Website](https://sumith1896.github.io/affordance-insertion/)]
* **Imagen-Editor**: "Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2212.06909)][[Website](https://imagen.research.google/editor/)]
* **SmartBrush**: "SmartBrush: Text and Shape Guided Object Inpainting with Diffusion Model", CVPR, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2212.05034)]
* **NÜWA-LIP**: "NÜWA-LIP: Language Guided Image Inpainting with Defect-free VQGAN", CVPR, 2023 (*Harbin Institute of Technology*). [[Paper](https://arxiv.org/abs/2202.05009)][[PyTorch](https://github.com/kodenii/NUWA-LIP)]
* **ProPainter**: "ProPainter: Improving Propagation and Transformer for Video Inpainting", ICCV, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2309.03897)][[PyTorch](https://github.com/sczhou/ProPainter)][[Website](https://shangchenzhou.com/projects/ProPainter/)]
* **Inst-Inpaint**: "Inst-Inpaint: Instructing to Remove Objects with Diffusion Models", arXiv, 2023 (*Bilkent University, Turkey*). [[Paper](https://arxiv.org/abs/2304.03246)]
* **Inpaint-Anything**: "Inpaint Anything: Segment Anything Meets Image Inpainting", arXiv, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2304.06790)][[PyTorch](https://github.com/geekyutao/Inpaint-Anything)]
* **TransRef**: "TransRef: Multi-Scale Reference Embedding Transformer for Reference-Guided Image Inpainting", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2306.11528)][[PyTorch](https://github.com/Cameltr/TransRef)]
* **DMT**: "Deficiency-Aware Masked Transformer for Video Inpainting", arXiv, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2307.08629)][[Code (in construction)](https://github.com/yeates/DMT)]
* **Magicremover**: "Magicremover: Tuning-free Text-guided Image inpainting with Diffusion Models", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2310.02848)][[Code (in construction)](https://github.com/exisas/Magicremover)]
* **LGVI**: "Towards Language-Driven Video Inpainting via Multimodal Large Language Models", arXiv, 2024 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2401.10226)][[Code (in construction)](https://github.com/jianzongwu/Language-Driven-Video-Inpainting)][[Website](https://jianzongwu.github.io/projects/rovi/)]

[[Back to Overview](#overview)]

### Image Generation
* **IT**: "Image Transformer", ICML, 2018 (*Google*). [[Paper](https://arxiv.org/abs/1802.05751)][[Tensorflow](https://github.com/tensorflow/tensor2tensor)]
* **PixelSNAIL**: "PixelSNAIL: An Improved Autoregressive Generative Model", ICML, 2018 (*Berkeley*). [[Paper](http://proceedings.mlr.press/v80/chen18h.html)][[Tensorflow](https://github.com/neocxi/pixelsnail-public)]
* **BigGAN**: "Large Scale GAN Training for High Fidelity Natural Image Synthesis", ICLR, 2019 (*DeepMind*). [[Paper](https://openreview.net/forum?id=B1xsqj09Fm)][[PyTorch](https://github.com/ajbrock/BigGAN-PyTorch)]
* **SAGAN**: "Self-Attention Generative Adversarial Networks", ICML, 2019 (*Google*). [[Paper](http://proceedings.mlr.press/v97/zhang19d.html)][[Tensorflow](https://github.com/brain-research/self-attention-gan)]
* **VQGAN**: "Taming Transformers for High-Resolution Image Synthesis", CVPR, 2021 (*Heidelberg University*). [[Paper](https://arxiv.org/abs/2012.09841)][[PyTorch](https://github.com/CompVis/taming-transformers)][[Website](https://compvis.github.io/taming-transformers/)]
* **?**: "High-Resolution Complex Scene Synthesis with Transformers", CVPRW, 2021 (*Heidelberg University*). [[Paper](https://arxiv.org/abs/2105.06458)]
* **GANsformer**: "Generative Adversarial Transformers", ICML, 2021 (*Stanford + Facebook*). [[Paper](https://arxiv.org/abs/2103.01209)][[Tensorflow](https://github.com/dorarad/gansformer)]
* **PixelTransformer**: "PixelTransformer: Sample Conditioned Signal Generation", ICML, 2021 (*Facebook*). [[Paper](https://arxiv.org/abs/2103.15813)][[Website](https://shubhtuls.github.io/PixelTransformer/)]
* **HWT**: "Handwriting Transformers", ICCV, 2021 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2104.03964)][[Code (in construction)](https://github.com/ankanbhunia/Handwriting-Transformers)]
* **Paint-Transformer**: "Paint Transformer: Feed Forward Neural Painting with Stroke Prediction", ICCV, 2021 (*Baidu*). [[Paper](https://arxiv.org/abs/2108.03798)][[Paddle](https://github.com/PaddlePaddle/PaddleGAN)][[PyTorch](https://github.com/Huage001/PaintTransformer)]
* **Geometry-Free**: "Geometry-Free View Synthesis: Transformers and no 3D Priors", ICCV, 2021 (*Heidelberg University*). [[Paper](https://arxiv.org/abs/2104.07652)][[PyTorch](https://github.com/CompVis/geometry-free-view-synthesis)]
* **VTGAN**: "VTGAN: Semi-supervised Retinal Image Synthesis and Disease Prediction using Vision Transformers", ICCVW, 2021 (*University of Nevada, Reno*). [[Paper](https://arxiv.org/abs/2104.06757)]
* **ATISS**: "ATISS: Autoregressive Transformers for Indoor Scene Synthesis", NeurIPS, 2021 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2110.03675)][[Website](https://nv-tlabs.github.io/ATISS/)]
* **GANsformer2**: "Compositional Transformers for Scene Generation", NeurIPS, 2021 (*Stanford + Facebook*). [[Paper](https://arxiv.org/abs/2111.08960)][[Tensorflow](https://github.com/dorarad/gansformer)]
* **TransGAN**: "TransGAN: Two Transformers Can Make One Strong GAN", NeurIPS, 2021 (*UT Austin*). [[Paper](https://arxiv.org/abs/2102.07074)][[PyTorch](https://github.com/VITA-Group/TransGAN)]
* **HiT**: "Improved Transformer for High-Resolution GANs", NeurIPS, 2021 (*Google*). [[Paper](https://arxiv.org/abs/2106.07631)][[Tensorflow](https://github.com/google-research/hit-gan)]
* **iLAT**: "The Image Local Autoregressive Transformer", NeurIPS, 2021 (*Fudan*). [[Paper](https://arxiv.org/abs/2106.02514)]
* **TokenGAN**: "Improving Visual Quality of Image Synthesis by A Token-based Generator with Transformers", NeurIPS, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.03481)]
* **SceneFormer**: "SceneFormer: Indoor Scene Generation with Transformers", arXiv, 2021 (*TUM*). [[Paper](https://arxiv.org/abs/2012.09793)]
* **SNGAN**: "Combining Transformer Generators with Convolutional Discriminators", arXiv, 2021 (*Fraunhofer ITWM*). [[Paper](https://arxiv.org/abs/2105.10189)]
* **Invertible-Attention**: "Invertible Attention", arXiv, 2021 (*ANU*). [[Paper](https://arxiv.org/abs/2106.09003)]
* **GPA**: "Grid Partitioned Attention: Efficient Transformer Approximation with Inductive Bias for High Resolution Detail Generation", arXiv, 2021 (*Zalando Research, Germany*). [[Paper](https://arxiv.org/abs/2107.03742)][[PyTorch (in construction)](https://github.com/zalandoresearch/gpa)]
* **ViTGAN**: "ViTGAN: Training GANs with Vision Transformers", ICLR, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2107.04589)][[PyTorch](https://github.com/mlpc-ucsd/ViTGAN)][[PyTorch (wilile26811249)](https://github.com/wilile26811249/ViTGAN)]
* **ViT-VQGAN**: "Vector-quantized Image Modeling with Improved VQGAN", ICLR, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2110.04627)]
* **Style-Transformer**: "Style Transformer for Image Inversion and Editing", CVPR, 2022 (*East China Normal University*). [[Paper](https://arxiv.org/abs/2203.07932)][[PyTorch](https://github.com/sapphire497/style-transformer)]
* **StyleSwin**: "StyleSwin: Transformer-based GAN for High-resolution Image Generation", CVPR, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2112.10762)][[PyTorch](https://github.com/microsoft/StyleSwin)]
* **Styleformer**: "Styleformer: Transformer based Generative Adversarial Networks with Style Vector", CVPR, 2022 (*Seoul National University*). [[Paper](https://arxiv.org/abs/2106.07023)][[PyTorch](https://github.com/Jeeseung-Park/Styleformer)]
* **?**: "User-Controllable Latent Transformer for StyleGAN Image Layout Editing", Pacific Graphics, 2022 (*University of Tsukuba*). [[Paper](https://arxiv.org/abs/2208.12408)][[Website](http://www.cgg.cs.tsukuba.ac.jp/~endo/projects/UserControllableLT/)]
* **DynaST**: "DynaST: Dynamic Sparse Transformer for Exemplar-Guided Image Generation", ECCV, 2022 (*NUS*). [[Paper](https://arxiv.org/abs/2207.06124)][[PyTorch](https://github.com/Huage001/DynaST)]
* **DoodleFormer**: "DoodleFormer: Creative Sketch Drawing with Transformers", ECCV, 2022 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2112.03258)][[PyTorch](https://github.com/ankanbhunia/doodleformer)][[Website](https://ankanbhunia.github.io/doodleformer/)]
* **U-Attention**: "Paying U-Attention to Textures: Multi-Stage Hourglass Vision Transformer for Universal Texture Synthesis", arXiv, 2022 (*Adobe*). [[Paper](https://arxiv.org/abs/2202.11703)]
* **MaskGIT**: "MaskGIT: Masked Generative Image Transformer", CVPR, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2202.04200)][[PyTorch (dome272)](https://github.com/dome272/MaskGIT-pytorch)]
* **AttnFlow**: "Generative Flows with Invertible Attentions", CVPR, 2022 (*ETHZ*). [[Paper](https://arxiv.org/abs/2106.03959)]
* **NÜWA**: "NÜWA: Visual Synthesis Pre-training for Neural visUal World creAtion", ECCV, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2111.12417)][[GitHub](https://github.com/microsoft/NUWA)]
* **Trans-INR**: "Transformers as Meta-Learners for Implicit Neural Representations", ECCV, 2022 (*UCSD*). [[Paper](https://arxiv.org/abs/2208.02801)][[PyTorch](https://github.com/yinboc/trans-inr)][[Websiste](https://yinboc.github.io/trans-inr/)]
* **ViewFormer**: "ViewFormer: NeRF-free Neural Rendering from Few Images Using Transformers", ECCV, 2022 (*Czech Technical University in Prague*). [[Paper](https://arxiv.org/abs/2203.10157)][[Tensorflow](https://github.com/jkulhanek/viewformer)]
* **Unleashing-Transformer**: "Unleashing Transformers: Parallel Token Prediction with Discrete Absorbing Diffusion for Fast High-Resolution Image Generation from Vector-Quantized Codes", ECCV, 2022 (*Durham University, UK*). [[Paper](https://arxiv.org/abs/2111.12701)][[PyTorch](https://github.com/samb-t/unleashing-transformers)]
* **CASD**: "Cross Attention Based Style Distribution for Controllable Person Image Synthesis", ECCV, 2022 (*East China Norma lUniversity*). [[Paper](https://arxiv.org/abs/2208.00712)]
* **VQGAN-CLIP**: "VQGAN-CLIP: Open Domain Image Generation and Manipulation Using Natural Language	", ECCV, 2022 (*EleutherAI*). [[Paper](https://arxiv.org/abs/2204.08583)][[PyTorch](https://github.com/EleutherAI/vqgan-clip)]
* **Token-Critic**: "Improved Masked Image Generation with Token-Critic", ECCV, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2209.04439)]
* **PromptGen**: "Generative Visual Prompt: Unifying Distributional Control of Pre-Trained Generative Models", NeurIPS, 2022 (*CMU*). [[Paper](https://arxiv.org/abs/2209.06970)][[PyTorch](https://github.com/ChenWu98/Generative-Visual-Prompt)]
* **Contextual-RQ-Transformer**: "Draft-and-Revise: Effective Image Generation with Contextual RQ-Transformer", NeurIPS, 2022 (*POSTECH + Kakao*). [[Paper](https://arxiv.org/abs/2206.04452)]
* **ViT-Patch**: "A Robust Framework of Chromosome Straightening with ViT-Patch GAN", arXiv, 2022 (*Xi'an Jiaotong-Liverpool University*). [[Paper](https://arxiv.org/abs/2203.02901)]
* **?**: "Transforming Image Generation from Scene Graphs", arXiv, 2022 (*University of Catania, Italy*). [[Paper](https://arxiv.org/abs/2207.00545)]
* **VisionNeRF**: "Vision Transformer for NeRF-Based View Synthesis from a Single Input Image", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2207.05736)][[Website](https://cseweb.ucsd.edu/~viscomp/projects/VisionNeRF/)]
* **NUWA-Infinity**: "NUWA-Infinity: Autoregressive over Autoregressive Generation for Infinite Visual Synthesis", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2207.09814)][[GitHub](https://github.com/microsoft/NUWA)][[Website](https://nuwa-infinity.microsoft.com/)]
* **Diffusion-ViT**: "Your ViT is Secretly a Hybrid Discriminative-Generative Diffusion Model", arXiv, 2022 (*Etsy, NY*). [[Paper](https://arxiv.org/abs/2208.07791)]
* **?**: "Visual Prompt Tuning for Generative Transfer Learning", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2210.00990)][[JAX](https://github.com/google-research/generative_transfer)]
* **SeQ-GAN**: "Rethinking the Objectives of Vector-Quantized Tokenizers for Image Synthesis", arXiv, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2212.03185)][[Code (in construction)](https://github.com/TencentARC/BasicVQ-GEN)]
* **?**: "Style-Guided Inference of Transformer for High-resolution Image Synthesis", WACV, 2023 (*NCSOFT, Korea*). [[Paper](https://arxiv.org/abs/2210.05533)]
* **Frido**: "Frido: Feature Pyramid Diffusion for Complex Scene Image Synthesis", AAAI, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2208.13753)][[PyTorch](https://github.com/davidhalladay/Frido)]
* **GNT**: "Is Attention All That NeRF Needs?", ICLR, 2023 (*UT Austin*). [[Paper](https://arxiv.org/abs/2207.13298)][[PyTorch](https://github.com/VITA-Group/GNT)][[Website](https://vita-group.github.io/GNT/)]
* **DPC**: "Discrete Predictor-Corrector Diffusion Models for Image Synthesis", ICLR, 2023 (*Google*). [[Paper](https://openreview.net/forum?id=VM8batVBWvg)]
* **LayoutDM**: "LayoutDM: Discrete Diffusion Model for Controllable Layout Generation", CVPR, 2023 (*CyberAgent, Japan*). [[Paper](https://arxiv.org/abs/2303.08137)][[PyTorch](https://github.com/CyberAgentAILab/layout-dm)][[Website](https://cyberagentailab.github.io/layout-dm/)]
* **GTGAN**: "Graph Transformer GANs for Graph-Constrained House Generation", CVPR, 2023 (*ETHZ*). [[Paper](https://arxiv.org/abs/2303.08225)]
* **U-ViT**: "All are Worth Words: A ViT Backbone for Diffusion Models", CVPR, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2209.12152)]
* **MQ-VAE**: "Not All Image Regions Matter: Masked Vector Quantization for Autoregressive Image Generation", CVPR, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2305.13607)][[PyTorch](https://github.com/CrossmodalGroup/MaskedVectorQuantization)]
* **MaskSketch**: "MaskSketch: Unpaired Structure-guided Masked Image Generation", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2302.05496)][[JAX](https://github.com/google-research/masksketch)][[Website](https://masksketch.github.io/)]
* **GAN-MAE**: "Masked Auto-Encoders Meet Generative Adversarial Networks and Beyond", CVPR, 2023 (*Meituan*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Fei_Masked_Auto-Encoders_Meet_Generative_Adversarial_Networks_and_Beyond_CVPR_2023_paper.html)]
* **Reg-VQ**: "Regularized Vector Quantization for Tokenized Image Synthesis", CVPR, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2303.06424)]
* **LCP-GAN**: "Exploring Intra-Class Variation Factors With Learnable Cluster Prompts for Semi-Supervised Image Synthesis", CVPR, 2023 (*South China University of Technology*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Exploring_Intra-Class_Variation_Factors_With_Learnable_Cluster_Prompts_for_Semi-Supervised_CVPR_2023_paper.html)]
* **Slot-VAE**: "Slot-VAE: Object-Centric Scene Generation with Slot Attention", ICML, 2023 (*Delft University of Technology, Netherland*). [[Paper](https://arxiv.org/abs/2306.06997)]
* **Efficient-VQGAN**: "Efficient-VQGAN: Towards High-Resolution Image Generation with Efficient Vision Transformers", ICCV, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2310.05400)]
* **MDT**: "Masked Diffusion Transformer is a Strong Image Synthesizer", ICCV, 2023 (*Sea AI Lab*). [[Paper](https://arxiv.org/abs/2303.14389)][[PyTorch](https://github.com/sail-sg/MDT)]
* **LayoutPrompter**: "LayoutPrompter: Awaken the Design Ability of Large Language Models", NeurIPS, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2311.06495)]
* **LayoutGPT**: "LayoutGPT: Compositional Visual Planning and Generation with Large Language Models", NeurIPS, 2023 (*UCSB*). [[Paper](https://arxiv.org/abs/2305.15393)][[PyTorch](https://github.com/weixi-feng/LayoutGPT)][[Website](https://layoutgpt.github.io/)]
* **Diff-Instruct**: "Diff-Instruct: A Universal Approach for Transferring Knowledge From Pre-trained Diffusion Models", NeurIPS, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2305.18455)]
* **VQ3D**: "VQ3D: Learning a 3D-Aware Generative Model on ImageNet", arXiv, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2302.06833)][[Website](https://kylesargent.github.io/vq3d)]
* **LayoutDiffuse**: "LayoutDiffuse: Adapting Foundational Diffusion Models for Layout-to-Image Generation", arXiv, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2302.08908)]
* **StraIT**: "StraIT: Non-autoregressive Generation with Stratified Image Transformer", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.00750)]
* **MMoT**: "MMoT: Mixture-of-Modality-Tokens Transformer for Composed Multimodal Conditional Image Synthesis", arXiv, 2023 (*South China University of Technology*). [[Paper](https://arxiv.org/abs/2305.05992)][[PyTorch (in construction)](https://github.com/jabir-zheng/MMoT-Transformer)][[Website](https://jabir-zheng.github.io/MMoT/)]
* **MAskDiT**: "Fast Training of Diffusion Models with Masked Transformers", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2306.09305)]
* **Dolfin**: "Dolfin: Diffusion Layout Transformers without Autoencoder", arXiv, 2023 (*UCSD*). [[Paper](https://arxiv.org/abs/2310.16305)]
* **RALF**: "Retrieval-Augmented Layout Transformer for Content-Aware Layout Generation", arXiv, 2023 (*The University of Tokyo*). [[Paper](https://arxiv.org/abs/2311.13602)][[Website](https://udonda.github.io/RALF/)]
* **GIVT**: "GIVT: Generative Infinite-Vocabulary Transformers", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2312.02116)]
* **DiffiT**: "DiffiT: Diffusion Vision Transformers for Image Generation", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2312.02139)][[Code (in construction)](https://github.com/NVlabs/DiffiT)]
* **RCG**: "Self-conditioned Image Generation via Generating Representations", arXiv, 2023 (*MIT*). [[Paper](https://arxiv.org/abs/2312.03701)][[PyTorch](https://github.com/LTH14/rcg)]
* **GSN**: "GSN: Generalisable Segmentation in Neural Radiance Field", AAAI, 2024 (*IIIT Hyderabad*). [[Paper](https://arxiv.org/abs/2402.04632)][[PyTorch](https://github.com/Vinayak-VG/GSN)][[Website](https://vinayak-vg.github.io/GSN/)]
* **HDiT**: "Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers", arXiv, 2024 (*Stability AI*). [[Paper](https://arxiv.org/abs/2401.11605)][[Website](https://crowsonkb.github.io/hourglass-diffusion-transformers/)]

[[Back to Overview](#overview)]

### Video Generation
* **Subscale**: "Scaling Autoregressive Video Models", ICLR, 2020 (*Google*). [[Paper](https://openreview.net/forum?id=rJgsskrFwH)][[Website](https://sites.google.com/view/video-transformer-samples)]
* **ConvTransformer**: "ConvTransformer: A Convolutional Transformer Network for Video Frame Synthesis", arXiv, 2020 (*Southeast University*). [[Paper](https://arxiv.org/abs/2011.10185)]
* **OCVT**: "Generative Video Transformer: Can Objects be the Words?", ICML, 2021 (*Rutgers University*). [[Paper](http://proceedings.mlr.press/v139/wu21h.html)]
* **AIST++**: "Learn to Dance with AIST++: Music Conditioned 3D Dance Generation", arXiv, 2021 (*Google*). [[Paper](https://arxiv.org/abs/2101.08779)][[Code](https://github.com/google/aistplusplus_api)][[Website](https://google.github.io/aichoreographer/)]
* **VideoGPT**: "VideoGPT: Video Generation using VQ-VAE and Transformers", arXiv, 2021 (*Berkeley*). [[Paper](https://arxiv.org/abs/2104.10157)][[PyTorch](https://github.com/wilson1yan/VideoGPT)][[Website](https://wilson1yan.github.io/videogpt/index.html)]
* **DanceFormer**: "DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer", AAAI, 2022 (*Huiye Technology, China*). [[Paper](https://arxiv.org/abs/2103.10206)]
* **VFIformer**: "Video Frame Interpolation with Transformer", CVPR, 2022 (*CUHK*). [[Paper](https://arxiv.org/abs/2205.07230)][[PyTorch](https://github.com/dvlab-research/VFIformer)]
* **VFIT**: "Video Frame Interpolation Transformer", CVPR, 2022 (*McMaster Univeristy, Canada*). [[Paper](https://arxiv.org/abs/2111.13817)][[PyTorch](https://github.com/zhshi0816/Video-Frame-Interpolation-Transformer)]
* **MoTrans**: "Motion Transformer for Unsupervised Image Animation", ECCV, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2209.14024)][[PyTorch](https://github.com/JialeTao/MoTrans)]
* **Transframer**: "Transframer: Arbitrary Frame Prediction with Generative Models", arXiv, 2022 (*DeepMind*). [[Paper](https://arxiv.org/abs/2203.09494)]
* **TATS**: "Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer", ECCV, 2022 (*Maryland*). [[Paper](https://arxiv.org/abs/2204.03638)][[Website](https://songweige.github.io/projects/tats/index.html)]
* **POVT**: "Patch-based Object-centric Transformers for Efficient Video Generation", arXiv, 2022 (*Berkeley*). [[Paper](https://arxiv.org/abs/2206.04003)][[PyTorch](https://github.com/wilson1yan/povt)][[Website](https://sites.google.com/view/povt-public)]
* **TAIN**: "Cross-Attention Transformer for Video Interpolation", arXiv, 2022 (*Duke*). [[Paper](https://arxiv.org/abs/2207.04132)]
* **TTVFI**: "TTVFI: Learning Trajectory-Aware Transformer for Video Frame Interpolation", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2207.09048)]
* **SlotFormer**: "SlotFormer: Unsupervised Visual Dynamics Simulation with Object-Centric Models", arXiv, 2022 (*University of Toronto*). [[Paper](https://arxiv.org/abs/2210.05861)][[Website](https://slotformer.github.io/)]
* **Human-MotionFormer**: "Human MotionFormer: Transferring Human Motions with Vision Transformers", ICLR, 2023 (*HKUST + Huya*). [[Paper](https://arxiv.org/abs/2302.11306)][[Code (in construction)](https://github.com/KumapowerLIU/Human-MotionFormer)]
* **MAGVIT**: "MAGVIT: Masked Generative Video Transformer", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2212.05199)][[Code (in construction)](https://github.com/MAGVIT/magvit)][[Website](https://magvit.cs.cmu.edu/)]
* **MeBT**: "Towards End-to-End Generative Modeling of Long Videos with Memory-Efficient Bidirectional Transformers", CVPR, 2023 (*Kakao*). [[Paper](https://arxiv.org/abs/2303.11251)][[PyTorch](https://github.com/Ugness/MeBT)][[Website](https://sites.google.com/view/mebt-cvpr2023)]
* **BiFormer**: "BiFormer: Learning Bilateral Motion Estimation via Bilateral Transformer for 4K Video Frame Interpolation", CVPR, 2023 (*Korea University*). [[Paper](https://arxiv.org/abs/2304.02225)][[PyTorch (in construction)](https://github.com/JunHeum/BiFormer)]
* **AMT**: "AMT: All-Pairs Multi-Field Transforms for Efficient Frame Interpolation", CVPR, 2023 (*Nankai University*). [[Paper](https://arxiv.org/abs/2304.09790)][[PyTorch](https://github.com/MCG-NKU/AMT)][[Website](https://nk-cs-zzl.github.io/projects/amt/)]
* **?**: "Frame Interpolation Transformer and Uncertainty Guidance", CVPR, 2023 (*Disney*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Plack_Frame_Interpolation_Transformer_and_Uncertainty_Guidance_CVPR_2023_paper.html)]
* **EMA-VFI**: "Extracting Motion and Appearance via Inter-Frame Attention for Efficient Video Frame Interpolation", CVPR, 2023 (*Nanjing University*). [[Paper](https://arxiv.org/abs/2303.00440)][[PyTorch](https://github.com/MCG-NJU/EMA-VFI)]
* **EIF-BiOFNet**: "Event-Based Video Frame Interpolation With Cross-Modal Asymmetric Bidirectional Motion Fields", CVPR, 2023 (*KAIST*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Kim_Event-Based_Video_Frame_Interpolation_With_Cross-Modal_Asymmetric_Bidirectional_Motion_Fields_CVPR_2023_paper.html)]
* **TECO**: "Temporally Consistent Video Transformer for Long-Term Video Prediction", ICML, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2210.02396)][[JAX](https://github.com/wilson1yan/teco)][[Website](https://wilson1yan.github.io/teco/index.html)]
* **VFIFT**: "Video Frame Interpolation with Flow Transformer", ACMMM, 2023 (*Nanjing University of Aeronautics and Astronautics*). [[Paper](https://arxiv.org/abs/2307.16144)]
* **ConvSSM**: "Convolutional State Space Models for Long-Range Spatiotemporal Modeling", NeurIPS, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2310.19694)][[JAX](https://github.com/NVlabs/ConvSSM)]
* **NUWA-XL**: "NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2303.12346)][[Website (in construction)](https://msra-nuwa.azurewebsites.net/)]
* **CAT-NeRF**: "CAT-NeRF: Constancy-Aware Tx<sup>2</sup>Former for Dynamic Body Modeling", arXiv, 2023 (*USC*). [[Paper](https://arxiv.org/abs/2304.07915)]
* **IconShop**: "IconShop: Text-Based Vector Icon Synthesis with Autoregressive Transformers", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2304.14400)][[Code (in construction)](https://github.com/kingnobro/IconShop)][[Website](https://kingnobro.github.io/iconshop/)]
* **VDT**: "VDT: An Empirical Study on Video Diffusion with Transformers", arXiv, 2023 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2305.13311)][[PyTorch](https://github.com/RERV/VDT)]
* **MAGVIT-v2**: "Language Model Beats Diffusion - Tokenizer is Key to Visual Generation", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2310.05737)][[Website](https://magvit.cs.cmu.edu/v2/)]
* **UVDv1**: "Sequential Modeling Enables Scalable Learning for Large Vision Models", arXiv, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2312.00785)][[Code (in construction)](https://github.com/ytongbai/LVM)][[Website](https://yutongbai.com/lvm.html)]
* **W.A.L.T**: "Photorealistic Video Generation with Diffusion Models", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2312.06662)][[Website](https://walt-video-diffusion.github.io/)]

[[Back to Overview](#overview)]

### Transfer / Translation / Manipulation
* **AdaAttN**: "AdaAttN: Revisit Attention Mechanism in Arbitrary Neural Style Transfer", ICCV, 2021 (*Baidu*). [[Paper](https://arxiv.org/abs/2108.03647)][[Paddle](https://github.com/PaddlePaddle/PaddleGAN)][[PyTorch](https://github.com/Huage001/AdaAttN)]
* **StyleCLIP**: "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery", ICCV, 2021 (*Hebrew University of Jerusalem*). [[Paper](https://arxiv.org/abs/2103.17249)][[PyTorch](https://github.com/orpatashnik/StyleCLIP)]
* **StyTr2**: "StyTr^2: Unbiased Image Style Transfer with Transformers", CVPR, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2105.14576)][[PyTorch](https://github.com/diyiiyiii/StyTR-2)]
* **InstaFormer**: "InstaFormer: Instance-Aware Image-to-Image Translation with Transformer", CVPR, 2022 (*Korea University*). [[Paper](https://arxiv.org/abs/2203.16248)]
* **ManiTrans**: "ManiTrans: Entity-Level Text-Guided Image Manipulation via Token-wise Semantic Alignment and Generation", CVPR, 2022 (*Huawei*). [[Paper](https://arxiv.org/abs/2204.04428)][[Website](https://jawang19.github.io/manitrans/)]
* **QS-Attn**: "QS-Attn: Query-Selected Attention for Contrastive Learning in I2I Translation", CVPR, 2022 (*Shanghai Key Laboratory*). [[Paper](https://arxiv.org/abs/2203.08483)][[PyTorch](https://github.com/sapphire497/query-selected-attention)]
* **Splice**: "Splicing ViT Features for Semantic Appearance Transfer", CVPR, 2022 (*Weizmann Institute of Science, Israel*). [[Paper](https://arxiv.org/abs/2201.00424)][[PyTorch](https://github.com/omerbt/Splice)][[Website](https://splice-vit.github.io/)]
* **ASSET**: "ASSET: Autoregressive Semantic Scene Editing with Transformers at High Resolutions", SIGGRAPH, 2022 (*Adobe*). [[Paper](https://arxiv.org/abs/2205.12231)][[PyTorch](https://github.com/DifanLiu/ASSET)][[Website](https://people.cs.umass.edu/~dliu/projects/ASSET/)]
* **SCAM**: "SCAM! Transferring humans between images with Semantic Cross Attention Modulation", ECCV, 2022 (*Univ Gustave Eiffel, France*). [[Paper](https://arxiv.org/abs/2210.04883)][[PyTorch](https://github.com/nicolas-dufour/SCAM)][[Website](https://imagine.enpc.fr/~dufourn/publications/scam.html)]
* **TargetCLIP**: "Image-Based CLIP-Guided Essence Transfer", ECCV, 2022 (*Tel Aviv*). [[Paper](https://arxiv.org/abs/2110.12427)][[PyTorch](https://github.com/hila-chefer/TargetCLIP)]
* **FFCLIP**: "One Model to Edit Them All: Free-Form Text-Driven Image Manipulation with Semantic Modulations", NeurIPS, 2022 (*Tencent*). [[Paper](https://arxiv.org/abs/2210.07883)][[Code (in construction)](https://github.com/KumapowerLIU/FFCLIP)]
* **STTR**: "Fine-Grained Image Style Transfer with Visual Transformers", ACCV, 2022 (*The Univerisity of Tokyo*). [[Paper](https://arxiv.org/abs/2210.05176)][[PyTorch (in construction)](https://github.com/researchmm/STTR)]
* **UVCGAN**: "UVCGAN: UNet Vision Transformer cycle-consistent GAN for unpaired image-to-image translation", arXiv, 2022 (*Brookhaven National Laboratory, NY*). [[Paper](https://arxiv.org/abs/2203.02557)]
* **ITTR**: "ITTR: Unpaired Image-to-Image Translation with Transformers", arXiv, 2022 (*Kuaishou*). [[Paper](https://arxiv.org/abs/2203.16015)]
* **CLIPasso**: "CLIPasso: Semantically-Aware Object Sketching", arXiv, 2022 (*EPFL*). [[Paper](https://arxiv.org/abs/2202.05822)][[PyTorch](https://github.com/yael-vinker/CLIPasso)][[Website](https://clipasso.github.io/clipasso/)]
* **CTrGAN**: "CTrGAN: Cycle Transformers GAN for Gait Transfer", arXiv, 2022 (*Ariel University, Israel*). [[Paper](https://arxiv.org/abs/2206.15248)]
* **PI-Trans**: "PI-Trans: Parallel-ConvMLP and Implicit-Transformation Based GAN for Cross-View Image Translation", arXiv, 2022 (*University of Trento, Italy*). [[Paper](https://arxiv.org/abs/2207.04242)][[PyTorch (in construction)](https://github.com/Amazingren/PI-Trans)]
* **CSLA**: "Bridging CLIP and StyleGAN through Latent Alignment for Image Editing", arXiv, 2022 (*Kuaishou*). [[Paper](https://arxiv.org/abs/2210.04506)]
* **CLIP-PAE**: "CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable, and Controllable Text-Guided Image Manipulation", arXiv, 2022 (*University of Cambridge*). [[Paper](https://arxiv.org/abs/2210.03919)]
* **S2WAT**: "S2WAT: Image Style Transfer via Hierarchical Vision Transformer using Strips Window Attention", arXiv, 2022 (*Sichuan Normal University*). [[Paper](https://arxiv.org/abs/2210.12381)]
* **DiffuseIT**: "Diffusion-based Image Translation using Disentangled Style and Content Representation", ICLR, 2023 (*KAIST*). [[Paper](https://arxiv.org/abs/2209.15264)]
* **MATEBIT**: "Masked and Adaptive Transformer for Exemplar Based Image Translation", CVPR, 2023 (*Hangzhou Dianzi University*). [[Paper](https://arxiv.org/abs/2303.17123)][[Pytorch](https://github.com/AiArt-HDU/MATEBIT)]
* **IPL**: "Zero-shot Generative Model Adaptation via Image-specific Prompt Learning", CVPR, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2304.03119)][[PyTorch](https://github.com/Picsart-AI-Research/IPL-Zero-Shot-Generative-Model-Adaptation)]
* **Master**: "Master: Meta Style Transformer for Controllable Zero-Shot and Few-Shot Artistic Style Transfer", CVPR, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2304.11818)]
* **LENeRF**: "Local 3D Editing via 3D Distillation of CLIP Knowledge", CVPR, 2023 (*Kakao*). [[Paper](https://arxiv.org/abs/2306.12570)]
* **SINE**: "SINE: SINgle Image Editing with Text-to-Image Diffusion Models", CVPR, 2023 (*Rutgers*). [[Paper](https://arxiv.org/abs/2212.04489)][[PyTorch](https://github.com/zhang-zx/SINE)]
* **Imagic**: "Imagic: Text-Based Real Image Editing with Diffusion Models", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2210.09276)][[Website](https://imagic-editing.github.io/)]
* **DATID-3D**: "DATID-3D: Diversity-Preserved Domain Adaptation Using Text-to-Image Diffusion for 3D Generative Model", CVPR, 2023 (*SNU*). [[Paper](https://arxiv.org/abs/2211.16374)][[PyTorch](https://github.com/gwang-kim/DATID-3D)][[Website](https://gwang-kim.github.io/datid_3d/)]
* **Null-text-Inversion**: "Null-text Inversion for Editing Real Images using Guided Diffusion Models", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2211.09794)]
* **LANIT**: "LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data", CVPR, 2023 (*Korea University*). [[Paper](https://arxiv.org/abs/2208.14889)][[PyTorch](https://github.com/KU-CVLAB/LANIT)]
* **StylerDALLE**: "StylerDALLE: Language-Guided Style Transfer Using a Vector-Quantized Tokenizer of a Large-Scale Generative Model", ICCV, 2023 (*University of Trento, Italy*). [[Paper](https://arxiv.org/abs/2303.09268)][[PyTorch](https://github.com/zipengxuc/StylerDALLE)]
* ****: "Disentangling Structure and Appearance in ViT Feature Space", ACM ToG, 2023 (*Weizmann Institute of Science (WIS), Israel*). [[Paper](https://arxiv.org/abs/2311.12193)][[PyTorch](https://github.com/omerbt/Splice)][[Website](https://splice-vit.github.io/)]
* **pix2pix-zero**: "Zero-shot Image-to-Image Translation", arXiv, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2302.03027)][[Code (in construction)](https://github.com/pix2pixzero/pix2pix-zero)][[Website](https://pix2pixzero.github.io/)]
* **SpectralCLIP**: "SpectralCLIP: Preventing Artifacts in Text-Guided Style Transfer from a Spectral Perspective", arXiv, 2023 (*University of Trento, Italy*). [[Paper](https://arxiv.org/abs/2303.09270)][[Code (in construction)](https://github.com/zipengxuc/SpectralCLIP)]
* **PGIC**: "A Unified Prompt-Guided In-Context Inpainting Framework for Reference-based Image Manipulations", arXiv, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2305.11577)][[Code (in construction)](https://github.com/ewrfcas/PGIC_inpainting)]

[[Back to Overview](#overview)]

### Other Low-Level Tasks
* Colorization: 
    * **ColTran**: "Colorization Transformer", ICLR, 2021 (*Google*). [[Paper](https://openreview.net/forum?id=5NA1PinlGFu)][[Tensorflow](https://github.com/google-research/google-research/tree/master/coltran)]
    * **ViT-I-GAN**: "ViT-Inception-GAN for Image Colourising", arXiv, 2021 (*D.Y Patil College of Engineering, India*). [[Paper](https://arxiv.org/abs/2106.06321)]
    * **CT<sup>2</sup>**: "CT<sup>2</sup>: Colorization Transformer via Color Tokens", ECCV, 2022 (*Peking University*). [[Paper](https://ci.idm.pku.edu.cn/Weng_ECCV22b.pdf)][[PyTorch](https://github.com/shuchenweng/CT2)]
    * **L-CoDer**: "L-CoDer: Language-based Colorization with Color-object Decoupling Transformer", ECCV, 2022 (*Beijing University of Posts and Telecommunications*). [[Paper](https://ci.idm.pku.edu.cn/Weng_ECCV22g.pdf)]
    * **ColorFormer**: "ColorFormer: Image Colorization via Color Memory assisted Hybrid-attention Transformer", ECCV, 2022 (*Tencent*). [[Paper](https://byjiang.com/assets/pdf/ColorFormer.pdf)]
    * **UniColor**: "UniColor: A Unified Framework for Multi-Modal Colorization with Transformer", SIGGRAPH Asia, 2022 (*CUHK*). [[Paper](https://arxiv.org/abs/2209.11223)][[Website](https://luckyhzt.github.io/unicolor/)]
    * **iColoriT**: "iColoriT: Towards Propagating Local Hint to the Right Region in Interactive Colorization by Leveraging Vision Transformer", arXiv, 2022 (*KAIST*). [[Paper](https://arxiv.org/abs/2207.06831)]
    * **L-CoIns**: "L-CoIns: Language-based Colorization with Instance Awareness", CVPR, 2023 (*Beijing University of Posts and Telecommunications*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Chang_L-CoIns_Language-Based_Colorization_With_Instance_Awareness_CVPR_2023_paper.html)]
    * **L-CAD**: "L-CAD: Language-based Colorization with Any-level Descriptions using Diffusion Priors", NeurIPS, 2023 (*Beijing University of Posts and Telecommunications*). [[Paper](https://arxiv.org/abs/2305.15217)][[PyTorch](https://github.com/changzheng123/L-CAD)]
* Enhancement:
    * **PanFormer**: "PanFormer: a Transformer Based Model for Pan-sharpening", ICME, 2022 (*Beihang University*). [[Paper](https://arxiv.org/abs/2203.02916)][[PyTorch](https://github.com/zhysora/PanFormer)]
    * **URSCT-UIE**: "Reinforced Swin-Convs Transformer for Underwater Image Enhancement", arXiv, 2022 (*Ningbo University*). [[Paper](https://arxiv.org/abs/2205.00434)]
    * **IAT**: "Illumination Adaptive Transformer", arXiv, 2022 (*The University of Tokyo*). [[Paper](https://arxiv.org/abs/2205.14871)][[PyTorch](https://github.com/cuiziteng/Illumination-Adaptive-Transformer)]
    * **SPGAT**: "Structural Prior Guided Generative Adversarial Transformers for Low-Light Image Enhancement", arXiv, 2022 (*The Hong Kong Polytechnic University*). [[Paper](https://arxiv.org/abs/2207.07828)]
    * **SSTF**: "End-to-end Transformer for Compressed Video Quality Enhancement", arXiv, 2022 (*Nanjing University of Information Science and Technology*). [[Paper](https://arxiv.org/abs/2210.13827)]
    * **CLIP-LiT**: "Iterative Prompt Learning for Unsupervised Backlit Image Enhancement", ICCV, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2303.17569)][[PyTorch](https://github.com/ZhexinLiang/CLIP-LIT)][[Website](https://zhexinliang.github.io/CLIP_LIT_page/)]
    * **Retinexformer**: "Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement", ICCV, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2303.06705)][[PyTorch](https://github.com/caiyuanhao1998/Retinexformer)]
* High Dynamic Range (HDR):
    * **CA-ViT**: "Ghost-free High Dynamic Range Imaging with Context-aware Transformer", ECCV, 2022 (*Megvii*). [[Paper](https://arxiv.org/abs/2208.05114)][[PyTorch](https://github.com/megvii-research/HDR-Transformer)]
    * **Selective-TransHDR**: "Selective TransHDR: Transformer-Based Selective HDR Imaging Using Ghost Region Mask", ECCV, 2022 (*Sogang University, Korea*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/6670_ECCV_2022_paper.php)]
    * **Text2Light**: "Text2Light: Zero-Shot Text-Driven HDR Panorama Generation", SIGGRAPH Asia, 2022 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2209.09898)][[PyTorch](https://github.com/FrozenBurning/Text2Light)][[Website](https://frozenburning.github.io/projects/text2light/)]
    * **SMAE**: "SMAE: Few-shot Learning for HDR Deghosting with Saturation-Aware Masked Autoencoders", CVPR, 2023 (*Northwestern Polytechnical University*). [[Paper](https://arxiv.org/abs/2304.06914)]
    * **SCTNet**: "Alignment-free HDR Deghosting with Semantics Consistent Transformer", ICCV, 2023 (*University of Bourgogne, France*). [[Paper](https://arxiv.org/abs/2305.18135)][[Website](https://steven-tel.github.io/sctnet/)]
    * **?**: "Online Overexposed Pixels Hallucination in Videos with Adaptive Reference Frame Selection", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2308.15462)]
    * **IFT**: "IFT: Image Fusion Transformer for Ghost-free High Dynamic Range Imaging", arXiv, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2309.15019)]
* Harmonization:
    * **HT**: "Image Harmonization With Transformer", ICCV, 2021 (*Ocean University of China*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Guo_Image_Harmonization_With_Transformer_ICCV_2021_paper.html)]
    * **LEMaRT**: "LEMaRT: Label-Efficient Masked Region Transform for Image Harmonization", CVPR, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2304.13166)]
* Compression:
    * **?**: "Towards End-to-End Image Compression and Analysis with Transformers", AAAI, 2022 (*1Harbin Institute of Technology*). [[Paper](https://arxiv.org/abs/2112.09300)][[PyTorch](https://github.com/BYchao100/Towards-Image-Compression-and-Analysis-with-Transformers)]
    * **Entroformer**: "Entroformer: A Transformer-based Entropy Model for Learned Image Compression", ICLR, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2202.05492)]
    * **STF**: "The Devil Is in the Details: Window-based Attention for Image Compression", CVPR, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2203.08450)][[PyTorch](https://github.com/Googolxx/STF)]
    * **Contextformer**: "Contextformer: A Transformer with Spatio-Channel Attention for Context Modeling in Learned Image Compression", ECCV, 2022 (*TUM*). [[Paper](https://arxiv.org/abs/2203.02452)]
    * **VCT**: "VCT: A Video Compression Transformer", NeurIPS, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2206.07307)]
    * **MIMT**: "MIMT: Masked Image Modeling Transformer for Video Compression", ICLR, 2023 (*Tencent*). [[Paper](https://openreview.net/forum?id=j9m-mVnndbm)]
    * **TCM**: "Learned Image Compression with Mixed Transformer-CNN Architectures", CVPR, 2023 (*Waseda University*). [[Paper](https://arxiv.org/abs/2303.14978)][[PyTorch](https://github.com/jmliu206/LIC_TCM)]
    * **TransTIC**: "TransTIC: Transferring Transformer-based Image Compression from Human Perception to Machine Perception", ICCV, 2023 (*NYCU*). [[Paper](https://arxiv.org/abs/2306.05085)]
    * **Prompt-ICM**: "Prompt-ICM: A Unified Framework towards Image Coding for Machines with Task-driven Prompts", arXiv, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2305.02578)]
    * **FAT-LIC**: "Frequency-Aware Transformer for Learned Image Compression", arXiv, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2310.16387)]
* Matting:
    * **MatteFormer**: "MatteFormer: Transformer-Based Image Matting via Prior-Tokens", CVPR, 2022 (*SNU + NAVER*). [[Paper](https://arxiv.org/abs/2203.15662)][[PyTorch](https://github.com/webtoon/matteformer)]
    * **TransMatting**: "TransMatting: Enhancing Transparent Objects Matting with Transformers", ECCV, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2208.03007)][[Code (in construction)](https://github.com/AceCHQ/TransMatting)]
    * **VMFormer**: "VMFormer: End-to-End Video Matting with Transformer", arXiv, 2022 (*PicsArt*). [[Paper](https://arxiv.org/abs/2208.12801)][[PyTorch](https://github.com/SHI-Labs/VMFormer)][[Website](https://chrisjuniorli.github.io/project/VMFormer/)]
    * **CLIPMat**: "Referring Image Matting", CVPR, 2023 (*The University of Sydney*). [[Paper](https://arxiv.org/abs/2206.05149)][[Code (in construction)](https://github.com/JizhiziLi/RIM)]
    * **ViTMatte**: "ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers", arXiv, 2023 (*Xiaobing.AI*). [[Paper](https://arxiv.org/abs/2305.15272)]
    * **MAM**: "Matting Anything", arXiv, 2023 (*UIUC*). [[Paper](https://arxiv.org/abs/2306.05399)][[PyTorch](https://github.com/SHI-Labs/Matting-Anything)][[Website](https://chrisjuniorli.github.io/project/Matting-Anything/)]
* Reconstruction:
    * **ET-Net**: "Event-Based Video Reconstruction Using Transformer", ICCV, 2021 (*University of Science and Technology of China*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Weng_Event-Based_Video_Reconstruction_Using_Transformer_ICCV_2021_paper.html)][[PyTorch](https://github.com/WarranWeng/ET-Net)]
    * **GradViT**: "GradViT: Gradient Inversion of Vision Transformers", CVPR, 2022 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2203.11894)][[Website](https://gradvit.github.io/)]
    * **MST**: "Mask-guided Spectral-wise Transformer for Efficient Hyperspectral Image Reconstruction", CVPR, 2022 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2111.07910)][[PyTorch](https://github.com/caiyuanhao1998/MST)]
    * **MST++**: "MST++: Multi-stage Spectral-wise Transformer for Efficient Spectral Reconstruction", CVPRW, 2022 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2204.07908)][[PyTorch](https://github.com/caiyuanhao1998/MST-plus-plus)]
    * **CST**: "Coarse-to-Fine Sparse Transformer for Hyperspectral Image Reconstruction", ECCV, 2022 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2203.04845)][[PyTorch](https://github.com/caiyuanhao1998/MST)]
    * **DAUHST**: "Degradation-Aware Unfolding Half-Shuffle Transformer for Spectral Compressive Imaging", NeurIPS, 2022 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2205.10102)][[PyTorch](https://github.com/caiyuanhao1998/MST)]
    * **S<sup>2</sup>-Transformer**: "S<sup>2</sup>-Transformer for Mask-Aware Hyperspectral Image Reconstruction", arXiv, 2022 (*Rochester Institute of Technology*). [[Paper](https://arxiv.org/abs/2209.12075)]
    * **NLOST**: "NLOST: Non-Line-of-Sight Imaging with Transformer", CVPR, 2023 (*USTC*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Li_NLOST_Non-Line-of-Sight_Imaging_With_Transformer_CVPR_2023_paper.html)][[Code (in construction)](https://github.com/Depth2World/NLOST)]
    * **MinD-Vis**: "Seeing Beyond the Brain: Conditional Diffusion Model with Sparse Masked Modeling for Vision Decoding", CVPR, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2211.06956)][[PyTorch](https://github.com/zjc062/mind-vis)][[Website](https://mind-vis.github.io/)]
    * **PADUT**: "Pixel Adaptive Deep Unfolding Transformer for Hyperspectral Image Reconstruction", ICCV, 2023 (*Beijing Institute of Technology*). [[Paper](https://arxiv.org/abs/2308.10820)][[PyTorch](https://github.com/MyuLi/PADUT)]
    * **GTA**: "Global-correlated 3D-decoupling Transformer for Clothed Avatar Reconstruction", NeurIPS, 2023 (*Zhejiang*). [[Paper](https://arxiv.org/abs/2309.13524)]
* Radiance Fields:
    * **NeXT**: "NeXT: Towards High Quality Neural Radiance Fields via Multi-Skip Transformer", ECCV, 2022 (*Tsinghua University*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1810_ECCV_2022_paper.php)][[JAX](https://github.com/Crishawy/NeXT)]
    * **TransNeRF**: "Generalizable Neural Radiance Fields for Novel View Synthesis with Transformer", arXiv, 2022 (*UBC*). [[Paper](https://arxiv.org/abs/2206.05375)]
    * **ABLE-NeRF**: "ABLE-NeRF: Attention-Based Rendering with Learnable Embeddings for Neural Radiance Field", CVPR, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2303.13817)]
    * **TransHuman**: "TransHuman: A Transformer-based Human Representation for Generalizable Neural Human Rendering", ICCV, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2307.12291)][[PyTorch](https://github.com/pansanity666/TransHuman/)][[Website](https://pansanity666.github.io/TransHuman/)]
    * **GNT-MOVE**: "Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts", ICCV, 2023 (*UT Austin*). [[Paper](https://arxiv.org/abs/2308.11793)][[PyTorch](https://github.com/VITA-Group/GNT-MOVE)]
    * **ReTR**: "ReTR: Modeling Rendering Via Transformer for Generalizable Neural Surface Reconstruction", NeurIPS, 2023 (*HKUST*). [[Paper](https://arxiv.org/abs/2305.18832)][[PyTorch](https://github.com/YixunLiang/ReTR)]
* 3D:
    * **MNSRNet**: "MNSRNet: Multimodal Transformer Network for 3D Surface Super-Resolution", CVPR, 2022 (*Shenzhen University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Xie_MNSRNet_Multimodal_Transformer_Network_for_3D_Surface_Super-Resolution_CVPR_2022_paper.html)]
* Others:
    * **TransMEF**: "TransMEF: A Transformer-Based Multi-Exposure Image Fusion Framework using Self-Supervised Multi-Task Learning", AAAI, 2022 (*Fudan*). [[Paper](https://arxiv.org/abs/2112.01030)]
    * **MS-Unet**: "Semi-Supervised Wide-Angle Portraits Correction by Multi-Scale Transformer", CVPR, 2022 (*Megvii*). [[Paper](https://arxiv.org/abs/2109.08024)][[Code (in construction)](https://github.com/megvii-research/Portraits_Correction)]
    * **TransCL**: "TransCL: Transformer Makes Strong and Flexible Compressive Learning", TPAMI, 2022 (*Peking University*). [[Paper](https://arxiv.org/abs/2207.11972)][[Code (in construction)](https://github.com/MC-E/TransCL/)]
    * **GAP-CSCoT**: "Spectral Compressive Imaging Reconstruction Using Convolution and Spectral Contextual Transformer", arXiv, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2201.05768)]
    * **MatFormer**: "MatFormer: A Generative Model for Procedural Materials", arXiv, 2022 (*Adobe*). [[Paper](https://arxiv.org/abs/2207.01044)]
    * **FishFormer**: "FishFormer: Annulus Slicing-based Transformer for Fisheye Rectification with Efficacy Domain Exploration", arXiv, 2022 (*Beijing Jiaotong University*). [[Paper](https://arxiv.org/abs/2207.01925)]
    * **STFormer**: "Spatial-Temporal Transformer for Video Snapshot Compressive Imaging", arXiv, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2209.01578)][[PyTorch](https://github.com/ucaswangls/STFormer)]
    * **OCTUF**: "Optimization-Inspired Cross-Attention Transformer for Compressive Sensing", CVPR, 2023 (*Peking University*). [[Paper](https://arxiv.org/abs/2304.13986)][[PyTorch](https://github.com/songjiechong/OCTUF)]
    * **TopNet**: "TopNet: Transformer-based Object Placement Network for Image Compositing", CVPR, 2023 (*Adobe*). [[Paper](https://arxiv.org/abs/2304.03372)]
    * **RHWF**: "Recurrent Homography Estimation Using Homography-Guided Image Warping and Focus Transformer", CVPR, 2023 (*Zhejiang University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Cao_Recurrent_Homography_Estimation_Using_Homography-Guided_Image_Warping_and_Focus_Transformer_CVPR_2023_paper.html)][[Code (in construction)](https://github.com/imdumpl78/rhwf)]
    * **M2T**: "M2T: Masking Transformers Twice for Faster Decoding", ICCV, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2304.07313)]
    * **CTM**: "Unfolding Framework with Prior of Convolution-Transformer Mixture and Uncertainty Estimation for Video Snapshot Compressive Imaging", ICCV, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2306.11316)]
    * **PromptGIP**: "Unifying Image Processing as Visual Prompting Question Answering", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2310.10513)]

[[Back to Overview](#overview)]


## Reinforcement Learning
### Navigation
* **VTNet**: "VTNet: Visual Transformer Network for Object Goal Navigation", ICLR, 2021 (*ANU*). [[Paper](https://openreview.net/forum?id=DILxQP08O3B)]
* **MaAST**: "MaAST: Map Attention with Semantic Transformersfor Efficient Visual Navigation", ICRA, 2021 (*SRI*). [[Paper](https://arxiv.org/abs/2103.11374)]
* **TransFuser**: "Multi-Modal Fusion Transformer for End-to-End Autonomous Driving", CVPR, 2021 (*MPI*). [[Paper](https://arxiv.org/abs/2104.09224)][[PyTorch](https://github.com/autonomousvision/transfuser)]
* **CMTP**: "Topological Planning With Transformers for Vision-and-Language Navigation", CVPR, 2021 (*Stanford*). [[Paper](https://arxiv.org/abs/2012.05292)]
* **VLN-BERT**: "VLN-BERT: A Recurrent Vision-and-Language BERT for Navigation", CVPR, 2021 (*ANU*). [[Paper](https://arxiv.org/abs/2011.13922)][[PyTorch](https://github.com/YicongHong/Recurrent-VLN-BERT)]
* **E.T.**: "Episodic Transformer for Vision-and-Language Navigation", ICCV, 2021 (*Google*). [[Paper](https://arxiv.org/abs/2105.06453)][[PyTorch](https://github.com/alexpashevich/E.T.)]
* **HAMT**: "History Aware Multimodal Transformer for Vision-and-Language Navigation", NeurIPS, 2021 (*INRIA*). [[Paper](https://arxiv.org/abs/2110.13309)][[PyTorch](https://github.com/cshizhe/VLN-HAMT)][[Website](https://cshizhe.github.io/projects/vln_hamt.html)]
* **SOAT**: "SOAT: A Scene- and Object-Aware Transformer for Vision-and-Language Navigation", NeurIPS, 2021 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2110.14143)]
* **OMT**: "Object Memory Transformer for Object Goal Navigation", ICRA, 2022 (*AIST, Japan*). [[Paper](https://arxiv.org/abs/2203.14708)]
* **ADAPT**: "ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts", CVPR, 2022 (*Huawei*). [[Paper](https://arxiv.org/abs/2205.15509)]
* **DUET**: "Think Global, Act Local: Dual-scale Graph Transformer for Vision-and-Language Navigation", CVPR, 2022 (*INRIA*). [[Paper](https://arxiv.org/abs/2202.11742)][[Website](https://cshizhe.github.io/projects/vln_duet.html)]
* **LSA**: "Local Slot Attention for Vision-and-Language Navigation", ICMR, 2022 (*Fudan*). [[Paper](https://arxiv.org/abs/2206.08645)]
* **?**: "Learning from Unlabeled 3D Environments for Vision-and-Language Navigation", ECCV, 2022 (*INRIA*). [[Paper](https://arxiv.org/abs/2208.11781)][[Website](https://cshizhe.github.io/projects/hm3d_autovln.html)]
* **MTVM**: "Multimodal Transformer with Variable-length Memory for Vision-and-Language Navigation", ECCV, 2022 (*ByteDance*). [[Paper](https://arxiv.org/abs/2111.05759)][[PyTorch](https://github.com/clin1223/MTVM)]
* **DDL**: "Learning Disentanglement with Decoupled Labels for Vision-Language Navigation", ECCV, 2022 (*Beijing Institute of Technology*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3622_ECCV_2022_paper.php)][[PyTorch](https://github.com/cwhao98/DDL)]
* **Sim2Sim**: "Sim-2-Sim Transfer for Vision-and-Language Navigation in Continuous Environments", ECCV, 2022 (*Oregon State University*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5303_ECCV_2022_paper.php)][[PyTorch](https://github.com/jacobkrantz/Sim2Sim-VLNCE)][[Website](https://jacobkrantz.github.io/sim-2-sim)]
* **AVLEN**: "AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments", NeurIPS, 2022 (*UC Riverside*). [[Paper](https://arxiv.org/abs/2210.07940)]
* **ZSON**: "ZSON: Zero-Shot Object-Goal Navigation using Multimodal Goal Embeddings", NeurIPS, 2022 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2206.12403)]
* **WS-MGMap**: "Weakly-Supervised Multi-Granularity Map Learning for Vision-and-Language Navigation", NeurIPS, 2022 (*South China University of Technology*). [[Paper](https://arxiv.org/abs/2210.07506)][[PyTorch (in construction)](https://github.com/PeihaoChen/WS-MGMap)]
* **CLIP-Nav**: "CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation", CoRLW, 2022 (*Amazon*). [[Paper](https://arxiv.org/abs/2211.16649)]
* **TransFuser**: "TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving", arXiv, 2022 (*MPI*). [[Paper](https://arxiv.org/abs/2205.15997)]
* **TD-STP**: "Target-Driven Structured Transformer Planner for Vision-Language Navigation", arXiv, 2022 (*Beihang University*). [[Paper](https://arxiv.org/abs/2207.11201)][[Code (in construction)](https://github.com/YushengZhao/TD-STP)]
* **DAVIS**: "Anticipating the Unseen Discrepancy for Vision and Language Navigation", arXiv, 2022 (*UCSB*). [[Paper](https://arxiv.org/abs/2209.04725)]
* **LOViS**: "LOViS: Learning Orientation and Visual Signals for Vision and Language Navigation", arXiv, 2022 (*Michigan State*). [[Paper](https://arxiv.org/abs/2209.12723)]
* **BEVBert**: "BEVBert: Topo-Metric Map Pre-training for Language-guided Navigation", arXiv, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2212.04385)]
* **Meta-Explore**: "Meta-Explore: Exploratory Hierarchical Vision-and-Language Navigation Using Scene Object Spectrum Grounding", CVPR, 2023 (*Seoul National University*). [[Paper](https://arxiv.org/abs/2303.04077)][[Website](https://rllab-snu.github.io/projects/Meta-Explore/doc.html)]
* **LANA**: "Lana: A Language-Capable Navigator for Instruction Following and Generation", CVPR, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2303.08409)][[PyTorch (in construction)](https://github.com/wxh1996/LANA-VLN)]
* **KERM**: "KERM: Knowledge Enhanced Reasoning for Vision-and-Language Navigation", CVPR, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2303.15796)][[PyTorch](https://github.com/XiangyangLi20/KERM)]
* **VLN-SIG**: "Improving Vision-and-Language Navigation by Generating Future-View Image Semantics", CVPR, 2023 (*UNC*). [[Paper](https://arxiv.org/abs/2304.04907)][[PyTorch](https://github.com/jialuli-luka/VLN-SIG)][[Website](https://jialuli-luka.github.io/VLN-SIG)]
* **GeoVLN**: "GeoVLN: Learning Geometry-Enhanced Visual Representation with Slot Attention for Vision-and-Language Navigation", CVPR, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2305.17102)]
* **IVLN**: "Iterative Vision-and-Language Navigation", CVPR, 2023 (*Oregon State University*). [[Paper](https://arxiv.org/abs/2210.03087)]
* **AZHP**: "Adaptive Zone-aware Hierarchical Planner for Vision-Language Navigation", CVPR, 2023 (*Beihang University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Gao_Adaptive_Zone-Aware_Hierarchical_Planner_for_Vision-Language_Navigation_CVPR_2023_paper.html)][[Code (in construction)](https://github.com/chengaopro/AZHP)]
* **MARVAL**: "A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2210.03112)]
* **VO-Transformer**: "Modality-invariant Visual Odometry for Embodied Vision", CVPR, 2023 (*EPFL*). [[Paper](https://arxiv.org/abs/2305.00348)][[Website](https://memmelma.github.io/vot/)]
* **VLN-Behave**: "Behavioral Analysis of Vision-and-Language Navigation Agents", CVPR, 2023 (*Oregon State*). [[Paper](https://arxiv.org/abs/2307.10790)][[Code](https://github.com/Yoark/vln-behave)]
* **Lily**: "Learning Vision-and-Language Navigation from YouTube Videos", ICCV, 2023 (*South China University of Technology*). [[Paper](https://arxiv.org/abs/2307.11984)][[PyTorch](https://github.com/JeremyLinky/YouTube-VLN)]
* **ScaleVLN**: "Scaling Data Generation in Vision-and-Language Navigation", ICCV, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2307.15644)][[PyTorch](https://github.com/wz0919/ScaleVLN)]
* **BSG**: "Bird's-Eye-View Scene Graph for Vision-Language Navigation", ICCV, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2308.04758)][[Code (in construction)](https://github.com/DefaultRui/BEV-Scene-Graph)]
* **AerialVLN**: "AerialVLN: Vision-and-Language Navigation for UAVs", ICCV, 2023 (*Northwestern Polytechnical University*). [[Paper](https://arxiv.org/abs/2308.06735)][[PyTorch](https://github.com/AirVLN/AirVLN)]
* **DREAMWALKER**: "DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation", ICCV, 2023 (*Beijing Institute of Technology*). [[Paper](https://arxiv.org/abs/2308.07498)][[Code (in construction)](https://github.com/hanqingwangai/Dreamwalker)]
* **VLN-PETL**: "VLN-PETL: Parameter-Efficient Transfer Learning for Vision-and-Language Navigation", ICCV, 2023 (*The University of Adelaide, Australia*). [[Paper](https://arxiv.org/abs/2308.10172)][[Code (in construction)](https://github.com/YanyuanQiao/VLN-PETL)]
* **MiC**: "March in Chat: Interactive Prompting for Remote Embodied Referring Expression", ICCV, 2023 (*The University of Adelaide, Australia*). [[Paper](https://arxiv.org/abs/2308.10141)][[Code (in construction)](https://github.com/YanyuanQiao/MiC)]
* **GELA**: "Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation", ICCV, 2023 (*Chinese Academy of Military Science*). [[Paper](https://arxiv.org/abs/2308.12587)][[PyTorch](https://github.com/CSir1996/VLN-GELA)]
* **GridMM**: "GridMM: Grid Memory Map for Vision-and-Language Navigation", ICCV, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2307.12907)][[PyTorch](https://github.com/MrZihan/GridMM)]
* **LLM-Planner**: "LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models", ICCV, 2023 (*OSU*). [[Paper](https://arxiv.org/abs/2212.04088)][[Code (in construction)](https://github.com/OSU-NLP-Group/LLM-Planner/)][[Website](https://dki-lab.github.io/LLM-Planner/)]
* **Le-RNR-Map**: "Language-enhanced RNR-Map: Querying Renderable Neural Radiance Field maps with natural language", ICCVW, 2023 (*University of Verona, Italy*). [[Paper](https://arxiv.org/abs/2308.08854)][[Code (in construction)](https://github.com/intelligolabs/Le-RNR-Map)][[Website](https://intelligolabs.github.io/Le-RNR-Map/)]
* **LACMA**: "LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following", EMNLP, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2310.12344)][[PyTorch](https://github.com/joeyy5588/LACMA)]
* **FGPrompt**: "FGPrompt: Fine-grained Goal Prompting for Image-goal Navigation", NeurIPS, 2023 (*South China University of Technology*). [[Paper](https://arxiv.org/abs/2310.07473)][[PyTorch](https://github.com/XinyuSun/FGPrompt)][[Website](https://xinyusun.github.io/fgprompt-pages/)]
* **PanoGen**: "PanoGen: Text-Conditioned Panoramic Environment Generation for Vision-and-Language Navigation", NeurIPS, 2023 (*UNC*). [[Paper](https://arxiv.org/abs/2305.19195)][[PyTorch](https://github.com/jialuli-luka/PanoGen)][[Website](https://pano-gen.github.io/)]
* **MLANet**: "MLANet: Multi-Level Attention Network with Sub-instruction for Continuous Vision-and-Language Navigation", arXiv, 2023 (*Tongji University*). [[Paper](https://arxiv.org/abs/2303.01396)][[PyTorch](https://github.com/RavenKiller/MLA)]
* **ENTL**: "ENTL: Embodied Navigation Trajectory Learner", arXiv, 2023 (*AI2*). [[Paper](https://arxiv.org/abs/2304.02639)]
* **MPM**: "Masked Path Modeling for Vision-and-Language Navigation", arXiv, 2023 (*UCLA*). [[Paper](https://arxiv.org/abs/2305.14268)]
* **NavGPT**: "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models", arXiv, 2023 (*The University of Adelaide, Australia*). [[Paper](https://arxiv.org/abs/2305.16986)]
* **MO-VLN**: "MO-VLN: A Multi-Task Benchmark for Open-set Zero-Shot Vision-and-Language Navigation", arXiv, 2023 (*Sun Yat-Sen University*). [[Paper](https://arxiv.org/abs/2306.10322)][[Code (in construction)](https://github.com/mligg23/MO-VLN/)][[Website](https://mligg23.github.io/MO-VLN-Site/)]
* **ViNT**: "ViNT: A Foundation Model for Visual Navigation", arXiv, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2306.14846)][[Code (in construction)](https://github.com/PrieureDeSion/visualnav-transformer)][[Website](https://visualnav-transformer.github.io/)]
* **A<sup>2</sup>Nav**: "A<sup>2</sup>Nav: Action-Aware Zero-Shot Robot Navigation by Exploiting Vision-and-Language Ability of Foundation Models", arXiv, 2023 (*South China University of Technology*). [[Paper](https://arxiv.org/abs/2308.07997)]
* **LangNav**: "LangNav: Language as a Perceptual Representation for Navigation", arXiv, 2023 (*MIT*). [[Paper](https://arxiv.org/abs/2310.07889)]
* **?**: "Multimodal Large Language Model for Visual Navigation", arXiv, 2023 (*Apple*). [[Paper](https://arxiv.org/abs/2310.08669)]
* **MapGPT**: "MapGPT: Map-Guided Prompting for Unified Vision-and-Language Navigation", arXiv, 2024 (*HKU*). [[Paper](https://arxiv.org/abs/2401.07314)]

[[Back to Overview](#overview)]

### Other RL Tasks
* **SVEA**: "Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation", arXiv, 2021 (*UCSD*). [[Paper](https://arxiv.org/abs/2107.00644)][[GitHub](https://github.com/nicklashansen/dmcontrol-generalization-benchmark)][[Website](https://nicklashansen.github.io/SVEA/)]
* **LocoTransformer**: "Learning Vision-Guided Quadrupedal Locomotion End-to-End with Cross-Modal Transformers", ICLR, 2022 (*UCSD*). [[Paper](https://arxiv.org/abs/2107.03996)][[Website](https://rchalyang.github.io/LocoTransformer/)]
* **STAM**: "Consistency driven Sequential Transformers Attention Model for Partially Observable Scenes", CVPR, 2022 (*McGill University, Canada*). [[Paper](https://arxiv.org/abs/2204.00656)][[PyTorch](https://github.com/samrudhdhirangrej/STAM-Sequential-Transformers-Attention-Model)]
* **CtrlFormer**: "CtrlFormer: Learning Transferable State Representation for Visual Control via Transformer", ICML, 2022 (*HKU*). [[Paper](https://arxiv.org/abs/2206.08883)][[PyTorch](https://github.com/YaoMarkMu/CtrlFormer-ICML2022)][[Website](https://sites.google.com/view/ctrlformer-icml/)]
* **PromptDT**: "Prompting Decision Transformer for Few-Shot Policy Generalization", ICML, 2022 (*CMU*). [[Paper](https://arxiv.org/abs/2206.13499)][[Website](https://mxu34.github.io/PromptDT/)]
* **StARformer**: "StARformer: Transformer with State-Action-Reward Representations for Visual Reinforcement Learning", ECCV, 2022 (*Stony Brook*). [[Paper](https://arxiv.org/abs/2110.06206)][[PyTorch](https://github.com/elicassion/StARformer)]
* **RAD**: "Evaluating Vision Transformer Methods for Deep Reinforcement Learning from Pixels", arXiv, 2022 (*UBC, Canada*). [[Paper](https://arxiv.org/abs/2204.04905)]
* **MWM**: "Masked World Models for Visual Control", arXiv, 2022 (*Berkeley*). [[Paper](https://arxiv.org/abs/2206.14244)][[Tensorflow](https://github.com/younggyoseo/MWM)][[Website](https://sites.google.com/view/mwm-rl)]
* **IRIS**: "Transformers are Sample Efficient World Models", arXiv, 2022 (*University of Geneva, Switzerland*). [[Paper](https://arxiv.org/abs/2209.00588)][[PyTorch](https://github.com/eloialonso/iris)]
* **InstructRL**: "Instruction-Following Agents with Jointly Pre-Trained Vision-Language Models", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2210.13431)]
* **STG-Transformer**: "Learning from Visual Observation via Offline Pretrained State-to-Go Transformer", NeurIPS, 2023 (*BAAI*). [[Paper](https://arxiv.org/abs/2306.12860)][[Code (in construction)](https://github.com/zhoubohan0/STG-Transformer)][[Website](https://sites.google.com/view/stgtransformer)]

[[Back to Overview](#overview)]


## Medical
### Medical Segmentation
* **Cross-Transformer**: "The entire network structure of Crossmodal Transformer", ICBSIP, 2021 (*Capital Medical University*). [[Paper](https://arxiv.org/abs/2104.14273)]
* **Segtran**: "Medical Image Segmentation using Squeeze-and-Expansion Transformers", IJCAI, 2021 (*A\*STAR*). [[Paper](https://arxiv.org/abs/2105.09511)]
* **i-ViT**: "Instance-based Vision Transformer for Subtyping of Papillary Renal Cell Carcinoma in Histopathological Image", MICCAI, 2021 (*Xi'an Jiaotong University*). [[Paper](https://arxiv.org/abs/2106.12265)][[PyTorch](https://github.com/ZeyuGaoAi/Instance_based_Vision_Transformer)][[Website](https://dataset.chenli.group/home/prcc-subtyping)]
* **UTNet**: "UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation", MICCAI, 2021 (*Rutgers*). [[Paper](https://arxiv.org/abs/2107.00781)]
* **MCTrans**: "Multi-Compound Transformer for Accurate Biomedical Image Segmentation", MICCAI, 2021 (*HKU + CUHK*). [[Paper](https://arxiv.org/abs/2106.14385)][[Code (in construction)](https://github.com/JiYuanFeng/MCTrans)]
* **Polyformer**: "Few-Shot Domain Adaptation with Polymorphic Transformers", MICCAI, 2021 (*A\*STAR*). [[Paper](https://arxiv.org/abs/2107.04805)][[PyTorch](https://github.com/askerlee/segtran)]
* **BA-Transformer**: "Boundary-aware Transformers for Skin Lesion Segmentation". MICCAI, 2021 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2110.03864)][[PyTorch](https://github.com/jcwang123/BA-Transformer)]
* **GT-U-Net**: "GT U-Net: A U-Net Like Group Transformer Network for Tooth Root Segmentation", MICCAIW, 2021 (*Hangzhou Dianzi University*). [[Paper](https://arxiv.org/abs/2109.14813)][[PyTorch](https://github.com/Kent0n-Li/GT-U-Net)]
* **STN**: "Automatic size and pose homogenization with spatial transformer network to improve and accelerate pediatric segmentation", ISBI, 2021 (*Institut Polytechnique de Paris*). [[Paper](https://arxiv.org/abs/2107.02655)]
* **T-AutoML**: "T-AutoML: Automated Machine Learning for Lesion Segmentation Using Transformers in 3D Medical Imaging", ICCV, 2021 (*NVIDIA*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Yang_T-AutoML_Automated_Machine_Learning_for_Lesion_Segmentation_Using_Transformers_in_ICCV_2021_paper.html)]
* **MedT**: "Medical Transformer: Gated Axial-Attention for Medical Image Segmentation", arXiv, 2021 (*Johns Hopkins*). [[Paper](https://arxiv.org/abs/2102.10662)][[PyTorch](https://github.com/jeya-maria-jose/Medical-Transformer)]
* **Convolution-Free**: "Convolution-Free Medical Image Segmentation using Transformers", arXiv, 2021 (*Harvard*). [[Paper](https://arxiv.org/abs/2102.13645)]
* **CoTR**: "CoTr: Efficiently Bridging CNN and Transformer for 3D Medical Image Segmentation", arXiv, 2021 (*Northwestern Polytechnical University*). [[Paper](https://arxiv.org/abs/2103.03024)][[PyTorch](https://github.com/YtongXie/CoTr)]
* **TransBTS**: "TransBTS: Multimodal Brain Tumor Segmentation Using Transformer", arXiv, 2021 (*University of Science and Technology Beijing*). [[Paper](https://arxiv.org/abs/2103.04430)][[PyTorch](https://github.com/Wenxuan-1119/TransBTS)]
* **SpecTr**: "SpecTr: Spectral Transformer for Hyperspectral Pathology Image Segmentation", arXiv, 2021 (*East China Normal University*). [[Paper](https://arxiv.org/abs/2103.03604)][[Code (in construction)](https://github.com/hfut-xc-yun/SpecTr)]
* **U-Transformer**: "U-Net Transformer: Self and Cross Attention for Medical Image Segmentation", arXiv, 2021 (*CEDRIC*). [[Paper](https://arxiv.org/abs/2103.06104)]
* **TransUNet**: "TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation", arXiv, 2021 (*Johns Hopkins*). [[Paper](https://arxiv.org/abs/2102.04306)][[PyTorch](https://github.com/Beckschen/TransUNet)]
* **PMTrans**: "Pyramid Medical Transformer for Medical Image Segmentation", arXiv, 2021 (*Washington University in St. Louis*). [[Paper](https://arxiv.org/abs/2104.14702)]
* **PBT-Net**: "Anatomy-Guided Parallel Bottleneck Transformer Network for Automated Evaluation of Root Canal Therapy", arXiv, 2021 (*Hangzhou Dianzi University*). [[Paper](https://arxiv.org/abs/2105.00381)]
* **Swin-Unet**: "Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation", arXiv, 2021 (*Huawei*). [[Paper](https://arxiv.org/abs/2105.05537)][[Code (in construction)](https://github.com/HuCaoFighting/Swin-Unet)]
* **MBT-Net**: "A Multi-Branch Hybrid Transformer Networkfor Corneal Endothelial Cell Segmentation", arXiv, 2021 (*Southern University of Science and Technology*). [[Paper](https://arxiv.org/abs/2106.07557)]
* **WAD**: "More than Encoder: Introducing Transformer Decoder to Upsample", arXiv, 2021 (*South China University of Technology*). [[Paper](https://arxiv.org/abs/2106.10637)]
* **LeViT-UNet**: "LeViT-UNet: Make Faster Encoders with Transformer for Medical Image Segmentation", arXiv, 2021 (*Wuhan Institute of Technology*). [[Paper](https://arxiv.org/abs/2107.08623)]
* **?**: "Evaluating Transformer based Semantic Segmentation Networks for Pathological Image Segmentation", arXiv, 2021 (*Vanderbilt University*). [[Paper](https://arxiv.org/abs/2108.11993)]
* **nnFormer**: "nnFormer: Interleaved Transformer for Volumetric Segmentation", arXiv, 2021 (*HKU + Xiamen University*). [[Paper](https://arxiv.org/abs/2109.03201)][[PyTorch](https://github.com/282857341/nnFormer)]
* **MISSFormer**: "MISSFormer: An Effective Medical Image Segmentation Transformer", arXiv, 2021 (*Beijing University of Posts and Telecommunications*). [[Paper](https://arxiv.org/abs/2109.07162)]
* **TUnet**: "Transformer-Unet: Raw Image Processing with Unet", arXiv, 2021 (*Beijing Zoezen Robot + Beihang University*). [[Paper](https://arxiv.org/abs/2109.08417)]
* **BiTr-Unet**: "BiTr-Unet: a CNN-Transformer Combined Network for MRI Brain Tumor Segmentation", arXiv, 2021 (*New York University*). [[Paper](https://arxiv.org/abs/2109.12271)]
* **?**: "Transformer Assisted Convolutional Network for Cell Instance Segmentation", arXiv, 2021 (*IIT Dhanbad*). [[Paper](https://arxiv.org/abs/2110.02270)]
* **?**: "Combining CNNs With Transformer for Multimodal 3D MRI Brain Tumor Segmentation With Self-Supervised Pretraining", arXiv, 2021 (*Ukrainian Catholic University*). [[Paper](https://arxiv.org/abs/2110.07919)]
* **UNETR**: "UNETR: Transformers for 3D Medical Image Segmentation", WACV, 2022 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2103.10504)][[PyTorch](https://github.com/Project-MONAI/research-contributions/tree/master/UNETR/BTCV)]
* **AFTer-UNet**: "AFTer-UNet: Axial Fusion Transformer UNet for Medical Image Segmentation", WACV, 2022 (*UC Irvine*). [[Paper](https://openaccess.thecvf.com/content/WACV2022/html/Yan_AFTer-UNet_Axial_Fusion_Transformer_UNet_for_Medical_Image_Segmentation_WACV_2022_paper.html)]
* **UCTransNet**: "UCTransNet: Rethinking the Skip Connections in U-Net from a Channel-wise Perspective with Transformer", AAAI, 2022 (*Northeastern University, China*). [[Paper](https://arxiv.org/abs/2109.04335)][[PyTorch](https://github.com/McGregorWwww/UCTransNet)]
* **Swin-UNETR**: "Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis", CVPR, 2022 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2111.14791)][[PyTorch](https://github.com/Project-MONAI/research-contributions/tree/main/SwinUNETR)]
* **?**: "Transformer-based out-of-distribution detection for clinically safe segmentation", Medical Imaging with Deep Learning (MIDL), 2022 (*King’s College London*). [[Paper](https://arxiv.org/abs/2205.10650)]
* **ScaleFormer**: "ScaleFormer: Revisiting the Transformer-based Backbones from a Scale-wise Perspective for Medical Image Segmentation", IJCAI, 2022 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2207.14552)][[Code (in construction)](https://github.com/ZJUGiveLab/ScaleFormer)]
* **FCBFormer**: "FCN-Transformer Feature Fusion for Polyp Segmentation", Annual Conference on Medical Image Understanding and Analysis (MIUA), 2022 (*University of Central Lancashire, UK*). [[Paper](https://arxiv.org/abs/2208.08352)][[PyTorch](https://github.com/ESandML/FCBFormer)]
* **UAMT-ViT**: "An uncertainty-aware transformer for MRI cardiac semantic segmentation via mean teachers", Medical Image Understanding and Analysis (MIUA), 2022 (*Oxford*). [[Paper](https://link.springer.com/chapter/10.1007/978-3-031-12053-4_37)][[PyTorch](https://github.com/ziyangwang007/CV-SSL-MIS)]
* **VDFormer**: "View-Disentangled Transformer for Brain Lesion Detection", ISBI, 2022 (*CUHK*). [[Paper](https://arxiv.org/abs/2209.09657)][[PyTorch](https://github.com/lhaof/ISBI-VDFormer)]
* **TFCNs**: "TFCNs: A CNN-Transformer Hybrid Network for Medical Image Segmentation", International Conference on Artificial Neural Networks (ICANN), 2022 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2207.03450)][[PyTorch (in construction)](https://github.com/HUANGLIZI/TFCNs)]
* **MIL**: "Transformer based multiple instance learning for weakly supervised histopathology image segmentation", MICCAI, 2022 (*Beihang University*). [[Paper](https://arxiv.org/abs/2205.08878)]
* **mmFormer**: "mmFormer: Multimodal Medical Transformer for Incomplete Multimodal Learning of Brain Tumor Segmentation", MICCAI, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2206.02425)][[PyTorch](https://github.com/YaoZhang93/mmFormer)]
* **Patcher**: "Patcher: Patch Transformers with Mixture of Experts for Precise Medical Image Segmentation", MICCAI, 2022 (*Pennsylvania State University*). [[Paper](https://arxiv.org/abs/2206.01741)]
* **NestedFormer**: "NestedFormer: Nested Modality-Aware Transformer for Brain Tumor Segmentation", MICCAI, 2022 (*Tianjin University*). [[Paper](https://arxiv.org/abs/2208.14876)][[Code (in construction)](https://github.com/920232796/NestedFormer)]
* **TransDeepLab**: "TransDeepLab: Convolution-Free Transformer-based DeepLab v3+ for Medical Image Segmentation", MICCAIW, 2022 (*RWTH Aachen University, Germany*). [[Paper](https://arxiv.org/abs/2208.00713)][[PyTorch](https://github.com/rezazad68/transdeeplab)]
* **CESSViT**: "Computationally-Efficient Vision Transformer for Medical Image Semantic Segmentation via Dual Pseudo-Label Supervision", ICIP, 2022 (*Oxford*). [[Paper](https://ieeexplore.ieee.org/abstract/document/9897482)][[PyTorch](https://github.com/ziyangwang007/CV-SSL-MIS)]
* **S4CVNet**: "When CNN Meet with ViT: Towards Semi-Supervised Learning for Multi-Class Medical Image Semantic Segmentation", ECCVW, 2022 (*Oxford*). [[Paper](https://arxiv.org/abs/2208.06449)][[PyTorch](https://github.com/ziyangwang007/CV-SSL-MIS)]
* **Video-TransUNet**: "Video-TransUNet: Temporally Blended Vision Transformer for CT VFSS Instance Segmentation", International Conference on Machine Vision (ICMV), 2022 (*University of Bristol, UK*). [[Paper](https://arxiv.org/abs/2208.08315)]
* **TransResNet**: "TransResNet: Integrating the Strengths of ViTs and CNNs for High Resolution Medical Image Segmentation via Feature Grafting", BMVC, 2022 (*MBZUAI*). [[Paper](https://bmvc2022.mpi-inf.mpg.de/293/)]
* **CAAViT**: "Adversarial Vision Transformer for Medical Image Semantic Segmentation with Limited Annotations", BMVC, 2022 (*Oxford*). [[Paper](https://bmvc2022.mpi-inf.mpg.de/1002.pdf)][[PyTorch](https://github.com/ziyangwang007/CV-SSL-MIS)][[Supp](https://bmvc2022.mpi-inf.mpg.de/1002_supp.pdf)]
* **CASTformer**: "Class-Aware Adversarial Transformers for Medical Image Segmentation", NeurIPS, 2022 (*Yale*). [[Paper](https://arxiv.org/abs/2201.10737)]
* **TransNorm**: "TransNorm: Transformer Provides a Strong Spatial Normalization Mechanism for a Deep Segmentation Model", IEEE Access, 2022 (*Aachen University, Germany*). [[Paper](https://arxiv.org/abs/2207.13415)][[PyTorch](https://github.com/rezazad68/transnorm)]
* **Tempera**: "Tempera: Spatial Transformer Feature Pyramid Network for Cardiac MRI Segmentation", arXiv, 2022 (*ICL*). [[Paper](https://arxiv.org/abs/2203.00355)]
* **UTNetV2**: "A Multi-scale Transformer for Medical Image Segmentation: Architectures, Model Efficiency, and Benchmarks", arXiv, 2022 (*Rutgers*). [[Paper](https://arxiv.org/abs/2203.00131)]
* **UNesT**: "Characterizing Renal Structures with 3D Block Aggregate Transformers", arXiv, 2022 (*Vanderbilt University, Tennessee*). [[Paper](https://arxiv.org/abs/2203.02430)]
* **PHTrans**: "PHTrans: Parallelly Aggregating Global and Local Representations for Medical Image Segmentation", arXiv, 2022 (*Beijing University of Posts and Telecommunications*). [[Paper](https://arxiv.org/abs/2203.04568)]
* **UNeXt**: "UNeXt: MLP-based Rapid Medical Image Segmentation Network", arXiv, 2022 (*JHU*). [[Paper](https://arxiv.org/abs/2203.04967)][[PyTorch](https://github.com/jeya-maria-jose/UNeXt-pytorch)]
* **TransFusion**: "TransFusion: Multi-view Divergent Fusion for Medical Image Segmentation with Transformers", arXiv, 2022 (*Rutgers*). [[Paper](https://arxiv.org/abs/2203.10726)]
* **UNetFormer**: "UNetFormer: A Unified Vision Transformer Model and Pre-Training Framework for 3D Medical Image Segmentation", arXiv, 2022 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2204.00631)][[GitHub](https://github.com/Project-MONAI/research-contributions)]
* **3D-Shuffle-Mixer**: "3D Shuffle-Mixer: An Efficient Context-Aware Vision Learner of Transformer-MLP Paradigm for Dense Prediction in Medical Volume", arXiv, 2022 (*Xi'an Jiaotong University*). [[Paper](https://arxiv.org/abs/2204.06779)]
* **?**: "Continual Hippocampus Segmentation with Transformers", arXiv, 2022 (*Technical University of Darmstadt, Germany*). [[Paper](https://arxiv.org/abs/2204.08043)]
* **TranSiam**: "TranSiam: Fusing Multimodal Visual Features Using Transformer for Medical Image Segmentation", arXiv, 2022 (*Tianjin University*). [[Paper](https://arxiv.org/abs/2204.12185)]
* **ColonFormer**: "ColonFormer: An Efficient Transformer based Method for Colon Polyp Segmentation", arXiv, 2022 (*Hanoi University of Science and Technology*). [[Paper](https://arxiv.org/abs/2205.08473)]
* **?**: "Transformer based Generative Adversarial Network for Liver Segmentation", arXiv, 2022 (*Northwestern University*). [[Paper](https://arxiv.org/abs/2205.10663)]
* **FCT**: "The Fully Convolutional Transformer for Medical Image Segmentation", arXiv, 2022 (*University of Glasgow, UK*). [[Paper](https://arxiv.org/abs/2206.00566)]
* **XBound-Former**: "XBound-Former: Toward Cross-scale Boundary Modeling in Transformers", arXiv, 2022 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2206.00806)][[PyTorch](https://github.com/jcwang123/xboundformer)]
* **Polyp-PVT**: "Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers", arXiv, 2022 (*IIAI*). [[Paper](https://arxiv.org/abs/2108.06932)][[PyTorch](https://github.com/DengPingFan/Polyp-PVT)]
* **SeATrans**: "SeATrans: Learning Segmentation-Assisted diagnosis model via Transformer", arXiv, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2206.05763)]
* **TransResU-Net**: "TransResU-Net: Transformer based ResU-Net for Real-Time Colonoscopy Polyp Segmentation", arXiv, 2022 (*Indira Gandhi National Open University*). [[Paper](https://arxiv.org/abs/2206.08985)][[Code (in construction)](https://github.com/nikhilroxtomar/TransResUNet)]
* **LViT**: "LViT: Language meets Vision Transformer in Medical Image Segmentation", arXiv, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2206.14718)][[Code (in construction)](https://github.com/HUANGLIZI/LViT)]
* **APFormer**: "The Lighter The Better: Rethinking Transformers in Medical Image Segmentation Through Adaptive Pruning", arXiv, 2022 (*Huazhong University of Science and Technology*). [[Paper](https://arxiv.org/abs/2206.14413)][[PyTorch](https://github.com/xianlin7/APFormer)]
* **?**: "Transformer based Models for Unsupervised Anomaly Segmentation in Brain MR Images", arXiv, 2022 (*University of Rennes, France*). [[Paper](https://arxiv.org/abs/2207.02059)][[Tensorflow](https://github.com/ahmedgh970/Transformers_Unsupervised_Anomaly_Segmentation)]
* **CKD-TransBTS**: "CKD-TransBTS: Clinical Knowledge-Driven Hybrid Transformer with Modality-Correlated Cross-Attention for Brain Tumor Segmentation", arXiv, 2022 (*South China University of Technology*). [[Paper](https://arxiv.org/abs/2207.07370)]
* **?**: "Contextual Attention Network: Transformer Meets U-Net", arXiv, 2022 (*RWTH Aachen University*). [[Paper](https://arxiv.org/abs/2203.01932)][[PyTorch](https://github.com/rezazad68/TMUnet)]
* **HRSTNet**: "High-Resolution Swin Transformer for Automatic Medical Image Segmentation", arXiv, 2022 (*Xi'an University of Posts and Telecommunications*). [[Paper](https://arxiv.org/abs/2207.11553)][[Code (in construction)](https://github.com/auroua/HRSTNet)]
* **CM-MLP**: "CM-MLP: Cascade Multi-scale MLP with Axial Context Relation Encoder for Edge Segmentation of Medical Image", arXiv, 2022 (*Zhengzhou University*). [[Paper](https://arxiv.org/abs/2208.10701)]
* **CATS**: "Cats: Complementary CNN and Transformer Encoders for Segmentation", arXiv, 2022 (*Vanderbilt University, Nashville*). [[Paper](https://arxiv.org/abs/2208.11572)]
* **TFusion**: "TFusion: Transformer based N-to-One Multimodal Fusion Block", arXiv, 2022 (*SouthChinaUniversityofTechnology*). [[Paper](https://arxiv.org/abs/2208.12776)]
* **AutoPET**: "AutoPET Challenge: Combining nn-Unet with Swin UNETR Augmented by Maximum Intensity Projection Classifier", arXiv, 2022 (*University Hospital Essen, Germany*). [[Paper](https://arxiv.org/abs/2209.01112)]
* **SPAN**: "Prior Knowledge-Guided Attention in Self-Supervised Vision Transformers", arXiv, 2022 (*Berkeley*). [[Paper](https://arxiv.org/abs/2209.03745)]
* **TMSS**: "TMSS: An End-to-End Transformer-based Multimodal Network for Segmentation and Survival Prediction", arXiv, 2022 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2209.05036)]
* **CR-Swin2-VT**: "Hybrid Window Attention Based Transformer Architecture for Brain Tumor Segmentation", arXiv, 2022 (*Monash University*). [[Paper](https://arxiv.org/abs/2209.07704)][[PyTorch](https://github.com/himashi92/vizviva_fets_2022)]
* **FocalUNETR**: "FocalUNETR: A Focal Transformer for Boundary-aware Segmentation of CT Images", arXiv, 2022 (*Wayne State University, Detroit*). [[Paper](https://arxiv.org/abs/2210.03189)]
* **LAPFormer**: "LAPFormer: A Light and Accurate Polyp Segmentation Transformer", arXiv, 2022 (*Sun\* Inc, Hanoi*). [[Paper](https://arxiv.org/abs/2210.04393)]
* **FINE**: "Memory transformers for full context and high-resolution 3D Medical Segmentation", arXiv, 2022 (*National Conservatory of Arts and Crafts, France*). [[Paper](https://arxiv.org/abs/2210.05313)]
* **ConvTransSeg**: "ConvTransSeg: A Multi-resolution Convolution-Transformer Network for Medical Image Segmentation", arXiv, 2022 (*University of Nottingham, UK*). [[Paper](https://arxiv.org/abs/2210.07072)]
* **CS-Unet**: "Optimizing Vision Transformers for Medical Image Segmentation and Few-Shot Domain Adaptation", arXiv, 2022 (*University of Glasgow, UK*). [[Paper](https://arxiv.org/abs/2210.08066)]
* **UNETR++**: "UNETR++: Delving into Efficient and Accurate 3D Medical Image Segmentation", arXiv, 2022 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2212.04497)][[PyTorch](https://github.com/Amshaker/unetr_plus_plus)]
* **HiFormer**: "HiFormer: Hierarchical Multi-scale Representations Using Transformers for Medical Image Segmentation", WACV, 2023 (*Iran University of Science and Technology*). [[Paper](https://arxiv.org/abs/2207.08518)][[PyTorch](https://github.com/amirhossein-kz/HiFormer)]
* **Att-SwinU-Net**: "Attention Swin U-Net: Cross-Contextual Attention Mechanism for Skin Lesion Segmentation", IEEE ISBI, 2023 (*Shahid Beheshti University, Iran*). [[Paper](https://arxiv.org/abs/2210.16898)][[PyTorch](https://github.com/NITR098/AttSwinUNet)]
* **3DUX-Net**: "3D UX-Net: A Large Kernel Volumetric ConvNet Modernizing Hierarchical Transformer for Medical Image Segmentation", ICLR, 2023 (*Vanderbilt University*). [[Paper](https://arxiv.org/abs/2209.15076)][[PyTorch](https://github.com/MASILab/3DUX-Net)]
* **?**: "Devil is in the Queries: Advancing Mask Transformers for Real-world Medical Image Segmentation and Out-of-Distribution Localization", CVPR, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2304.00212)]
* **CVM**: "Weakly supervised segmentation with point annotations for histopathology images via contrast-based variational model", CVPR, 2023 (*University of Liverpool, UK*). [[Paper](https://arxiv.org/abs/2304.03572)]
* **MAESTER**: "MAESTER: Masked Autoencoder Guided Segmentation at Pixel Resolution for Accurate, Self-Supervised Subcellular Structure Recognition", CVPR, 2023 (*University of Toronto*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Xie_MAESTER_Masked_Autoencoder_Guided_Segmentation_at_Pixel_Resolution_for_Accurate_CVPR_2023_paper.html)]
* **Universal-Model**: "CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection", ICCV, 2023 (*JHU*). [[Paper](https://arxiv.org/abs/2301.00785)][[PyTorch](https://github.com/ljwztc/CLIP-Driven-Universal-Model)]
* **MDViT**: "MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets", MICCAI, 2023 (*UBC*). [[Paper](https://arxiv.org/abs/2307.02100)][[PyTorch](https://github.com/siyi-wind/MDViT)]
* **ConvFormer**: "ConvFormer: Plug-and-Play CNN-Style Transformers for Improving Medical Image Segmentation", MICCAI, 2023 (*Huazhong University of Science and Technology*). [[Paper](https://arxiv.org/abs/2309.05674)][[PyTorch](https://github.com/xianlin7/ConvFormer)]
* **TP-SIS**: "Text Promptable Surgical Instrument Segmentation with Vision-Language Models", NeurIPS, 2023 (*King's College London*). [[Paper](https://arxiv.org/abs/2306.09244)][[PyTorch](https://github.com/franciszzj/TP-SIS)]
* **UniSeg**: "UniSeg: A Prompt-driven Universal Segmentation Model as well as A Strong Representation Learner", arXiv, 2023 (*Northwestern Polytechnical University, China*). [[Paper](https://arxiv.org/abs/2304.03493)][[PyTorch (in construction)](https://github.com/yeerwen/UniSeg)]
* **UniverSeg**: "UniverSeg: Universal Medical Image Segmentation", arXiv, 2023 (*MIT*). [[Paper](https://arxiv.org/abs/2304.06131)][[PyTorch](https://github.com/JJGO/UniverSeg)][[Website](https://universeg.csail.mit.edu/)]
* **3DSAM-adapter**: "3DSAM-adapter: Holistic Adaptation of SAM from 2D to 3D for Promptable Medical Image Segmentation", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2306.13465)]
* **CMCL**: "Disruptive Autoencoders: Leveraging Low-level features for 3D Medical Image Pre-training", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2307.16896)]
* **AdaptiveSAM**: "AdaptiveSAM: Towards Efficient Tuning of SAM for Surgical Scene Segmentation", arXiv, 2023 (*JHU*). [[Paper](https://arxiv.org/abs/2308.03726)][[PyTorch](https://github.com/JayParanjape/biastuning)]
* **SAM-Med2D**: "SAM-Med2D", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2308.16184)][[Pytorch](https://github.com/OpenGVLab/SAM-Med2D)]
* **SAM-Med3D**: "SAM-Med3D", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2310.15161)][[PyTorch](https://github.com/uni-medical/SAM-Med3D)]

[[Back to Overview](#overview)]

### Medical Classification
* **COVID19T**: "A Transformer-Based Framework for Automatic COVID19 Diagnosis in Chest CTs", ICCVW, 2021 (*?*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021W/MIA-COV19D/html/Zhang_A_Transformer-Based_Framework_for_Automatic_COVID19_Diagnosis_in_Chest_CTs_ICCVW_2021_paper.html)][[PyTorch](https://github.com/leizhangtech/COVID19T)]
* **TransMIL**: "TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classication", NeurIPS, 2021 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2106.00908)][[PyTorch](https://github.com/szc19990412/TransMIL)]
* **TransMed**: "TransMed: Transformers Advance Multi-modal Medical Image Classification", arXiv, 2021 (*Northeastern University*). [[Paper](https://arxiv.org/abs/2103.05940)]
* **CXR-ViT**: "Vision Transformer using Low-level Chest X-ray Feature Corpus for COVID-19 Diagnosis and Severity Quantification", arXiv, 2021 (*KAIST*). [[Paper](https://arxiv.org/abs/2104.07235)]
* **ViT-TSA**: "Shoulder Implant X-Ray Manufacturer Classification: Exploring with Vision Transformer", arXiv, 2021 (*Queen’s University*). [[Paper](https://arxiv.org/abs/2104.07667)]
* **GasHis-Transformer**: "GasHis-Transformer: A Multi-scale Visual Transformer Approach for Gastric Histopathology Image Classification", arXiv, 2021 (*Northeastern University*). [[Paper](https://arxiv.org/abs/2104.14528)]
* **POCFormer**: "POCFormer: A Lightweight Transformer Architecture for Detection of COVID-19 Using Point of Care Ultrasound", arXiv, 2021 (*The Ohio State University*). [[Paper](https://arxiv.org/abs/2105.09913)]
* **COVID-ViT**: "COVID-VIT: Classification of COVID-19 from CT chest images based on vision transformer models", arXiv, 2021 (*Middlesex University, UK*). [[Paper](https://arxiv.org/abs/2107.01682)][[PyTorch](https://github.com/xiaohong1/COVID-ViT)]
* **EEG-ConvTransformer**: "EEG-ConvTransformer for Single-Trial EEG based Visual Stimuli Classification", arXiv, 2021 (*IIT Ropar*). [[Paper](https://arxiv.org/abs/2107.03983)]
* **CCAT**: "Visual Transformer with Statistical Test for COVID-19 Classification", arXiv, 2021 (*NCKU*). [[Paper](https://arxiv.org/abs/2107.05334)]
* **M3T**: "M3T: Three-Dimensional Medical Image Classifier Using Multi-Plane and Multi-Slice Transformer", CVPR, 2022 (*Yonsei University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Jang_M3T_Three-Dimensional_Medical_Image_Classifier_Using_Multi-Plane_and_Multi-Slice_Transformer_CVPR_2022_paper.html)]
* **?**: "A comparative study between vision transformers and CNNs in digital pathology", CVPRW, 2022 (*Roche, Switzerland*). [[Paper](https://arxiv.org/abs/2206.00389)]
* **SCT**: "Context-Aware Transformers For Spinal Cancer Detection and Radiological Grading", MICCAI, 2022 (*Oxford*). [[Paper](https://arxiv.org/abs/2206.13173)]
* **KAT**: "Kernel Attention Transformer (KAT) for Histopathology Whole Slide Image Classification", MICCAI, 2022 (*Beihang University*). [[Paper](https://arxiv.org/abs/2206.13156)][[PyTorch](https://github.com/zhengyushan/kat)]
* **SEViT**: "Self-Ensembling Vision Transformer (SEViT) for Robust Medical Image Classification", MICCAI, 2022 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2208.02851)][[PyTorch](https://github.com/faresmalik/SEViT)]
* **MF-ViT**: "Multi-Feature Vision Transformer via Self-Supervised Representation Learning for Improvement of COVID-19 Diagnosis", MICCAIW, 2022 (*Rutgers University*). [[Paper](https://arxiv.org/abs/2208.01843)][[PyTorch](https://github.com/endiqq/Multi-Feature-ViT)]
* **SB-SSL**: "SB-SSL: Slice-Based Self-Supervised Transformers for Knee Abnormality Classification from MRI", MICCAIW, 2022 (*University of Surrey, UK*). [[Paper](https://arxiv.org/abs/2208.13923)]
* **RadioTransformer**: "RadioTransformer: A Cascaded Global-Focal Transformer for Visual Attention-guided Disease Classification", ECCV, 2022 (*Stony Brook*). [[Paper](https://arxiv.org/abs/2202.11781)][[Tensorflow (in construction)](https://github.com/bmi-imaginelab/radiotransformer)]
* **ScoreNet**: "ScoreNet: Learning Non-Uniform Attention and Augmentation for Transformer-Based Histopathological Image Classification", arXiv, 2022 (*EPFL*). [[Paper](https://arxiv.org/abs/2202.07570)]
* **LA-MIL**: "Local Attention Graph-based Transformer for Multi-target Genetic Alteration Prediction", arXiv, 2022 (*TUM*). [[Paper](https://arxiv.org/abs/2205.06672)]
* **HoVer-Trans**: "HoVer-Trans: Anatomy-aware HoVer-Transformer for ROI-free Breast Cancer Diagnosis in Ultrasound Images", arXiv, 2022 (*South China University of Technology*). [[Paper](https://arxiv.org/abs/2205.08390)]
* **GTP**: "A graph-transformer for whole slide image classification", IEEE Transactions on Medical Imaging (TMI), 2022 (*Boston University*). [[Paper](https://arxiv.org/abs/2205.09671)][[PyTorch](https://github.com/vkola-lab/tmi2022)]
* **?**: "Zero-Shot and Few-Shot Learning for Lung Cancer Multi-Label Classification using Vision Transformer", arXiv, 2022 (*Harvard*). [[Paper](https://arxiv.org/abs/2205.15290)]
* **SwinCheX**: "SwinCheX: Multi-label classification on chest X-ray images with transformers", arXiv, 2022 (*Sharif University of Technology, Iran*). [[Paper](https://arxiv.org/abs/2206.04246)]
* **SGT**: "Rectify ViT Shortcut Learning by Visual Saliency", arXiv, 2022 (*Northwestern Polytechnical University, China*). [[Paper](https://arxiv.org/abs/2206.08567)]
* **IPMN-ViT**: "Neural Transformers for Intraductal Papillary Mucosal Neoplasms (IPMN) Classification in MRI images", arXiv, 2022 (*University of Catania, Italy*). [[Paper](https://arxiv.org/abs/2206.10531)]
* **?**: "Multi-Label Retinal Disease Classification using Transformers", arXiv, 2022 (*Khalifa University, UAE*). [[Paper](https://arxiv.org/abs/2207.02335)][[PyTorch](https://github.com/manuel-rdz/C-Tran)]
* **TractoFormer**: "TractoFormer: A Novel Fiber-level Whole Brain Tractography Analysis Framework Using Spectral Embedding and Vision Transformers", arXiv, 2022 (*Harvard*). [[Paper](https://arxiv.org/abs/2207.02327)]
* **BrainFormer**: "BrainFormer: A Hybrid CNN-Transformer Model for Brain fMRI Data Classification", arXiv, 2022 (*Chinese PLA General Hospital*). [[Paper](https://arxiv.org/abs/2208.03028)]
* **SI-ViT**: "Shuffle Instances-based Vision Transformer for Pancreatic Cancer ROSE Image Classification", arXiv, 2022 (*Beihang University*). [[Paper](https://arxiv.org/abs/2208.06833)][[PyTorch](https://github.com/sagizty/MIL-SI)]
* **IPS**: "Iterative Patch Selection for High-Resolution Image Recognition", ICLR, 2023 (*Hasso Plattner Institute, Germany*). [[Paper](https://arxiv.org/abs/2210.13007)]
* **ILRA-MIL**: "Exploring Low-Rank Property in Multiple Instance Learning for Whole Slide Image Classification", ICLR, 2023 (*Tencent*). [[Paper](https://openreview.net/forum?id=01KmhBsEPFO)]
* **BolT**: "BolT: Fused window transformers for fMRI time series analysis", Medical Image Analysis, 2023 (*Bilkent University*). [[Paper](https://www.sciencedirect.com/science/article/pii/S1361841523001019)][[PyTorch](https://github.com/icon-lab/BolT)]
* **TOP**: "The Rise of AI Language Pathologists: Exploring Two-level Prompt Learning for Few-shot Weakly-supervised Whole Slide Image Classification", NeurIPS, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2305.17891)][[Code (in construction)](https://github.com/miccaiif/TOP)]
* **DreaMR**: "DreaMR: Diffusion-driven Counterfactual Explanation for Functional MRI", arXiv, 2023 (*Bilkent University*). [[Paper](https://arxiv.org/abs/2307.09547)][[PyTorch](https://github.com/icon-lab/DreaMR)]
* **LongViT**: "When an Image is Worth 1,024 x 1,024 Words: A Case Study in Computational Pathology", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2312.03558)][[PyTorch](https://github.com/microsoft/torchscale)]

[[Back to Overview](#overview)]

### Medical Detection
* **COTR**: "COTR: Convolution in Transformer Network for End to End Polyp Detection", arXiv, 2021 (*Fuzhou University*). [[Paper](https://arxiv.org/abs/2105.10925)]
* **TR-Net**: "Transformer Network for Significant Stenosis Detection in CCTA of Coronary Arteries", arXiv, 2021 (*Harbin Institute of Technology*). [[Paper](https://arxiv.org/abs/2107.03035)]
* **CAE-Transformer**: "CAE-Transformer: Transformer-based Model to Predict Invasiveness of Lung Adenocarcinoma Subsolid Nodules from Non-thin Section 3D CT Scans", arXiv, 2021 (*Concordia University, Canada*). [[Paper](https://arxiv.org/abs/2110.08721)]
* **SwinFPN**: "SwinFPN: Leveraging Vision Transformers for 3D Organs-At-Risk Detection", MIDL, 2022 (*TUM*). [[Paper](https://openreview.net/forum?id=yiIz7DhgRU5)][[PyTorch](https://github.com/bwittmann/transoar)]
* **DATR**: "DATR: Domain-adaptive transformer for multi-domain landmark detection", arXiv, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2203.06433)]
* **SATr**: "SATr: Slice Attention with Transformer for Universal Lesion Detection", arXiv, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2203.07373)]
* **AC-Former**: "Affine-Consistent Transformer for Multi-Class Cell Nuclei Detection", ICCV, 2023 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2310.14154)][[PyTorch](https://github.com/lhaof/ACFormer)]
* **PGT**: "Prompt-based Grouping Transformer for Nucleus Detection and Classification", MICCAI, 2023 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2310.14176)][[PyTorch](https://github.com/lhaof/PGT)]
* **Focused-Decoder**: "Focused Decoding Enables 3D Anatomical Detection by Transformers", MELBA, 2023 (*University of Zurich*). [[Paper](https://arxiv.org/abs/2207.10774)][[PyTorch](https://github.com/bwittmann/transoar)][[Website](https://www.melba-journal.org/papers/2023:003.html)]

[[Back to Overview](#overview)]

### Medical Reconstruction
* **T<sup>2</sup>Net**: "Task Transformer Network for Joint MRI Reconstruction and Super-Resolution", MICCAI, 2021 (*Harbin Institute of Technology*). [[Paper](https://arxiv.org/abs/2106.06742)][[PyTorch](https://github.com/chunmeifeng/T2Net)]
* **FIT**: "Fourier Image Transformer", arXiv, 2021 (*MPI*). [[Paper](https://arxiv.org/abs/2104.02555)][[PyTorch](https://github.com/juglab/FourierImageTransformer)]
* **SLATER**: "Unsupervised MRI Reconstruction via Zero-Shot Learned Adversarial Transformers", arXiv, 2021 (*Bilkent University*). [[Paper](https://arxiv.org/abs/2105.08059)]
* **MTrans**: "MTrans: Multi-Modal Transformer for Accelerated MR Imaging", arXiv, 2021 (*Harbin Institute of Technology*). [[Paper](https://arxiv.org/abs/2106.14248)][[PyTorch](https://github.com/chunmeifeng/MTrans)]
* **SDAUT**: "Swin Deformable Attention U-Net Transformer (SDAUT) for Explainable Fast MRI", MICCAI, 2022 (*ICL*). [[Paper](https://arxiv.org/abs/2207.02390)]
* **?**: "Adaptively Re-weighting Multi-Loss Untrained Transformer for Sparse-View Cone-Beam CT Reconstruction", arXiv, 2022 (*Zhejiang Lab*). [[Paper](https://arxiv.org/abs/2203.12476)]
* **K-Space-Transformer**: "K-Space Transformer for Fast MRI Reconstruction with Implicit Representation", arXiv, 2022 (*Shanghai Jiao Tong University*). [[Paper](https://arxiv.org/abs/2206.06947)][[Code (in construction)](https://github.com/zhaoziheng/K-Space-Transformer)][[Website](https://zhaoziheng.github.io/Website/K-Space-Transformer/)]
* **McSTRA**: "Multi-head Cascaded Swin Transformers with Attention to k-space Sampling Pattern for Accelerated MRI Reconstruction", arXiv, 2022 (*Monash University, Australia*). [[Paper](https://arxiv.org/abs/2207.08412)]
* **?**: "Colonoscopy Landmark Detection using Vision Transformers", arXiv, 2022 (*Intuitive Surgical, CA*). [[Paper](https://arxiv.org/abs/2209.11304)]
* **FedPR**: "Learning Federated Visual Prompt in Null Space for MRI Reconstruction", CVPR, 2023 (*A\*STAR*). [[Paper](https://arxiv.org/abs/2303.16181)][[PyTorch](https://github.com/chunmeifeng/FedPR)]
* **?**: "Contrast, Attend and Diffuse to Decode High-Resolution Images from Brain Activities", NeurIPS, 2023 (*KU Leuven*). [[Paper](https://arxiv.org/abs/2305.17214)][[PyTorch](https://github.com/soinx0629/vis_dec_neurips/)]
* **?**: "Brain encoding models based on multimodal transformers can transfer across language and vision", NeurIPS, 2023 (*UT Austin*). [[Paper](https://arxiv.org/abs/2305.12248)]
* **MinD-Video**: "Cinematic Mindscapes: High-quality Video Reconstruction from Brain Activity", NeurIPS, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2305.11675)][[PyTorch](https://github.com/jqin4749/MindVideo)][[Website](https://www.mind-video.com/)]

[[Back to Overview](#overview)]

### Medical Low-Level Vision
* **Eformer**: "Eformer: Edge Enhancement based Transformer for Medical Image Denoising", ICCV, 2021 (*BITS Pilani, India*). [[Paper](https://arxiv.org/abs/2109.08044)]
* **PTNet**: "PTNet: A High-Resolution Infant MRI Synthesizer Based on Transformer", arXiv, 2021 (* Columbia *). [[Paper](https://arxiv.org/ftp/arxiv/papers/2105/2105.13993.pdf)]
* **ResViT**: "ResViT: Residual vision transformers for multi-modal medical image synthesis", arXiv, 2021 (*Bilkent University, Turkey*). [[Paper](https://arxiv.org/abs/2106.16031)]
* **CyTran**: "CyTran: Cycle-Consistent Transformers for Non-Contrast to Contrast CT Translation", arXiv, 2021 (*University Politehnica of Bucharest, Romania*). [[Paper](https://arxiv.org/abs/2110.06400)][[PyTorch](https://github.com/ristea/cycle-transformer)]
* **McMRSR**: "Transformer-empowered Multi-scale Contextual Matching and Aggregation for Multi-contrast MRI Super-resolution", CVPR, 2022 (*Yantai University, China*). [[Paper](https://arxiv.org/abs/2203.13963)][[PyTorch](https://github.com/XAIMI-Lab/McMRSR)]
* **RPLHR-CT**: "RPLHR-CT Dataset and Transformer Baseline for Volumetric Super-Resolution from CT Scans", MICCAI, 2022 (*Infervision Medical Technology, China*). [[Paper](https://arxiv.org/abs/2206.06253)][[Code (in construction)](https://arxiv.org/abs/2206.06253)]
* **W-G2L-ART**: "Wide Range MRI Artifact Removal with Transformers", BMVC, 2022 (*KTH*). [[Paper](https://arxiv.org/abs/2210.07976)]
* **RFormer**: "RFormer: Transformer-based Generative Adversarial Network for Real Fundus Image Restoration on A New Clinical Benchmark", arXiv, 2022 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2201.00466)]
* **CTformer**: "CTformer: Convolution-free Token2Token Dilated Vision Transformer for Low-dose CT Denoising", arXiv, 2022 (*UMass Lowell*). [[Paper](https://arxiv.org/abs/2202.13517)][[PyTorch](https://github.com/wdayang/CTformer)]
* **Cohf-T**: "Cross-Modality High-Frequency Transformer for MR Image Super-Resolution", arXiv, 2022 (*Xidian University*). [[Paper](https://arxiv.org/abs/2203.15314)]
* **SIST**: "Low-Dose CT Denoising via Sinogram Inner-Structure Transformer", arXiv, 2022 (*?*). [[Paper](https://arxiv.org/abs/2204.03163)]
* **Spach-Transformer**: "Spach Transformer: Spatial and Channel-wise Transformer Based on Local and Global Self-attentions for PET Image Denoising", arXiv, 2022 (*Harvard*). [[Paper](https://arxiv.org/abs/2209.03300)]
* **ConvFormer**: "ConvFormer: Combining CNN and Transformer for Medical Image Segmentation", arXiv, 2022 (*University of Notre Dame*). [[Paper](https://arxiv.org/abs/2211.08564)]
* **?**: "Unaligned 2D to 3D Translation with Conditional Vector-Quantized Code Diffusion using Transformers", ICCV, 2023 (*Durham University, UK*). [[Paper](https://arxiv.org/abs/2308.14152)]

[[Back to Overview](#overview)]

### Medical Vision-Language
* **CGT**: "Cross-modal Clinical Graph Transformer for Ophthalmic Report Generation", CVPR, 2022 (*University of Technology Sydney*). [[Paper](https://arxiv.org/abs/2206.01988)]
* **MCGN**: "A Medical Semantic-Assisted Transformer for Radiographic Report Generation", MICCAI, 2022 (*University of Sydney*). [[Paper](https://arxiv.org/abs/2208.10358)]
* **M3AE**: "Multi-Modal Masked Autoencoders for Medical Vision-and-Language Pre-Training", MICCAI, 2022 (*CUHK*). [[Paper](https://arxiv.org/abs/2209.07098)][[PyTorch](https://github.com/zhjohnchan/M3AE)]
* **BioViL**: "Making the Most of Text Semantics to Improve Biomedical Vision-Language Processing", ECCV, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2204.09817)][[Code](https://hi-ml.readthedocs.io/en/latest/multimodal.html)]
* **MGCA**: "Multi-Granularity Cross-modal Alignment for Generalized Medical Visual Representation Learning", NeurIPS, 2022 (*HKU*). [[Paper](https://arxiv.org/abs/2210.06044)]
* **MedCLIP**: "MedCLIP: Contrastive Learning from Unpaired Medical Images and Text", EMNLP, 2022 (*UIUC*). [[Paper](https://arxiv.org/abs/2210.10163)][[PyTorch](https://github.com/RyanWangZf/MedCLIP)]
* **MDBERT**: "Hierarchical BERT for Medical Document Understanding", arXiv, 2022 (*IQVIA, NC*). [[Paper](https://arxiv.org/abs/2204.09600)]
* **Surgical-VQA**: "Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer", arXiv, 2022 (*NUS*). [[Paper](https://arxiv.org/abs/2206.11053)][[PyTorch (in construction)](https://github.com/lalithjets/Surgical_VQA)]
* **SwinMLP-TranCAP**: "Rethinking Surgical Captioning: End-to-End Window-Based MLP Transformer Using Patches", arXiv, 2022 (*CUHK*). [[Paper](https://arxiv.org/abs/2207.00113)][[PyTorch](https://github.com/XuMengyaAmy/SwinMLP_TranCAP)]
* **SAT**: "Medical Image Captioning via Generative Pretrained Transformers", arXiv, 2022 (*Philips Innovation Labs Rus, Russia*). [[Paper](https://arxiv.org/abs/2209.13983)]
* **RepsNet**: "RepsNet: Combining Vision with Language for Automated Medical Reports", arXiv, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2209.13171)][[Website](https://sites.google.com/view/repsnet)]
* **MF<sup>2</sup>-MVQA**: "MF<sup>2</sup>-MVQA: A Multi-stage Feature Fusion method for Medical Visual Question Answering", arXiv, 2022 (*University of Science and Technology Beijing*). [[Paper](https://arxiv.org/abs/2211.05991)]
* **RoentGen**: "RoentGen: Vision-Language Foundation Model for Chest X-ray Generation", arXiv, 2022 (*Stanford*). [[Paper](https://arxiv.org/abs/2211.12737)]
* **?**: "Medical Image Understanding with Pretrained Vision Language Models: A Comprehensive Study", ICLR, 2023 (*Sichuan University*). [[Paper](https://arxiv.org/abs/2209.15517)]
* **METransformer**: "METransformer: Radiology Report Generation by Transformer with Multiple Learnable Expert Tokens", CVPR, 2023 (*University of Sydney*). [[Paper](https://arxiv.org/abs/2304.02211)]
* **MI-Zero**: "Visual Language Pretrained Multiple Instance Zero-Shot Transfer for Histopathology Images", CVPR, 2023 (*Harvard*). [[Paper](https://arxiv.org/abs/2306.07831)]
* **KiUT**: "KiUT: Knowledge-Injected U-Transformer for Radiology Report Generation", CVPR, 2023 (*Shanghai AI Lab*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Huang_KiUT_Knowledge-Injected_U-Transformer_for_Radiology_Report_Generation_CVPR_2023_paper.html)]
* **BioViL-T**: "Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2301.04558)]
* **?**: "Evidential Interactive Learning for Medical Image Captioning", ICML, 2023 (*Rochester Institute of Technology, NY*). [[Paper](https://openreview.net/forum?id=6wfqx3CdKv)]
* **PRIOR**: "PRIOR: Prototype Representation Joint Learning from Medical Images and Reports", ICCV, 2023 (*Southern University of Science and Technology*). [[Paper](https://arxiv.org/abs/2307.12577)][[Code (in construction)](https://github.com/QtacierP/PRIOR)]
* **MedKLIP**: "MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training in Radiology", ICCV, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2301.02228)][[PyTorch](https://github.com/MediaBrain-SJTU/MedKLIP)][[Website](https://chaoyi-wu.github.io/MedKLIP/)]
* **PTUnifier**: "Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts", ICCV, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2302.08958)][[PyTorch](https://github.com/zhjohnchan/PTUnifier)]
* **?**: "Localized Questions in Medical Visual Question Answering", MICCAI, 2023 (*University of Bern, Switzerland*). [[Paper](https://arxiv.org/abs/2307.01067)]
* **CXR-CLIP**: "CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training", MICCAI, 2023 (*Kakao*). [[Paper](https://arxiv.org/abs/2310.13292)]
* **LLaVA-Med**: "LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day", NeurIPS (Datasets and Benchmarks), 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2306.00890)][[PyTorch](https://github.com/microsoft/LLaVA-Med)]
* **Med-UniC**: "Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias", NeurIPS, 2023 (*OSU*). [[Paper](https://arxiv.org/abs/2305.19894)][[PyTorch](https://github.com/SUSTechBruce/Med-UniC)]
* **EHRXQA**: "EHRXQA: A Multi-Modal Question Answering Dataset for Electronic Health Records with Chest X-ray Images", NeurIPS (Datasets and Benchmarks), 2023 (*KAIST*). [[Paper](https://arxiv.org/abs/2310.18652)][[Code](https://github.com/baeseongsu/ehrxqa)]
* **Quilt**: "Quilt-1M: One Million Image-Text Pairs for Histopathology", NeurIPS, 2023 (*UW*). [[Paper](https://arxiv.org/abs/2306.11207)]
* **RAMM**: "RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2303.00534)]
* **PT**: "Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models", arXiv, 2023 (*University of Amsterdam*). [[Paper](https://arxiv.org/abs/2303.05977)]
* **PMC-CLIP**: "PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents", arXiv, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2303.07240)]
* **Q2ATransformer**: "Q2ATransformer: Improving Medical VQA via an Answer Querying Decoder", arXiv, 2023 (*The University of Sydney*). [[Paper](https://arxiv.org/abs/2304.01611)]
* **PMC-VQA**: "PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering", arXiv, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2305.10415)][[Code (in construction)](https://github.com/xiaoman-zhang/PMC-VQA)][[Website](https://xiaoman-zhang.github.io/PMC-VQA/)]
* **MedBLIP**: "MedBLIP: Bootstrapping Language-Image Pre-training from 3D Medical Images and Texts", arXiv, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2305.10799)][[Code (in construction)](https://github.com/Qybc/MedBLIP)]
* **GTGM**: "Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation", arXiv, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2306.04811)]
* **XrayGPT**: "XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models", arXiv, 2023 (*MBZUAI*). [[Paper](https://arxiv.org/abs/2306.07971)][[PyTorch](https://github.com/mbzuai-oryx/XrayGPT)]
* **CONCH**: "Towards a Visual-Language Foundation Model for Computational Pathology", arXiv, 2023 (*Harvard*). [[Paper](https://arxiv.org/abs/2307.12914)]
* **Med-Flamingo**: "Med-Flamingo: a Multimodal Medical Few-shot Learner", arXiv, 2023 (*Stanford*). [[Paper](https://arxiv.org/abs/2307.15189)][[PyTorch](https://github.com/snap-stanford/med-flamingo)]
* **?**: "Can GPT-4V(ision) Serve Medical Applications? Case Studies on GPT-4V for Multimodal Medical Diagnosis", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2310.09909)][[GitHub](https://github.com/chaoyi-wu/GPT-4V_Medical_Evaluation)]
* **CLIP-MUSED**: "CLIP-MUSED: CLIP-Guided Multi-Subject Visual Neural Information Semantic Decoding", ICLR, 2024 (*CAS*). [[Paper](https://arxiv.org/abs/2402.08994)][[PyTorch](https://github.com/CLIP-MUSED/CLIP-MUSED)]
* **RAD-DINO**: "RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision", arXiv, 2024 (*Microsoft*). [[Paper](https://arxiv.org/abs/2401.10815)]

[[Back to Overview](#overview)]

### Medical Others
* **LAT**: "Lesion-Aware Transformers for Diabetic Retinopathy Grading", CVPR, 2021 (*USTC*). [[Paper](https://openaccess.thecvf.com/content/CVPR2021/html/Sun_Lesion-Aware_Transformers_for_Diabetic_Retinopathy_Grading_CVPR_2021_paper.html)]
* **UVT**: "Ultrasound Video Transformers for Cardiac Ejection Fraction Estimation", MICCAI, 2021 (*ICL*). [[Paper](https://arxiv.org/abs/2107.00977)][[PyTorch](https://github.com/HReynaud/UVT)]
* **?**: "Surgical Instruction Generation with Transformers", MICCAI, 2021 (*Bournemouth University, UK*). [[Paper](https://arxiv.org/abs/2107.06964)]
* **AlignTransformer**: "AlignTransformer: Hierarchical Alignment of Visual Regions and Disease Tags for Medical Report Generation", MICCAI, 2021 (*Peking University*). [[Paper](https://arxiv.org/abs/2203.10095)]
* **MCAT**: "Multimodal Co-Attention Transformer for Survival Prediction in Gigapixel Whole Slide Images", ICCV, 2021 (*Harvard*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Multimodal_Co-Attention_Transformer_for_Survival_Prediction_in_Gigapixel_Whole_Slide_ICCV_2021_paper.html)][[PyTorch](https://github.com/mahmoodlab/MCAT)]
* **?**: "Is it Time to Replace CNNs with Transformers for Medical Images?", ICCVW, 2021 (*KTH, Sweden*). [[Paper](https://arxiv.org/abs/2108.09038)]
* **HAT-Net**: "HAT-Net: A Hierarchical Transformer Graph Neural Network for Grading of Colorectal Cancer Histology Images", BMVC, 2021 (*Beijing
University of Posts and Telecommunications*). [[Paper](https://www.bmvc2021-virtualconference.com/assets/papers/1245.pdf)]
* **?**: "Federated Split Vision Transformer for COVID-19 CXR Diagnosis using Task-Agnostic Training", NeurIPS, 2021 (*KAIST*). [[Paper](https://arxiv.org/abs/2111.01338)]
* **ViT-Path**: "Self-Supervised Vision Transformers Learn Visual Concepts in Histopathology", NeurIPSW, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2203.00585)]
* **Global-Local-Transformer**: "Global-Local Transformer for Brain Age Estimation", IEEE Transactions on Medical Imaging, 2021 (*Harvard*). [[Paper](https://arxiv.org/abs/2109.01663)][[PyTorch](https://github.com/shengfly/global-local-transformer)]
* **CE-TFE**: "Deep Transformers for Fast Small Intestine Grounding in Capsule Endoscope Video", arXiv, 2021 (*Sun Yat-Sen University*). [[Paper](https://arxiv.org/abs/2104.02866)]
* **DeepProg**: "DeepProg: A Transformer-based Framework for Predicting Disease Prognosis", arXiv, 2021 (*University of Oulu*). [[Paper](https://arxiv.org/abs/2104.03642)]
* **Medical-Transformer**: "Medical Transformer: Universal Brain Encoder for 3D MRI Analysis", arXiv, 2021 (*Korea University*). [[Paper](https://arxiv.org/abs/2104.13633)]
* **RATCHET**: "RATCHET: Medical Transformer for Chest X-ray Diagnosis and Reporting", arXiv, 2021 (*ICL*). [[Paper](https://arxiv.org/abs/2107.02104)]
* **C2FViT**: "Affine Medical Image Registration with Coarse-to-Fine Vision Transformer", CVPR, 2022 (*HKUST*). [[Paper](https://arxiv.org/abs/2203.15216)][[Code (in construction)](https://github.com/cwmok/C2FViT)]
* **HIPT**: "Scaling Vision Transformers to Gigapixel Images via Hierarchical Self-Supervised Learning", CVPR, 2022 (*Harvard*). [[Paper](https://arxiv.org/abs/2206.02647)]
* **SiT**: "Surface Analysis with Vision Transformers", CVPRW, 2022 (*King’s College London, UK*). [[Paper](https://arxiv.org/abs/2205.15836)][[PyTorch](https://github.com/metrics-lab/surface-vision-transformers)]
* **SiT**: "Surface Vision Transformers: Attention-Based Modelling applied to Cortical Analysis", Medical Imaging with Deep Learning (MIDL), 2022 (*King’s College London, UK*). [[Paper](https://arxiv.org/abs/2203.16414)]
* **ViT-V-Net**: "ViT-V-Net: Vision Transformer for Unsupervised Volumetric Medical Image Registration", ICML, 2022 (*JHU*). [[Paper](https://arxiv.org/abs/2104.06468)][[PyTorch](https://github.com/junyuchen245/ViT-V-Net_for_3D_Image_Registration_Pytorch)]
* **HybridStereoNet**: "Deep Laparoscopic Stereo Matching with Transformers", MICCAI, 2022 (*Monash University, Australia*). [[Paper](https://arxiv.org/abs/2207.12152)][[PyTorch](https://github.com/XuelianCheng/HybridStereoNet)]
* **BabyNet**: "BabyNet: Residual Transformer Module for Birth Weight Prediction on Fetal Ultrasound Video", MICCAI, 2022 (*Sano Centre for Computational Medicine, Poland*). [[Paper](https://arxiv.org/abs/2205.09382)][[PyTorch](https://github.com/SanoScience/BabyNet)]
* **TLT**: "Transformer Lesion Tracker", MICCAI, 2022 (*InferVision Medical Technology, China*). [[Paper](https://arxiv.org/abs/2206.06252)]
* **XMorpher**: "XMorpher: Full Transformer for Deformable Medical Image Registration via Cross Attention", MICCAI, 2022 (*Southeast University, China*). [[Paper](https://arxiv.org/abs/2206.07349)][[PyTorch](https://github.com/Solemoon/XMorpher)]
* **SVoRT**: "SVoRT: Iterative Transformer for Slice-to-Volume Registration in Fetal Brain MRI", MICCAI, 2022 (*MIT*). [[Paper](https://arxiv.org/abs/2206.10802)]
* **GaitForeMer**: "GaitForeMer: Self-Supervised Pre-Training of Transformers via Human Motion Forecasting for Few-Shot Gait Impairment Severity Estimation", MICCAI, 2022 (*Stanford*). [[Paper](https://arxiv.org/abs/2207.00106)][[PyTorch](https://github.com/markendo/GaitForeMer)]
* **LKU-Net**: "U-Net vs Transformer: Is U-Net Outdated in Medical Image Registration?", MICCAIW, 2022 (*University of Birmingham, UK*). [[Paper](https://arxiv.org/abs/2208.04939)]
* **LVOT**: "Shifted Windows Transformers for Medical Image Quality Assessment", MICCAIW, 2022 (*Istanbul Technical University, Turkey*). [[Paper](https://arxiv.org/abs/2208.06034)]
* **MINiT**: "Multiple Instance Neuroimage Transformer", MICCAIW, 2022 (*Stanford*). [[Paper](https://arxiv.org/abs/2208.09567)][[Code (in construction)](https://github.com/singlaayush/MINIT)]
* **BrainNetTF**: "Brain Network Transformer", NeurIPS, 2022 (*Emory University*). [[Paper](https://arxiv.org/abs/2210.06681)][[PyTorch](https://github.com/Wayfear/BrainNetworkTransformer)]
* **SiT**: "Surface Vision Transformers: Flexible Attention-Based Modelling of Biomedical Surfaces", arXiv, 2022 (*King’s College London, UK*). [[Paper](https://arxiv.org/abs/2204.03408)][[PyTorch](https://github.com/metrics-lab/surface-vision-transformers)]
* **TransMorph**: "TransMorph: Transformer for unsupervised medical image registration", arXiv, 2022 (*JHU*). [[Paper](https://arxiv.org/abs/2111.10480)]
* **SymTrans**: "Symmetric Transformer-based Nwholeetwork for Unsupervised Image Registration", arXiv, 2022 (*Jilin University*). [[Paper](https://arxiv.org/abs/2204.13575)]
* **MMT**: "One Model to Synthesize Them All: Multi-contrast Multi-scale Transformer for Missing Data Imputation", arXiv, 2022 (*JHU*). [[Paper](https://arxiv.org/abs/2204.13738)]
* **EG-ViT**: "Eye-gaze-guided Vision Transformer for Rectifying Shortcut Learning", arXiv, 2022 (*Northwestern Polytechnical University*). [[Paper](https://arxiv.org/abs/2205.12466)]
* **CSM**: "Contrastive Transformer-based Multiple Instance Learning for Weakly Supervised Polyp Frame Detection", arXiv, 2022 (*University of Adelaide, Australia*). [[Paper](https://arxiv.org/abs/2203.12121)]
* **CASHformer**: "CASHformer: Cognition Aware SHape Transformer for Longitudinal Analysis", arXiv, 2022 (*TUM*). [[Paper](https://arxiv.org/abs/2207.02091)]
* **ARST**: "ARST: Auto-Regressive Surgical Transformer for Phase Recognition from Laparoscopic Videos", arXiv, 2022 (*Shanghai Jiao Tong University*). [[Paper](https://arxiv.org/abs/2209.01148)]
* **SSiT**: "SSiT: Saliency-guided Self-supervised Image Transformer for Diabetic Retinopathy Grading", arXiv, 2022 (*Southern University of Science and Techonology, China*). [[Paper](https://arxiv.org/abs/2210.10969)][[Code (in construction)](https://github.com/YijinHuang/SSiT)]
* **MulGT**: "MulGT: Multi-task Graph-Transformer with Task-aware Knowledge Injection and Domain Knowledge-driven Pooling for Whole Slide Image Analysis", AAAI, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2302.10574)]
* **HVTSurv**: "HVTSurv: Hierarchical Vision Transformer for Patient-Level Survival Prediction from Whole Slide Image", AAAI, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2306.17373)][[PyTorch](https://github.com/szc19990412/HVTSurv)]
* **AMIGO**: "AMIGO: Sparse Multi-Modal Graph Transformer with Shared-Context Processing for Representation Learning of Giga-pixel Images", CVPR, 2023 (*UBC*). [[Paper](https://arxiv.org/abs/2303.00865)]
* **ACAT**: "ACAT: Adversarial Counterfactual Attention for Classification and Detection in Medical Imaging", ICML, 2023 (*University of Edinburgh, UK*). [[Paper](https://arxiv.org/abs/2303.15421)]
* **ConSlide**: "ConSlide: Asynchronous Hierarchical Interaction Transformer with Breakup-Reorganize Rehearsal for Continual Whole Slide Image Analysis", ICCV, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2308.13324)]
* **MOTCat**: "Multimodal Optimal Transport-based Co-Attention Transformer with Global Structure Consistency for Survival Prediction", ICCV, 2023 (*HKUST*). [[Paper](https://arxiv.org/abs/2306.08330)][[PyTorch](https://github.com/Innse/MOTCat)]
* **ViT-DAE**: "ViT-DAE: Transformer-driven Diffusion Autoencoder for Histopathology Image Analysis", arXiv, 2023 (*Stony Brook*). [[Paper](https://arxiv.org/abs/2304.01053)]

[[Back to Overview](#overview)]


## Other Tasks
* Active Learning:
    * **TJLS**: "Visual Transformer for Task-aware Active Learning", arXiv, 2021 (*ICL*). [[Paper](https://arxiv.org/abs/2106.03801)][[PyTorch](https://github.com/razvancaramalau/Visual-Transformer-for-Task-aware-Active-Learning)]
* Agriculture:
    * **PlantXViT**: "Explainable vision transformer enabled convolutional neural network for plant disease identification: PlantXViT", arXiv, 2022 (*Indian Institute of Information Technology*). [[Paper](https://arxiv.org/abs/2207.07919)]
    * **MMST-ViT**: "MMST-ViT: Climate Change-aware Crop Yield Prediction via Multi-Modal Spatial-Temporal Vision Transformer", ICCV, 2023 (*University of Delaware, Delaware*). [[Paper](https://arxiv.org/abs/2309.09067)][[PyTorch](https://github.com/fudong03/MMST-ViT)]
* Aesthetic:
    * **CSKD**: "CLIP Brings Better Features to Visual Aesthetics Learners", arXiv, 2023 (*OPPO*). [[Paper](https://arxiv.org/abs/2307.15640)]
    * **AesBench**: "AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception", arXiv, 2024 (*Xidian University*). [[Paper](https://arxiv.org/abs/2401.08276)][[GitHub](https://github.com/yipoh/AesBench)]
* Animation-related:
    * **AnT**: "The Animation Transformer: Visual Correspondence via Segment Matching", ICCV, 2021 (*Cadmium*). [[Paper](https://arxiv.org/abs/2109.02614)]
    * **AniFormer**: "AniFormer: Data-driven 3D Animation with Transformer", BMVC, 2021 (*University of Oulu, Finland*). [[Paper](https://arxiv.org/abs/2110.10533)][[PyTorch](https://github.com/mikecheninoulu/AniFormer)]
* Bird's Eye View (BEV):
    * **ViT-BEVSeg**: "ViT-BEVSeg: A Hierarchical Transformer Network for Monocular Birds-Eye-View Segmentation", IJCNN, 2022 (*Maynooth University, Ireland*). [[Paper](https://arxiv.org/abs/2205.15667)][[Code (in construction)](https://github.com/robotvisionmu/ViT-BEVSeg)]
    * **BEVFormer**: "BEVFormer: Learning Bird's-Eye-View Representation from Multi-Camera Images via Spatiotemporal Transformers", ECCV, 2022 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2203.17270)][[PyTorch](https://github.com/zhiqi-li/BEVFormer)]
    * **CoBEVT**: "CoBEVT: Cooperative Bird's Eye View Semantic Segmentation with Sparse Transformers", CoRL, 2022 (*UCLA*). [[Paper](https://arxiv.org/abs/2207.02202)][[PyTorch](https://github.com/DerrickXuNu/CoBEVT)]
    * **GKT**: "Efficient and Robust 2D-to-BEV Representation Learning via Geometry-guided Kernel Transformer", arXiv, 2022 (*Huazhong University of Science and Technology*). [[Paper](https://arxiv.org/abs/2206.04584)][[Code (in construction)](https://github.com/hustvl/GKT)]
    * **BEVSegFormer**: "BEVSegFormer: Bird's Eye View Semantic Segmentation From Arbitrary Camera Rigs", WACV, 2023 (*Nullmax, China*). [[Paper](https://arxiv.org/abs/2203.04050)]
    * **BEVDistill**: "BEVDistill: Cross-Modal BEV Distillation for Multi-View 3D Object Detection", ICLR, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2211.09386)][[Code (in constrcution)](https://github.com/zehuichen123/BEVDistill)]
    * **BEVFormer-v2**: "BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision", CVPR, 2023 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2211.10439)]
    * **BEV-SAN**: "BEV-SAN: Accurate BEV 3D Object Detection via Slice Attention Networks", CVPR, 2023 (*Peking University*). [[Paper](https://arxiv.org/abs/2212.01231)]
    * **BEVGuide**: "BEV-Guided Multi-Modality Fusion for Driving Perception", CVPR, 2023 (*UIUC*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Man_BEV-Guided_Multi-Modality_Fusion_for_Driving_Perception_CVPR_2023_paper.html)][[Code (in construction)](https://github.com/YunzeMan/BEVGuide)][[Website](https://yunzeman.github.io/BEVGuide/)]
    * **FB-OCC**: "FB-OCC: 3D Occupancy Prediction based on Forward-Backward View Transformation", CVPRW, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2307.01492)][[Code (in construction)](https://github.com/NVlabs/FB-BEV)]
    * **FB-BEV**: "FB-BEV: BEV Representation from Forward-Backward View Transformations", ICCV, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2308.02236)][[Code (in construction)](https://github.com/NVlabs/FB-BEV)]
    * **BEV-DG**: "BEV-DG: Cross-Modal Learning under Bird's-Eye View for Domain Generalization of 3D Semantic Segmentation", ICCV, 2023 (*Xiamen University*). [[Paper](https://arxiv.org/abs/2308.06530)]
    * **UniTR**: "UniTR: A Unified and Efficient Multi-Modal Transformer for Bird's-Eye-View Representation", ICCV, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2308.07732)][[PyTorch](https://github.com/Haiyang-W/UniTR)]
    * **SparseBEV**: "SparseBEV: High-Performance Sparse 3D Object Detection from Multi-Camera Videos", ICCV, 2023 (*Nanjing University*). [[Paper](https://arxiv.org/abs/2308.09244)][[Code (in construction)](https://github.com/MCG-NJU/SparseBEV)]
    * **OCBEV**: "OCBEV: Object-Centric BEV Transformer for Multi-View 3D Object Detection", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2306.01738)]
    * **FusionFormer**: "FusionFormer: A Multi-sensory Fusion in Bird's-Eye-View and Temporal Consistent Transformer for 3D Objection", arXiv, 2023 (*Cainiao Network, China*). [[Paper](https://arxiv.org/abs/2309.05257)]
    * **Talk2BEV**: "Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving", arXiv, 2023 (*IIIT Hyderabad*). [[Paper](https://arxiv.org/abs/2310.02251)][[Code](https://github.com/llmbev/talk2bev)][[Website](https://llmbev.github.io/talk2bev/)]
    * **SparseOcc**: "Fully Sparse 3D Panoptic Occupancy Prediction", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2312.17118)]
* Biology:
    * **?**: "A State-of-the-art Survey of Object Detection Techniques in Microorganism Image Analysis: from Traditional Image Processing and Classical Machine Learning to Current Deep Convolutional Neural Networks and Potential Visual Transformers", arXiv, 2021 (*Northeastern University*). [[Paper](https://arxiv.org/abs/2105.03148)]
* Brain Score:
    * **CrossViT**: "Joint rotational invariance and adversarial training of a dual-stream Transformer yields state of the art Brain-Score for Area V4", CVPRW, 2022 (*MIT*). [[Paper](https://arxiv.org/abs/2203.06649)][[PyTorch](https://github.com/williamberrios/BrainScore-Transformers)]
* Camera-related:
    * **CTRL-C**: "CTRL-C: Camera calibration TRansformer with Line-Classification", ICCV, 2021 (*Kakao + Kookmin University*). [[Paper](https://arxiv.org/abs/2109.02259)][[PyTorch](https://github.com/jwlee-vcl/CTRL-C)]
    * **MS-Transformer**: "Learning Multi-Scene Absolute Pose Regression with Transformers", ICCV, 2021 (*Bar-Ilan University, Israel*). [[Paper](https://arxiv.org/abs/2103.11468)][[PyTorch](https://github.com/yolish/multi-scene-pose-transformer)]
    * **GTCaR**: "GTCaR: Graph Transformer for Camera Re-localization", ECCV, 2022 (*Magic Leap*). [[Paper](https://www3.cs.stonybrook.edu/~hling/publication/GTCaR22.pdf)]
    * **?**: "Boosting 3-DoF Ground-to-Satellite Camera Localization Accuracy via Geometry-Guided Cross-View Transformer", ICCV, 2023 (*ANU*). [[Paper](https://arxiv.org/abs/2307.08015)]
* Change Detection:
    * **MapFormer**: "MapFormer: Boosting Change Detection by Using Pre-change Information", ICCV, 2023 (*LMU Munich*). [[Paper](https://arxiv.org/abs/2303.17859)][[PyTorch](https://github.com/mxbh/mapformer)]
* Character/Text Recognition:
    * **BTTR**: "Handwritten Mathematical Expression Recognition with Bidirectionally Trained Transformer", arXiv, 2021 (*Peking*). [[Paper](https://arxiv.org/abs/2105.02412)]
    * **TrOCR**: "TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models", arXiv, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2109.10282)][[PyTorch](https://github.com/microsoft/unilm/tree/master/trocr)]
    * **?**: "Robustness Evaluation of Transformer-based Form Field Extractors via Form Attacks", arXiv, 2021 (*Salesforce*). [[Paper](https://arxiv.org/abs/2110.04413)]
    * **T<sup>3</sup>**: "TrueType Transformer: Character and Font Style Recognition in Outline Format", Document Analysis Systems (DAS), 2022 (*Kyushu University*). [[Paper](https://arxiv.org/abs/2203.05338)]
    * **?**: "Transformer-based HTR for Historical Documents", ComHum, 2022 (*University of Zurich, Switzerland*). [[Paper](https://arxiv.org/abs/2203.11008)]
    * **?**: "SVG Vector Font Generation for Chinese Characters with Transformer", ICIP, 2022 (*The University of Tokyo*). [[Paper](https://arxiv.org/abs/2206.10329)]
    * **LP-Transformer**: "Forensic License Plate Recognition with Compression-Informed Transformers", ICIP, 2022 (*University of Erlangen-Nurnberg, Germany*). [[Paper](https://arxiv.org/abs/2207.14686)]
    * **CoMER**: "CoMER: Modeling Coverage for Transformer-based Handwritten Mathematical Expression Recognition", ECCV, 2022 (*Peking University*). [[Paper](https://arxiv.org/abs/2207.04410)][[PyTorch](https://github.com/Green-Wood/CoMER)]
    * **MATRN**: "Multi-modal Text Recognition Networks: Interactive Enhancements between Visual and Semantic Features", ECCV, 2022 (*KAIST*). [[Paper](https://arxiv.org/abs/2111.15263)][[PyTorch](https://github.com/byeonghu-na/MATRN)]
    * **CONSENT**: "CONSENT: Context Sensitive Transformer for Bold Words Classification", arXiv, 2022 (*Amazon*). [[Paper](https://arxiv.org/abs/2205.07683)]
    * **DeepVecFont-v2**: "DeepVecFont-v2: Exploiting Transformers to Synthesize Vector Fonts with Higher Quality", CVPR, 2023 (*Peking University*). [[Paper](https://arxiv.org/abs/2303.14585)][[Code (in construction)](https://github.com/yizhiwang96/deepvecfont-v2)]
    * **SVGformer**: "SVGformer: Representation Learning for Continuous Vector Graphics Using Transformers", CVPR, 2023 (*Adobe*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Cao_SVGformer_Representation_Learning_for_Continuous_Vector_Graphics_Using_Transformers_CVPR_2023_paper.html)]
    * **SIGA**: "Self-supervised Implicit Glyph Attention for Text Recognition", CVPR, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2203.03382)]
    * **LISTER**: "LISTER: Neighbor Decoding for Length-Insensitive Scene Text Recognition", ICCV, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2308.12774)][[PyTorch](https://github.com/AlibabaResearch/AdvancedLiterateMachinery)]
    * **CCR-CLIP**: "Chinese Text Recognition with A Pre-Trained CLIP-Like Model Through Image-IDS Aligning", ICCV, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2309.01083)][[PyTorch](https://github.com/FudanVI/FudanOCR/tree/main/image-ids-CTR)]
    * **CLIPTER**: "CLIPTER: Looking at the Bigger Picture in Scene Text Recognition", ICCV, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2301.07464)]
    * **CLIP4STR**: "CLIP4STR: A Simple Baseline for Scene Text Recognition with Pre-trained Vision-Language Model", arXiv, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2305.14014)]
* Curriculum Learning:
    * **SSTN**: "Spatial Transformer Networks for Curriculum Learning", arXiv, 2021 (*TU Kaiserslautern, Germany*). [[Paper](https://arxiv.org/abs/2108.09696)]
* Defect Classification:
    * **MSHViT**: "Multi-Scale Hybrid Vision Transformer and Sinkhorn Tokenizer for Sewer Defect Classification", CVPRW, 2022 (*Aalborg University, Denmark*). [[Paper](https://drive.google.com/file/d/1UkSUL2z1hDVy1uCdw_Iu8dZaFyeXdQSR/view)]
    * **DefT**: "Defect Transformer: An Efficient Hybrid Transformer Architecture for Surface Defect Detection", arXiv, 2022 (*Nanjing University of Aeronautics and Astronautics*). [[Paper](https://arxiv.org/abs/2207.08319)]
* Digital Holography:
    * **?**: "Convolutional Neural Network (CNN) vs Visual Transformer (ViT) for Digital Holography", ICCCR, 2022 (*UBFC, France*). [[Paper](https://arxiv.org/abs/2108.09147)]
* Disentangled representation:
    * **VCT**: "Visual Concepts Tokenization", NeurIPS, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2205.10093)][[PyTorch](https://github.com/thomasmry/VCT)]
* E-Commerce:
    * **WebShop**: "WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents", NeurIPS, 2022 (*Princeton*). [[Paper](https://arxiv.org/abs/2207.01206)][[PyTorch](https://github.com/princeton-nlp/WebShop)][[Website](https://webshop-pnlp.github.io/)]
    * **ECLIP**: "Learning Instance-Level Representation for Large-Scale Multi-Modal Pretraining in E-commerce", CVPR, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2304.02853)]
* Event data:
    * **EvT**: "Event Transformer: A sparse-aware solution for efficient event data processing", arXiv, 2022 (*Universidad de Zaragoza, Spain*). [[Paper](https://arxiv.org/abs/2204.03355)][[PyTorch](https://github.com/AlbertoSabater/EventTransformer)]
    * **ETB**: "Event Transformer", arXiv, 2022 (*Nanjing University*). [[Paper](https://arxiv.org/abs/2204.05172)]
    * **RVT**: "Recurrent Vision Transformers for Object Detection with Event Cameras", CVPR, 2023 (*University of Zurich*). [[Paper](https://arxiv.org/abs/2212.05598)]
    * **Eventful-Transformer**: "Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers", ICCV, 2023 (*UW Madison*). [[Paper](https://arxiv.org/abs/2308.13494)][[PyTorch](https://github.com/WISION-Lab/eventful-transformer)][[Website](https://wisionlab.com/project/eventful-transformers/)]
    * **GET**: "GET: Group Event Transformer for Event-Based Vision", ICCV, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2310.02642)][[PyTorch](https://github.com/Peterande/GET-Group-Event-Transformer)]
    * **?**: "Cross-modal Orthogonal High-rank Augmentation for RGB-Event Transformer-trackers", ICCV, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2307.04129)][[PyTorch](https://github.com/ZHU-Zhiyu/High-Rank_RGB-Event_Tracker)]
    * **SODFormer**: "SODFormer: Streaming Object Detection with Transformer Using Events and Frames", TPAMI, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2308.04047)][[PyTorch](https://github.com/dianzl/SODFormer)]
    * **EventSAM**: "Segment Any Events via Weighted Adaptation of Pivotal Tokens", arXiv, 2023 (*Xidian University*). [[Paper](https://arxiv.org/abs/2312.16222)][[PyTorch (in construction)](https://github.com/happychenpipi/EventSAM)]
* Fashion:
    * **Kaleido-BERT**: "Kaleido-BERT: Vision-Language Pre-training on Fashion Domain", CVPR, 2021 (*Alibaba*). [[Paper](https://arxiv.org/abs/2103.16110)][[Tensorflow](https://github.com/mczhuge/Kaleido-BERT)]
    * **CIT**: "Cloth Interactive Transformer for Virtual Try-On", arXiv, 2021 (*University of Trento*). [[Paper](https://arxiv.org/abs/2104.05519)][[Code (in construction)](https://github.com/Amazingren/CIT)]
    * **ClothFormer**: "ClothFormer: Taming Video Virtual Try-on in All Module", CVPR, 2022 (*iQIYI*). [[Paper](https://arxiv.org/abs/2204.12151)][[Website](https://cloth-former.github.io/)]
    * **FashionVLP**: "FashionVLP: Vision Language Transformer for Fashion Retrieval With Feedback", CVPR, 2022 (*Amazon*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Goenka_FashionVLP_Vision_Language_Transformer_for_Fashion_Retrieval_With_Feedback_CVPR_2022_paper.html)]
    * **FashionViL**: "FashionViL: Fashion-Focused Vision-and-Language Representation Learning", ECCV, 2022 (*University of Surrey, UK*). [[Paper](https://arxiv.org/abs/2207.08150)][[PyTorch](https://github.com/BrandonHanx/mmf)]
    * **OutfitTransformer**: "OutfitTransformer: Learning Outfit Representations for Fashion Recommendation", arXiv, 2022 (*Amazon*). [[Paper](https://arxiv.org/abs/2204.04812)]
    * **FaD-VLP**: "FaD-VLP: Fashion Vision-and-Language Pre-training towards Unified Retrieval and Captioning", EMNLP, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2210.15028)]
    * **Fashionformer**: "Fashionformer: A simple, Effective and Unified Baseline for Human Fashion Segmentation and Recognition", ECCV, 2022 (*Peking*). [[Paper](https://arxiv.org/abs/2204.04654)][[PyTorch](https://github.com/xushilin1/FashionFormer)]
    * **FAME-ViL**: "FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks", CVPR, 2023 (*University of Surrey*). [[Paper](https://arxiv.org/abs/2303.02483)][[PyTorch](https://github.com/BrandonHanx/FAME-ViL)]
    * **FashionSAP**: "FashionSAP: Symbols and Attributes Prompt for Fine-grained Fashion Vision-Language Pre-training", CVPR, 2023 (*Harbin Institute of Technology*). [[Paper](https://arxiv.org/abs/2304.05051)][[PyTorch](https://github.com/hssip/FashionSAP)]
    * **OpenFashionCLIP**: "OpenFashionCLIP: Vision-and-Language Contrastive Learning with Open-Source Fashion Data", ICIAP, 2023 (*UniMoRE, Italy*). [[Paper](https://arxiv.org/abs/2309.05551)][[PyTorch](https://github.com/aimagelab/open-fashion-clip)]
    * **MVLT**: "Masked Vision-Language Transformer in Fashion", Machine Intelligence Research, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2210.15110)][[PyTorch](https://github.com/GewelsJI/MVLT)]
    * **UniDiff**: "UniDiff: Advancing Vision-Language Models with Generative and Discriminative Learning", arXiv, 2023 (*Sun Yat-Sen University*). [[Paper](https://arxiv.org/abs/2306.00813)]
* Feature Matching:
    * **SuperGlue**: "SuperGlue: Learning Feature Matching with Graph Neural Networks", CVPR, 2020 (*Magic Leap*). [[Paper](https://arxiv.org/abs/1911.11763)][[PyTorch](https://github.com/magicleap/SuperGluePretrainedNetwork)]
    * **LoFTR**: "LoFTR: Detector-Free Local Feature Matching with Transformers", CVPR, 2021 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2104.00680)][[PyTorch](https://github.com/zju3dv/LoFTR)][[Website](https://zju3dv.github.io/loftr/)]
    * **COTR**: "COTR: Correspondence Transformer for Matching Across Images", ICCV, 2021 (*UBC*). [[Paper](https://arxiv.org/abs/2103.14167)]
    * **CATs**: "CATs: Cost Aggregation Transformers for Visual Correspondence", NeurIPS, 2021 (*Yonsei University + Korea University*). [[Paper](https://arxiv.org/abs/2106.02520)][[PyTorch](https://github.com/SunghwanHong/Cost-Aggregation-transformers)][[Website](https://sunghwanhong.github.io/CATs/)]
    * **TransforMatcher**: "TransforMatcher: Match-to-Match Attention for Semantic Correspondence", CVPR, 2022 (*POSTECH*). [[Paper](https://arxiv.org/abs/2205.11634)]
    * **ASpanFormer**: "ASpanFormer: Detector-Free Image Matching with Adaptive Span Transformer", ECCV, 2022 (*HKUST*). [[Paper](https://arxiv.org/abs/2208.14201)][[Website](https://aspanformer.github.io/)]
    * **CATs++**: "CATs++: Boosting Cost Aggregation with Convolutions and Transformers", arXiv, 2022 (*Korea University*). [[Paper](https://arxiv.org/abs/2202.06817)]
    * **LoFTR-TensorRT**: "Local Feature Matching with Transformers for low-end devices", arXiv, 2022 (*?*). [[Paper](https://arxiv.org/abs/2202.00770)][[PyTorch](https://github.com/Kolkir/Coarse_LoFTR_TRT)]
    * **MatchFormer**: "MatchFormer: Interleaving Attention in Transformers for Feature Matching", arXiv, 2022 (*Karlsruhe Institute of Technology, Germany*). [[Paper](https://arxiv.org/abs/2203.09645)]
    * **OpenGlue**: "OpenGlue: Open Source Graph Neural Net Based Pipeline for Image Matching", arXiv, 2022 (*Ukrainian Catholic University*). [[Paper](https://arxiv.org/abs/2204.08870)][[PyTorch](https://github.com/ucuapps/OpenGlue)]
    * **ParaFormer**: "ParaFormer: Parallel Attention Transformer for Efficient Feature Matching", AAAI, 2023 (*Southeast University, China*). [[Paper](https://arxiv.org/abs/2303.00941)]
    * **ASTR**: "Adaptive Spot-Guided Transformer for Consistent Local Feature Matching", CVPR, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2303.16624)][[Website](https://astr2023.github.io/)]
    * **ACTR**: "Correspondence Transformers with Asymmetric Feature Learning and Matching Flow Super-Resolution", CVPR, 2023 (*Fudan*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Sun_Correspondence_Transformers_With_Asymmetric_Feature_Learning_and_Matching_Flow_Super-Resolution_CVPR_2023_paper.html)][[Code (in construction)](https://github.com/YXSUNMADMAX/ACTR)]
    * **D<sup>2</sup>Former**: "D<sup>2</sup>Former: Jointly Learning Hierarchical Detectors and Contextual Descriptors via Agent-based Transformers", CVPR, 2023 (*USTC*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/He_D2Former_Jointly_Learning_Hierarchical_Detectors_and_Contextual_Descriptors_via_Agent-Based_CVPR_2023_paper.html)]
    * **PMatch**: "PMatch: Paired Masked Image Modeling for Dense Geometric Matching", CVPR, 2023 (*Michigan State*). [[Paper](https://arxiv.org/abs/2303.17342)][[Code (in construction)](https://github.com/ShngJZ/PMatch)]
    * **2D3D-MATR**: "2D3D-MATR: 2D-3D Matching Transformer for Detection-free Registration between Images and Point Clouds", ICCV, 2023 (*National University of Defense Technology, China*). [[Paper](https://arxiv.org/abs/2308.05667)][[PyTorch (in construction)](https://github.com/minhaolee/2D3DMATR)]
    * **CasMTR**: "Improving Transformer-based Image Matching by Cascaded Capturing Spatially Informative Keypoints", ICCV, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2303.02885)][[PyTorch](https://github.com/ewrfcas/CasMTR)]
    * **Fuse-ViT**: "A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence", NeurIPS, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2305.15347)][[Website](https://sd-complements-dino.github.io/)]
    * **Diffusion-Hyperfeature**: "Diffusion Hyperfeatures: Searching Through Time and Space for Semantic Correspondence", NeurIPS, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2305.14334)][[PyTorch](https://github.com/diffusion-hyperfeatures/diffusion_hyperfeatures)][[Website](https://diffusion-hyperfeatures.github.io/)]
    * **LDM-correspondence**: "Unsupervised Semantic Correspondence Using Stable Diffusion", NeurIPS, 2023 (*UBC*). [[Paper](https://arxiv.org/abs/2305.15581)][[PyTorch](https://github.com/ubc-vision/LDM_correspondences)][[Website](https://ubc-vision.github.io/LDM_correspondences/)]
    * **VSFormer**: "VSFormer: Visual-Spatial Fusion Transformer for Correspondence Pruning", AAAI, 2024 (*Wenzhou University*). [[Paper](https://arxiv.org/abs/2312.08774)][[Code (in construction)](https://github.com/sugar-fly/VSFormer)]
* Fine-grained:
    * **ViT-FGVC**: "Exploring Vision Transformers for Fine-grained Classification", CVPRW, 2021 (*Universidad de Valladolid*). [[Paper](https://arxiv.org/abs/2106.10587)]
    * **FFVT**: "Feature Fusion Vision Transformer for Fine-Grained Visual Categorization", BMVC, 2021 (*Griffith University, Australia*). [[Paper](https://arxiv.org/abs/2107.02341)][[PyTorch](https://github.com/Markin-Wang/FFVT)]
    * **TPSKG**: "Transformer with Peak Suppression and Knowledge Guidance for Fine-grained Image Recognition", arXiv, 2021 (*Beihang University*). [[Paper](https://arxiv.org/abs/2107.06538)]
    * **AFTrans**: "A free lunch from ViT: Adaptive Attention Multi-scale Fusion Transformer for Fine-grained Visual Recognition", arXiv, 2021 (*Peking University*). [[Paper](https://arxiv.org/abs/2110.01240)]
    * **TransFG**: "TransFG: A Transformer Architecture for Fine-grained Recognition", AAAI, 2022 (*Johns Hopkins*). [[Paper](https://arxiv.org/abs/2103.07976)][[PyTorch](https://github.com/TACJu/TransFG)]
    * **DynamicMLP**: "Dynamic MLP for Fine-Grained Image Classification by Leveraging Geographical and Temporal Information", CVPR, 2022 (*Megvii*). [[Paper](https://arxiv.org/abs/2203.03253)][[PyTorch](https://github.com/ylingfeng/DynamicMLP)]
    * **SIM-Trans**: "SIM-Trans: Structure Information Modeling Transformer for Fine-grained Visual Categorization", ACMMM, 2022 (*Peking University*). [[Paper](https://arxiv.org/abs/2208.14607)][[PyTorch](https://github.com/PKU-ICST-MIPL/SIM-Trans_ACMMM2022)]
    * **MetaFormer**: "MetaFormer: A Unified Meta Framework for Fine-Grained Recognition", arXiv, 2022 (*ByteDance*). [[Paper](https://arxiv.org/abs/2203.02751)][[PyTorch](https://github.com/dqshuai/MetaFormer)]
    * **ViT-FOD**: "ViT-FOD: A Vision Transformer based Fine-grained Object Discriminator", arXiv, 2022 (*Shandong University*). [[Paper](https://arxiv.org/abs/2203.12816)]
    * **PLEor**: "Open-Set Fine-Grained Retrieval via Prompting Vision-Language Evaluator", CVPR, 2023 (*Dalian University of Technology*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Open-Set_Fine-Grained_Retrieval_via_Prompting_Vision-Language_Evaluator_CVPR_2023_paper.html)]
    * **MultitaskVLFM**: "Leveraging Vision-Language Foundation Models for Fine-Grained Downstream Tasks", arXiv, 2023 (*Conservatoire National des Arts et Métiers (CEDRIC) France*). [[Paper](https://arxiv.org/abs/2307.06795)][[PyTorch](https://github.com/FactoDeepLearning/MultitaskVLFM)]
    * **M2Former**: "M2Former: Multi-Scale Patch Selection for Fine-Grained Visual Recognition", arXiv, 2023 (*Dongguk University, Korea*). [[Paper](https://arxiv.org/abs/2308.02161)]
    * **MP-FGVC**: "Delving into Multimodal Prompting for Fine-grained Visual Classification", arXiv, 2023 (*Nanjing University of Science and Technology*). [[Paper](https://arxiv.org/abs/2309.08912)]
    * **HGCLIP**: "HGCLIP: Exploring Vision-Language Models with Graph Representations for Hierarchical Understanding", arXiv, 2023 (*Monash*). [[Paper](https://arxiv.org/abs/2311.14064)][[PyTorch](https://github.com/richard-peng-xia/HGCLIP)]
    * **FineR**: "Democratizing Fine-grained Visual Recognition with Large Language Models", ICLR, 2024 (*University of Trento*). [[Paper](https://arxiv.org/abs/2401.13837)][[Code (in construction)](https://github.com/OatmealLiu/FineR)][[Website](https://projfiner.github.io/)]
* Gait:
    * **Gait-TR**: "Spatial Transformer Network on Skeleton-based Gait Recognition", arXiv, 2022 (*South China University of Technology*). [[Paper](https://arxiv.org/abs/2204.03873)]
    * **MMGaitFormer**: "Multi-Modal Gait Recognition via Effective Spatial-Temporal Feature Fusion", CVPR, 2023 (*Beihang University*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Cui_Multi-Modal_Gait_Recognition_via_Effective_Spatial-Temporal_Feature_Fusion_CVPR_2023_paper.html)]
* Gaze:
    * **GazeTR**: "Gaze Estimation using Transformer", arXiv, 2021 (*Beihang University*). [[Paper](https://arxiv.org/abs/2105.14424)][[PyTorch](https://github.com/yihuacheng/GazeTR)]
    * **HGTTR**: "End-to-End Human-Gaze-Target Detection with Transformers", CVPR, 2022 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2203.10433)]
    * **MGTR**: "MGTR: End-to-End Mutual Gaze Detection with Transformer", ACCV, 2022 (*Nankai University*). [[Paper](https://arxiv.org/abs/2209.10930)][[PyTorch](https://github.com/Gmbition/MGTR)]
    * **GLC**: "In the Eye of Transformer: Global-Local Correlation for Egocentric Gaze Estimation", arXiv, 2022 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2208.04464)][[Website](https://bolinlai.github.io/GLC-EgoGazeEst/)]
    * **Gazeformer**: "Gazeformer: Scalable, Effective and Fast Prediction of Goal-Directed Human Attention", CVPR, 2023 (*Stony Brook*). [[Paper](https://arxiv.org/abs/2303.15274)][[PyTorch](https://github.com/cvlab-stonybrook/Gazeformer)]
    * **Sharingan**: "Sharingan: A Transformer-based Architecture for Gaze Following", arXiv, 2023 (*EPFL*). [[Paper](https://arxiv.org/abs/2310.00816)]
* Geo-Localization:
    * **EgoTR**: "Cross-view Geo-localization with Evolving Transformer", arXiv, 2021 (*Shenzhen University*). [[Paper](https://arxiv.org/abs/2107.00842)]
    * **TransGeo**: "TransGeo: Transformer Is All You Need for Cross-view Image Geo-localization", CVPR, 2022 (*UCF*). [[Paper](https://arxiv.org/abs/2204.00097)][[PyTorch](https://github.com/Jeff-Zilence/TransGeo2022)]
    * **GAMa**: "GAMa: Cross-view Video Geo-localization", ECCV, 2022 (*UCF*). [[Paper](https://arxiv.org/abs/2207.02431)][[Code (in construction)](https://github.com/svyas23/GAMa)]
    * **TransLocator**: "Where in the World is this Image? Transformer-based Geo-localization in the Wild", ECCV, 2022 (*JHU*). [[Paper](https://arxiv.org/abs/2204.13861)]
    * **TransGCNN**: "Transformer-Guided Convolutional Neural Network for Cross-View Geolocalization", arXiv, 2022 (*Southeast University, China*). [[Paper](https://arxiv.org/abs/2204.09967)]
    * **MGTL**: "Mutual Generative Transformer Learning for Cross-view Geo-localization", arXiv, 2022 (*University of Electronic Science and Technology of China*). [[Paper](https://arxiv.org/abs/2203.09135)]
    * **GeoGuessNet**: "Where We Are and What We're Looking At: Query Based Worldwide Image Geo-localization Using Hierarchies and Scenes", CVPR, 2023 (*UCF*). [[Paper](https://arxiv.org/abs/2303.04249)][[PyTorch (in construction)](https://github.com/AHKerrigan/GeoGuessNet)]
    * **GeoCLIP**: "GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization", NeurIPS, 2023 (*UCF*). [[Paper](https://arxiv.org/abs/2309.16020)]
* Homography Estimation:
    * **LocalTrans**: "LocalTrans: A Multiscale Local Transformer Network for Cross-Resolution Homography Estimation", ICCV, 2021 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2106.04067)]
* Image Registration:
    * **AiR**: "Attention for Image Registration (AiR): an unsupervised Transformer approach", arXiv, 2021 (*INRIA*). [[Paper](https://arxiv.org/abs/2105.02282)]
* Image Retrieval:
    * **RRT**: "Instance-level Image Retrieval using Reranking Transformers", ICCV, 2021 (*University of Virginia*). [[Paper](https://arxiv.org/abs/2103.12236)][[PyTorch](https://github.com/uvavision/RerankingTransformer)]
    * **SwinFGHash**: "SwinFGHash: Fine-grained Image Retrieval via Transformer-based Hashing Network", BMVC, 2021 (*Tsinghua*). [[Paper](https://www.bmvc2021-virtualconference.com/assets/papers/1551.pdf)]
    * **ViT-Retrieval**: "Investigating the Vision Transformer Model for Image Retrieval Tasks", arXiv, 2021 (*Democritus University of Thrace*). [[Paper](https://arxiv.org/abs/2101.03771)]
    * **IRT**: "Training Vision Transformers for Image Retrieval", arXiv, 2021 (*Facebook + INRIA*). [[Paper](https://arxiv.org/abs/2102.05644)]
    * **TransHash**: "TransHash: Transformer-based Hamming Hashing for Efficient Image Retrieval", arXiv, 2021 (*Shanghai Jiao Tong University*). [[Paper](https://arxiv.org/abs/2105.01823)]
    * **VTS**: "Vision Transformer Hashing for Image Retrieval", arXiv, 2021 (*IIIT-Allahabad*). [[Paper](https://arxiv.org/abs/2109.12564)]
    * **GTZSR**: "Zero-Shot Sketch Based Image Retrieval using Graph Transformer", arXiv, 2022 (*IIT Bombay*). [[Paper](https://arxiv.org/abs/2201.10185)]
    * **EViT**: "EViT: Privacy-Preserving Image Retrieval via Encrypted Vision Transformer in Cloud Computing", arXiv, 2022 (*Jinan University*). [[Paper](https://arxiv.org/abs/2208.14657)][[PyTorch (in construction)](https://github.com/onlinehuazai/EViT)]
    * **?**: "Transformers and CNNs both Beat Humans on SBIR", arXiv, 2022 (*University of Mons, Belgium*). [[Paper](https://arxiv.org/abs/2209.06629)]
    * **DToP**: "Boosting vision transformers for image retrieval", WACV, 2023 (*Dealicious, Korea*). [[Paper](https://arxiv.org/abs/2210.11909)][[Code (in construction)](https://github.com/dealicious-inc/DToP)]
    * **?**: "A Light Touch Approach to Teaching Transformers Multi-view Geometry", CVPR, 2023 (*Oxford*). [[Paper](https://arxiv.org/abs/2211.15107)]
    * **IRGen**: "IRGen: Generative Modeling for Image Retrieval", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2303.10126)]
    * **CIReVL**: "Vision-by-Language for Training-Free Compositional Image Retrieval", arXiv, 2023 (*University of Tübingen, Germany*). [[Paper](https://arxiv.org/abs/2310.09291)]
* Layout Generation:
    * **VTN**: "Variational Transformer Networks for Layout Generation", CVPR, 2021 (*Google*). [[Paper](https://arxiv.org/abs/2104.02416)]
    * **LayoutTransformer**: "LayoutTransformer: Scene Layout Generation With Conceptual and Spatial Diversity", CVPR, 2021 (*NTU*). [[Paper](https://openaccess.thecvf.com/content/CVPR2021/html/Yang_LayoutTransformer_Scene_Layout_Generation_With_Conceptual_and_Spatial_Diversity_CVPR_2021_paper.html)][[PyTorch](https://github.com/davidhalladay/LayoutTransformer)]
    * **LayoutTransformer**: "LayoutTransformer: Layout Generation and Completion with Self-attention", ICCV, 2021 (*Amazon*). [[Paper](https://arxiv.org/abs/2006.14615)][[Website](https://kampta.github.io/layout/)]
    * **LGT-Net**: "LGT-Net: Indoor Panoramic Room Layout Estimation with Geometry-Aware Transformer Network", CVPR, 2022 (*East China Normal University*). [[Paper](https://arxiv.org/abs/2203.01824)][[PyTorch](https://github.com/zhigangjiang/LGT-Net)]
    * **CADTransformer**: "CADTransformer: Panoptic Symbol Spotting Transformer for CAD Drawings", CVPR, 2022 (*UT Austin*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Fan_CADTransformer_Panoptic_Symbol_Spotting_Transformer_for_CAD_Drawings_CVPR_2022_paper.html)]
    * **GAT-CADNet**: "GAT-CADNet: Graph Attention Network for Panoptic Symbol Spotting in CAD Drawings", CVPR, 2022 (*TUM + Alibaba*). [[Paper](https://arxiv.org/abs/2201.00625)]
    * **LayoutBERT**: "LayoutBERT: Masked Language Layout Model for Object Insertion", CVPRW, 2022 (*Adobe*). [[Paper](https://arxiv.org/abs/2205.00347)]
    * **ICVT**: "Geometry Aligned Variational Transformer for Image-conditioned Layout Generation", ACMMM, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2209.00852)]
    * **BLT**: "BLT: Bidirectional Layout Transformer for Controllable Layout Generation", ECCV, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2112.05112)][[Tensorflow](https://github.com/google-research/google-research/tree/master/layout-blt)][[Website](https://shawnkx.github.io/blt)]
    * **ATEK**: "ATEK: Augmenting Transformers with Expert Knowledge for Indoor Layout Synthesis", arXiv, 2022 (*New Jersey Institute of Technology*). [[Paper](https://arxiv.org/abs/2202.00185)]
    * **?**: "Extreme Floorplan Reconstruction by Structure-Hallucinating Transformer Cascades", arXiv, 2022 (*Simon Fraser*). [[Paper](https://arxiv.org/abs/2206.00645)]
    * **LayoutFormer++**: "LayoutFormer++: Conditional Graphic Layout Generation via Constraint Serialization and Decoding Space Restriction", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2208.08037)]
    * **RoomFormer**: "Connecting the Dots: Floorplan Reconstruction Using Two-Level Queries", CVPR, 2023 (*ETH Zurich*). [[Paper](https://arxiv.org/abs/2211.15658)][[PyTorch](https://github.com/ywyue/RoomFormer)][[Website](https://ywyue.github.io/RoomFormer/)]
    * **LayoutDM**: "LayoutDM: Transformer-based Diffusion Model for Layout Generation", CVPR, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2305.02567)]
    * **DLT**: "DLT: Conditioned layout generation with Joint Discrete-Continuous Diffusion Layout Transformer", ICCV, 2023 (*Wix.com*). [[Paper](https://arxiv.org/abs/2303.03755)]
* Livestock Monitoring:
    * **STARFormer**: "Livestock Monitoring with Transformer", BMVC, 2021 (*IIT Dhanbad*). [[Paper](https://arxiv.org/abs/2111.00801)]
* Metric Learning:
    * **Hyp-ViT**: "Hyperbolic Vision Transformers: Combining Improvements in Metric Learning", CVPR, 2022 (*University of Trento, Italy*). [[Paper](https://arxiv.org/abs/2203.10833)][[PyTorch](https://github.com/htdt/hyp_metric)]
    * **BGFormer**: "Rethinking Batch Sample Relationships for Data Representation: A Batch-Graph Transformer based Approach", arXiv, 2022 (*Anhui University*). [[Paper](https://arxiv.org/abs/2211.10622)]
    * **?**: "Cross-Image-Attention for Conditional Embeddings in Deep Metric Learning", CVPR, 2023 (*LMU Munich*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Kotovenko_Cross-Image-Attention_for_Conditional_Embeddings_in_Deep_Metric_Learning_CVPR_2023_paper.html)]
* Multi-Input:
    * **MixViT**: "Adapting Multi-Input Multi-Output schemes to Vision Transformers", CVPRW, 2022 (*Sorbonne Universite, France*). [[Paper](https://remysun.github.io/publication/wcvpr_2022_mixvit/paper.pdf)]
* Multi-label:
    * **C-Tran**: "General Multi-label Image Classification with Transformers", CVPR, 2021 (*University of Virginia*). [[Paper](https://arxiv.org/abs/2011.14027)]
    * **TDRG**: "Transformer-Based Dual Relation Graph for Multi-Label Image Recognition", ICCV, 2021 (*Tencent*). [[Paper](https://arxiv.org/abs/2110.04722)]
    * **MlTr**: "MlTr: Multi-label Classification with Transformer", arXiv, 2021 (*KuaiShou*). [[Paper](https://arxiv.org/abs/2106.06195)]
    * **GATN**: "Graph Attention Transformer Network for Multi-Label Image Classification", arXiv, 2022 (*Southeast University, China*). [[Paper](https://arxiv.org/abs/2203.04049)]
    * **CDUL**: "CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification", ICCV, 2023 (*University of Southern Mississippi, Mississippi*). [[Paper](https://arxiv.org/abs/2307.16634)]
    * **TagCLIP**: "TagCLIP: A Local-to-Global Framework to Enhance Open-Vocabulary Multi-Label Classification of CLIP Without Training", AAAI, 2024 (*Zhejiang*). [[Paper](https://arxiv.org/abs/2312.12828)][[PyTorch](https://github.com/linyq2117/TagCLIP)]
* Multi-task:
    * **MulT**: "MulT: An End-to-End Multitask Learning Transformer", CVPR, 2022 (*EPFL*). [[Paper](https://arxiv.org/abs/2205.08303)]
    * **UFO**: "UFO: Unified Feature Optimization", ECCV, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2207.10341)][[PaddlePaddle](https://github.com/PaddlePaddle/VIMER/tree/main/UFO)]
    * **Painter**: "Images Speak in Images: A Generalist Painter for In-Context Visual Learning", CVPR, 2023 (*BAAI*). [[Paper](https://arxiv.org/abs/2212.02499)][[Code (in construction)](https://github.com/baaivision/Painter)]
* Open Set:
    * **OSR-ViT**: "Open Set Recognition using Vision Transformer with an Additional Detection Head", arXiv, 2022 (*Vanderbilt University, Tennessee*). [[Paper](https://arxiv.org/abs/2203.08441)]
* Operator Learning for PDEs:
    * **Galerkin Transformer**: "Choose a Transformer: Fourier or Galerkin", NeurIPS, 2021 (*Washington University, St. Louis*). [[Paper](https://arxiv.org/abs/2105.14995)][[PyTorch](https://github.com/scaomath/galerkin-transformer)]
    * **Coupled Attention**: "Learning operators with coupled attention", JMLR, 2022 (*University of Pennsylvania*). [[Paper](https://arxiv.org/abs/2201.01032)]
    * **HT-Net**: "HT-Net: Hierarchical Transformer based Operator Learning Model for Multiscale PDEs", arXiv, 2022 (*KAUST*). [[Paper](https://arxiv.org/abs/2210.10890)]
    * **Relative-PE**: "Transformer for Partial Differential Equations' Operator Learning", arXiv, 2022 (*CMU*). [[Paper](https://arxiv.org/abs/2205.13671)]
* Out-Of-Distribution (OOD):
    * **OODformer**: "OODformer: Out-Of-Distribution Detection Transformer", BMVC, 2021 (*LMU Munich*). [[Paper](https://arxiv.org/abs/2107.08976)][[PyTorch](https://github.com/rajatkoner08/oodformer)]
    * **MCM**: "Delving into Out-of-Distribution Detection with Vision-Language Representations", NeurIPS, 2022 (*UW-Madison*). [[Paper](https://arxiv.org/abs/2211.13445)]
    * **MOOD**: "Rethinking Out-of-distribution (OOD) Detection: Masked Image Modeling is All You Need", CVPR, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2302.02615)][[PyTorch](https://github.com/JulietLJY/MOOD)]
    * **?**: "Masked Images Are Counterfactual Samples for Robust Fine-tuning", CVPR, 2023 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2303.03052)][[PyTorch](https://github.com/Coxy7/robust-finetuning)]
    * **CLIPood**: "CLIPood: Generalizing CLIP to Out-of-Distributions", ICML, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2302.00864)]
    * **CLIPN**: "CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No", ICCV, 2023 (*HKUST*). [[Paper](https://arxiv.org/abs/2308.12213)][[PyTorch](https://github.com/xmed-lab/CLIPN)]
    * **?**: "Distilling Large Vision-Language Model with Out-of-Distribution Generalizability", ICCV, 2023 (*UCSD*). [[Paper](https://arxiv.org/abs/2307.03135)][[PyTorch](https://github.com/xuanlinli17/large_vlm_distillation_ood)]
    * **DREAM-OOD**: "Dream the Impossible: Outlier Imagination with Diffusion Models", NeurIPS, 2023 (*UW Madison*). [[Paper](https://arxiv.org/abs/2309.13415)]
    * **LoCoOp**: "LoCoOp: Few-Shot Out-of-Distribution Detection via Prompt Learning", NeurIPS, 2023 (*The University of Tokyo*). [[Paper](https://arxiv.org/abs/2306.01293)][[PyTorch](https://github.com/AtsuMiyai/LoCoOp)]
    * **?**: "A Closer Look at the Robustness of Contrastive Language-Image Pre-Training (CLIP)", NeurIPS, 2023 (*ANU*). [[Paper](https://arxiv.org/abs/2402.07410)]
    * **GL-MCM**: "Zero-Shot In-Distribution Detection in Multi-Object Settings Using Vision-Language Foundation Models", arXiv, 2023 (*The University of Tokyo*). [[Paper](https://arxiv.org/abs/2304.04521)]
    * **CLIP-OOD**: "Does CLIP's Generalization Performance Mainly Stem from High Train-Test Similarity?", arXiv, 2023 (*University of Tübingen*). [[Paper](https://arxiv.org/abs/2310.09562)]
    * **MOODv2**: "MOODv2: Masked Image Modeling for Out-of-Distribution Detection", arXiv, 2024 (*CUHK*). [[Paper](https://arxiv.org/abs/2401.02611)]
    * **AutoFT**: "AutoFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data", arXiv, 2024 (*Stanford*). [[Paper](https://arxiv.org/abs/2401.10220)]
* Pedestrian Intention:
    * **IntFormer**: "IntFormer: Predicting pedestrian intention with the aid of the Transformer architecture", arXiv, 2021 (*Universidad de Alcala*). [[Paper](https://arxiv.org/abs/2105.08647)]
* Physics Simulation:
    * **TIE**: "Transformer with Implicit Edges for Particle-based Physics Simulation", ECCV, 2022 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2207.10860)][[PyTorch](https://github.com/ftbabi/TIE_ECCV2022)][[Website](https://www.mmlab-ntu.com/project/tie/index.html)]
* Place Recognition:
    * **SVT-Net**: "SVT-Net: A Super Light-Weight Network for Large Scale Place Recognition using Sparse Voxel Transformers", AAAI, 2022 (*Renmin University of China*). [[Paper](https://arxiv.org/abs/2105.00149)]
    * **TransVPR**: "TransVPR: Transformer-based place recognition with multi-level attention aggregation", CVPR, 2022 (*Xi'an Jiaotong*). [[Paper](https://arxiv.org/abs/2201.02001)]
    * **OverlapTransformer**: "OverlapTransformer: An Efficient and Rotation-Invariant Transformer Network for LiDAR-Based Place Recognition", IROS, 2022 (*HAOMO.AI, China*). [[Paper](https://arxiv.org/abs/2203.03397)][[PyTorch](https://github.com/haomo-ai/OverlapTransformer)]
    * **SeqOT**: "SeqOT: A Spatial-Temporal Transformer Network for Place Recognition Using Sequential LiDAR Data", arXiv, 2022 (*National University of Defense Technology, China*). [[Paper](https://arxiv.org/abs/2209.07951)][[PyTorch](https://github.com/BIT-MJY/SeqOT)]
    * **R<sup>2</sup>Former**: "R<sup>2</sup>Former: Unified Retrieval and Reranking Transformer for Place Recognition", CVPR, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2304.03410)][[Code (in construction)](https://github.com/Jeff-Zilence/R2Former)]
* Remote Sensing/Hyperspectral/Satellite:
    * **DCFAM**: "Transformer Meets DCFAM: A Novel Semantic Segmentation Scheme for Fine-Resolution Remote Sensing Images", arXiv, 2021 (*Wuhan University*). [[Paper](https://arxiv.org/abs/2104.12137)]
    * **WiCNet**: "Looking Outside the Window: Wider-Context Transformer for the Semantic Segmentation of High-Resolution Remote Sensing Images", arXiv, 2021 (*University of Trento*). [[Paper](https://arxiv.org/abs/2106.15754)]
    * **?**: "Vision Transformers For Weeds and Crops Classification Of High Resolution UAV Images", arXiv, 2021 (*University of Orleans, France*). [[Paper](https://arxiv.org/abs/2109.02716)]
    * **Satellite-ViT**: "Manipulation Detection in Satellite Images Using Vision Transformer", arXiv, 2021 (*Purdue*). [[Paper](https://arxiv.org/abs/2105.06373)]
    * **?**: "Self-supervised Vision Transformers for Joint SAR-optical Representation Learning", IGARSS, 2022 (*German Aerospace Center*). [[Paper](https://arxiv.org/abs/2204.05381)]
    * **VBFusion**: "Multi-Modal Fusion Transformer for Visual Question Answering in Remote Sensing", SPIE Remote Sensing, 2022 (*Technische Universitat Berlin, Germany*). [[Paper](https://arxiv.org/abs/2210.04510)][[PyTorch](https://git.tu-berlin.de/rsim/multi-modal-fusion-transformer-for-vqa-in-rs)]
    * **SatMAE**: "SatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery", NeurIPS, 2022 (*Stanford*). [[Paper](https://arxiv.org/abs/2207.08051)]
    * **ANDT**: "Anomaly Detection in Aerial Videos with Transformers", IEEE Transactions on Geoscience and Remote Sensing (TGRS), 2022 (*TUM*). [[Paper](https://arxiv.org/abs/2209.13363)]
    * **RNGDet**: "RNGDet: Road Network Graph Detection by Transformer in Aerial Images", arXiv, 2022 (*HKUST*). [[Paper](https://arxiv.org/abs/2202.07824)]
    * **FSRA**: "A Transformer-Based Feature Segmentation and Region Alignment Method For UAV-View Geo-Localization", arXiv, 2022 (*China Jiliang University*). [[Paper](https://arxiv.org/abs/2201.09206)][[PyTorch](https://github.com/Dmmm1997/FSRA)]
    * **?**: "Multiscale Convolutional Transformer with Center Mask Pretraining for Hyperspectral Imag (e Cl)assificationtion", arXiv, 2022 (*Shenzhen University*). [[Paper](https://arxiv.org/abs/2203.04771)]
    * **?**: "Deep Hyperspectral Unmixing using Transformer Network", arXiv, 2022 (*Jalpaiguri Engineering College, India*). [[Paper](https://arxiv.org/abs/2203.17076)]
    * **SiamixFormer**: "SiamixFormer: A Siamese Transformer Network For Building Detection And Change Detection From Bi-Temporal Remote Sensing Images", arXiv, 2022 (*Tarbiat Modares University, Iran*). [[Paper](https://arxiv.org/abs/2208.00657)]
    * **DAHiTrA**: "DAHiTrA: Damage Assessment Using a Novel Hierarchical Transformer Architecture", arXiv, 2022 (*Simon Fraser University, Canada*). [[Paper](https://arxiv.org/abs/2208.02205)]
    * **RVSA**: "Advancing Plain Vision Transformer Towards Remote Sensing Foundation Model", arXiv, 2022 (*Wuhan University + The University of Sydney*). [[Paper](https://arxiv.org/abs/2208.03987)]
    * **SatViT**: "Transfer Learning with Pretrained Remote Sensing Transformers", arXiv, 2022 (*?*). [[Paper](https://arxiv.org/abs/2209.14969)][[PyTorch](https://github.com/antofuller/SatViT)]
    * **FTN**: "Fully Transformer Network for Change Detection of Remote Sensing Images", arXiv, 2022 (*Dalian University of Technology*). [[Paper](https://arxiv.org/abs/2210.00757)]
    * **MCTNet**: "MCTNet: A Multi-Scale CNN-Transformer Network for Change Detection in Optical Remote Sensing Images", arXiv, 2022 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2210.07601)]
    * **?**: "Transformers For Recognition In Overhead Imagery: A Reality Check", arXiv, 2022 (*Duke University*). [[Paper](https://arxiv.org/abs/2210.12599)]
    * **TSViT**: "ViTs for SITS: Vision Transformers for Satellite Image Time Series", CVPR, 2023 (*ICL*). [[Paper](https://arxiv.org/abs/2301.04944)][[PyTorch](https://github.com/michaeltrs/DeepSatModels)]
    * **MethaneMapper**: "MethaneMapper: Spectral Absorption aware Hyperspectral Transformer for Methane Detection", CVPR, 2023 (*UCSB*). [[Paper](https://arxiv.org/abs/2304.02767)]
    * **GFM**: "Towards Geospatial Foundation Models via Continual Pretraining", ICCV, 2023 (*Amazon*). [[Paper](https://arxiv.org/abs/2302.04476)][[PyTorch (in construction)](https://github.com/mmendiet/GFM)]
    * **Scale-MAE**: "Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning", ICCV, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2212.14532)]
    * **SAMRS**: "SAMRS: Scaling-up Remote Sensing Segmentation Dataset with Segment Anything Model", NeurIPS (Datasets and Benchmarks), 2023 (*iFlytek, China*). [[Paper](https://arxiv.org/abs/2305.02034)][[PyTorch](https://github.com/ViTAE-Transformer/SAMRS)]
    * **RS5M**: "RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model", arXiv, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2306.11300)][[Code (in construction)](https://github.com/om-ai-lab/RS5M)]
    * **RSGPT**: "RSGPT: A Remote Sensing Vision Language Model and Benchmark", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2307.15266)][[Code (in construction)](https://github.com/Lavender105/RSGPT)]
    * **EarthGPT**: "EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain", arXiv, 2024 (*Beijing Institute of Technology*). [[Paper](https://arxiv.org/abs/2401.16822)]
    * **AnyChange**: "Segment Any Change", arXiv, 2024 (*Stanford*). [[Paper](https://arxiv.org/abs/2402.01188)]
* Robotics:
    * **TF-Grasp**: "When Transformer Meets Robotic Grasping: Exploits Context for Efficient Grasp Detection", arXiv, 2022 (*University of Science and Technology of China*). [[Paper](https://arxiv.org/abs/2202.11911)][[Code (in construction)](https://github.com/WangShaoSUN/grasp-transformer)]
    * **BeT**: "Behavior Transformers: Cloning k modes with one stone", arXiv, 2022 (*NYU*). [[Paper](https://arxiv.org/abs/2206.11251)][[PyTorch](https://github.com/notmahi/bet)]
    * **Perceiver-Actor**: "Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation", Conference on Robot Learning (CoRL), 2022 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2209.05451)][[Website](https://peract.github.io/)]
    * **PACT**: "PACT: Perception-Action Causal Transformer for Autoregressive Robotics Pre-Training", arXiv, 2022 (*Microsoft*). [[Paper](https://arxiv.org/abs/2209.11133)]
    * **?**: "A Strong Transfer Baseline for RGB-D Fusion in Vision Transformers", arXiv, 2022 (*University of Groningen, The Netherlands*). [[Paper](https://arxiv.org/abs/2210.00843)]
    * **?**: "Grounding Language with Visual Affordances over Unstructured Data", arXiv, 2022 (*University of Freiburg, Germany*). [[Paper](https://arxiv.org/abs/2210.01911)][[Website](http://hulc2.cs.uni-freiburg.de/)]
    * **?**: "Lossless Adaptation of Pretrained Vision Models For Robotic Manipulation", ICLR, 2023 (*DeepMind*). [[Paper](https://openreview.net/forum?id=5IND3TXJRb-)]
    * **LOCATE**: "LOCATE: Localize and Transfer Object Parts for Weakly Supervised Affordance Grounding", CVPR, 2023 (*University of Edinburgh, UK*). [[Paper](https://arxiv.org/abs/2303.09665)][[PyTorch](https://github.com/Reagan1311/LOCATE)][[Website](https://reagan1311.github.io/locate/)]
    * **Afformer**: "Affordance Grounding from Demonstration Video to Target Image", CVPR, 2023 (*NUS*). [[Paper](https://arxiv.org/abs/2303.14644)][[PyTorch](https://github.com/showlab/afformer)]
    * **MV-MWM**: "Multi-View Masked World Models for Visual Robotic Manipulation", ICML, 2023 (*KAIST*). [[Paper](https://arxiv.org/abs/2302.02408)][[Tensorflow2](https://github.com/younggyoseo/MV-MWM)][[Website](https://sites.google.com/view/mv-mwm)]
    * **MTM**: "Masked Trajectory Models for Prediction, Representation, and Control", ICML, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2305.02968)][[PyTorch](https://github.com/facebookresearch/mtm)][[Website](https://wuphilipp.github.io/mtm/)]
    * **Skill-Transformer**: "Skill Transformer: A Monolithic Policy for Mobile Manipulation", ICCV, 2023 (*Georgia Tech*). [[Paper](https://arxiv.org/abs/2308.09873)]
    * **RUPs**: "Nonrigid Object Contact Estimation With Regional Unwrapping Transformer", ICCV, 2023 (*Southeast University, China*). [[Paper](https://arxiv.org/abs/2308.14074)]
    * **IAG**: "Grounding 3D Object Affordance from 2D Interactions in Images", ICCV, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2303.10437)][[Website](https://yyvhang.github.io/publications/IAG/index.html)][[PyTorch](https://github.com/yyvhang/IAGNet)]
    * **RVT**: "RVT: Robotic View Transformer for 3D Object Manipulation", CoRL, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2306.14896)][[PyTorch](https://github.com/nvlabs/rvt)][[Website](https://robotic-view-transformer.github.io/)]
    * **M2T2**: "M2T2: Multi-Task Masked Transformer for Object-centric Pick and Place", CoRL, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2311.00926)][[PyTorch](https://github.com/NVlabs/M2T2)][[Website](https://m2-t2.github.io/)]
* Scene Decomposition:
    * **SRT**: "Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations", CVPR, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2111.13152)][[PyTorch (stelzner)](https://github.com/stelzner/srt)][[Website](https://srt-paper.github.io/)]
    * **OSRT**: "Object Scene Representation Transformer", NeurIPS, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2206.06922)][[Website](https://osrt-paper.github.io/)]
    * **Prompter**: "Prompter: Utilizing Large Language Model Prompting for a Data Efficient Embodied Instruction Following", arXiv, 2022 (*Hitachi*). [[Paper](https://arxiv.org/abs/2211.03267)]
    * **RePAST**: "RePAST: Relative Pose Attention Scene Representation Transformer", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2304.00947)]
    * **GTA**: "GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers", arXiv, 2023 (*University of Tubingen*). [[Paper](https://arxiv.org/abs/2310.10375)]
* Scene Text Recognition:
    * **ViTSTR**: "Vision Transformer for Fast and Efficient Scene Text Recognition", ICDAR, 2021 (*University of the Philippines*). [[Paper](https://arxiv.org/abs/2105.08582)]
    * **STKM**: "Self-attention based Text Knowledge Mining for Text Detection", CVPR, 2021 (*?*). [[Paper]()][[Code (in construction)](https://github.com/CVI-SZU/STKM)]
    * **I2C2W**: "I2C2W: Image-to-Character-to-Word Transformers for Accurate Scene Text Recognition", arXiv, 2021 (*NTU Singapoer*). [[Paper](https://arxiv.org/abs/2105.08383)]
    * **CornerTransformer**: "Toward Understanding WordArt: Corner-Guided Transformer for Scene Text Recognition", ECCV, 2022 (*Huazhong University of Science and Technology*). [[Paper](https://arxiv.org/abs/2208.00438)][[PyTorch](https://github.com/xdxie/WordArt)]
    * **CUTE**: "Contextual Text Block Detection towards Scene Text Understanding", ECCV, 2022 (*NTU Singapore*). [[Paper](https://arxiv.org/abs/2207.12955)][[Website](https://sg-vilab.github.io/publication/xue2022contextual/)]
    * **PARSeq**: "Scene Text Recognition with Permuted Autoregressive Sequence Models", ECCV, 2022 (*University of the Philippines*). [[Paper](https://arxiv.org/abs/2207.06966)][[PyTorch](https://github.com/baudm/parseq)]
    * **PTIE**: "Pure Transformer with Integrated Experts for Scene Text Recognition", ECCV, 2022 (*NTU Singapore*). [[Paper](https://arxiv.org/abs/2211.04963)]
    * **MGP-STR**: "Multi-Granularity Prediction for Scene Text Recognition", ECCV, 2022 (*Alibaba*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/3821_ECCV_2022_paper.php)]
    * **VLAMD**: "Vision-Language Adaptive Mutual Decoder for OOV-STR", ECCVW, 2022 (*iFLYTEK, China*). [[Paper](https://arxiv.org/abs/2209.00859)]
    * **MVLT**: "Masked Vision-Language Transformers for Scene Text Recognition", BMVC, 2022 (*Westone Information Industry Inc., China*). [[Paper](https://arxiv.org/abs/2211.04785)][[PyTorch](https://github.com/onealwj/MVLT)]
* Sign Language:
    * **LWTA**: "Stochastic Transformer Networks with Linear Competing Units: Application to end-to-end SL Translation", ICCV, 2021 (*Cyprus University of Technology*). [[Paper](https://arxiv.org/abs/2109.13318)]
    * **CiCo**: "CiCo: Domain-Aware Sign Language Retrieval via Cross-Lingual Contrastive Learning", CVPR, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2303.12793)][[Code (in construction)](https://github.com/FangyunWei/SLRT)]
    * **GFSLT-VLP**: "Gloss-free Sign Language Translation: Improving from Visual-Language Pretraining", ICCV, 2023 (*Macau University of Science and Technology (MUST)*). [[Paper](https://arxiv.org/abs/2307.14768)][[Code (in construction)](https://github.com/zhoubenjia/GFSLT-VLP)]
    * **IP-SLT**: "Sign Language Translation with Iterative Prototype", ICCV, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2308.12191)]
    * **SignBERT+**: "SignBERT+: Hand-model-aware Self-supervised Pre-training for Sign Language Understanding", TPAMI, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2305.04868)][[Website](https://signbert-zoo.github.io/)]
* Spike:
    * **Spikformer**: "Spikformer: When Spiking Neural Network Meets Transformer", arXiv, 2022 (*Peking*). [[Paper](https://arxiv.org/abs/2209.15425)]
    * **SDSA**: "Spike-driven Transformer", NeurIPS, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2307.01694)][[PyTorch](https://github.com/BICLab/Spike-Driven-Transformer)]
* Stereo:
    * **STTR**: "Revisiting Stereo Depth Estimation From a Sequence-to-Sequence Perspective with Transformers", ICCV, 2021 (*Johns Hopkins*). [[Paper](https://arxiv.org/abs/2011.02910)][[PyTorch](https://github.com/mli0603/stereo-transformer)]
    * **PS-Transformer**: "PS-Transformer: Learning Sparse Photometric Stereo Network using Self-Attention Mechanism", BMVC, 2021 (*National Institute of Informatics, JAPAN*). [[Paper](https://arxiv.org/abs/2211.11386)][[PyTorch](https://github.com/satoshi-ikehata/PS-Transformer-BMVC2021)]
    * **ChiTransformer**: "ChiTransformer: Towards Reliable Stereo from Cues", CVPR, 2022 (*GSU*). [[Paper](https://arxiv.org/abs/2203.04554)]
    * **TransMVSNet**: "TransMVSNet: Global Context-aware Multi-view Stereo Network with Transformers", CVPR, 2022 (*Megvii*). [[Paper](https://arxiv.org/abs/2111.14600)][[Code (in construction)](https://github.com/MegviiRobot/TransMVSNet)]
    * **MVSTER**: "MVSTER: Epipolar Transformer for Efficient Multi-View Stereo", ECCV, 2022 (*CAS*). [[Paper](https://arxiv.org/abs/2204.07346)][[PyTorch](https://github.com/JeffWang987/MVSTER)]
    * **CEST**: "Context-Enhanced Stereo Transformer", ECCV, 2022 (*CAS*). [[Paper](Context-Enhanced Stereo Transformer)][[PyTorch](https://github.com/guoweiyu/Context-Enhanced-Stereo-Transformer)]
    * **WT-MVSNet**: "WT-MVSNet: Window-based Transformers for Multi-view Stereo", NeurIPS, 2022 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2205.14319)]
    * **MVSFormer**: "MVSFormer: Learning Robust Image Representations via Transformers and Temperature-based Depth for Multi-View Stereo", arXiv, 2022 (*Fudan University*). [[Paper](https://arxiv.org/abs/2208.02541)]
    * **MVSFormer++**: "MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo", ICLR, 2024 (*Fudan*). [[Paper](https://arxiv.org/abs/2401.11673)][[Code (in construction)](https://github.com/maybeLx/MVSFormerPlusPlus)]
* Tactile:
    * **UniTouch**: "Binding Touch to Everything: Learning Unified Multimodal Tactile Representations", arXiv, 2024 (*Yale*). [[Paper](https://arxiv.org/abs/2401.18084)][[Code (in construction)](https://github.com/cfeng16/UniTouch)][[Website](https://cfeng16.github.io/UniTouch/)]
* Time Series:
    * **MissFormer**: "MissFormer: (In-)attention-based handling of missing observations for trajectory filtering and prediction", arXiv, 2021 (*Fraunhofer IOSB, Germany*). [[Paper](https://arxiv.org/abs/2106.16009)]
* Traffic:
    * **NEAT**: "NEAT: Neural Attention Fields for End-to-End Autonomous Driving", ICCV, 2021 (*MPI*). [[Paper](https://arxiv.org/abs/2109.04456)][[PyTorch](https://github.com/autonomousvision/neat)]
    * **ViTAL**: "Novelty Detection and Analysis of Traffic Scenario Infrastructures in the Latent Space of a Vision Transformer-Based Triplet Autoencoder", IV, 2021 (*Technische Hochschule Ingolstadt*). [[Paper](https://arxiv.org/abs/2105.01924)]
    * **?**: "Predicting Vehicles Trajectories in Urban Scenarios with Transformer Networks and Augmented Information", IVS, 2021 (*Universidad de Alcala*). [[Paper](https://arxiv.org/abs/2106.00559)]
    * **?**: "Translating Images into Maps", ICRA, 2022 (*University of Surrey, UK*). [[Paper](https://arxiv.org/abs/2110.00966)][[PyTorch (in construction)](https://github.com/avishkarsaha/translating-images-into-maps)]
    * **Crossview-Transformer**: "Cross-view Transformers for real-time Map-view Semantic Segmentation", CVPR, 2022 (*UT Austin*). [[Paper](https://arxiv.org/abs/2205.02833)][[PyTorch](https://github.com/bradyz/cross_view_transformers)]
    * **MSF3DDETR**: "MSF3DDETR: Multi-Sensor Fusion 3D Detection Transformer for Autonomous Driving", ICPRW, 2022 (*University of Coimbra, Portugal*). [[Paper](https://arxiv.org/abs/2210.15316)]
    * **TransLPC**: "Transformers for Object Detection in Large Point Clouds", ITSC, 2022 (*Bosch*). [[Paper](https://arxiv.org/abs/2209.15258)]
    * **PicT**: "PicT: A Slim Weakly Supervised Vision Transformer for Pavement Distress Classification", ACMMM, 2022 (*Chongqing University*). [[Paper](https://arxiv.org/abs/2209.10074)][[PyTorch (in construction)](https://github.com/DearCaat/PicT)]
    * **JPerceiver**: "JPerceiver: Joint Perception Network for Depth, Pose and Layout Estimation in Driving Scenes", ECCV, 2022 (*The University of Sydney*). [[Paper](https://arxiv.org/abs/2207.07895)][[PyTorch](https://github.com/sunnyHelen/JPerceiver)]
    * **V2X-ViT**: "V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer", ECCV, 2022 (*UCLA*). [[Paper](https://arxiv.org/abs/2203.10638)]
    * **?**: "Can Transformer Attention Spread Give Insights Into Uncertainty of Detected and Tracked Objects?", IROSW, 2022 (*Bosch*). [[Paper](https://arxiv.org/abs/2210.14391)]
    * **MTR**: "Motion Transformer with Global Intention Localization and Local Movement Refinement", NeurIPS, 2022 (*MPI*). [[Paper](https://arxiv.org/abs/2209.13508)][[Code (in construction)](https://github.com/sshaoshuai/MTR)]
    * **PlanT**: "PlanT: Explainable Planning Transformers via Object-Level Representations", Conference on Robot Learning (CoRL), 2022 (*TUM*). [[Paper](https://arxiv.org/abs/2210.14222)][[PyTorch](https://github.com/autonomousvision/plant)][[Website](https://www.katrinrenz.de/plant/)]
    * **ParkPredict+**: "ParkPredict+: Multimodal Intent and Motion Prediction for Vehicles in Parking Lots with CNN and Transformer", arXiv, 2022 (*Berkeley*). [[Paper](https://arxiv.org/abs/2204.10777)]
    * **?**: "Pyramid Transformer for Traffic Sign Detection", arXiv, 2022 (*Iran University of Science and Technology*). [[Paper](https://arxiv.org/abs/2207.06067)]
    * **STrajNet**: "STrajNet: Occupancy Flow Prediction via Multi-modal Swin Transformer", arXiv, 2022 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2208.00394)]
    * **MTPP**: "Multi-modal Transformer Path Prediction for Autonomous Vehicle", arXiv, 2022 (*National Central University*). [[Paper](https://arxiv.org/abs/2208.07256)]
    * **DCT**: "A Dual-Cycled Cross-View Transformer Network for Unified Road Layout Estimation and 3D Object Detection in the Bird's-Eye-View", arXiv, 2022 (*Gwang-ju Institute of Science and Technology*). [[Paper](https://arxiv.org/abs/2209.08844)]
    * **C-ViT**: "Traffic Accident Risk Forecasting using Contextual Vision Transformers", arXiv, 2022 (*University of Technology Sydney*). [[Paper](https://arxiv.org/abs/2209.11180)]
    * **MapTR**: "MapTR: Structured Modeling and Learning for Online Vectorized HD Map Construction", ICLR, 2023 (*Horizon Robotics*). [[Paper](https://arxiv.org/abs/2208.14437)][[PyTorch](https://github.com/hustvl/MapTR)]
    * **VE-Prompt**: "Visual Exemplar Driven Task-Prompting for Unified Perception in Autonomous Driving", CVPR, 2023 (*Sun Yat-sen University*). [[Paper](https://arxiv.org/abs/2303.01788)]
    * **TPVFormer**: "Tri-Perspective View for Vision-Based 3D Semantic Occupancy Prediction", CVPR, 2023 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2302.07817)][[PyTorch](https://github.com/wzzheng/TPVFormer)]
    * **TBP-Former**: "TBP-Former: Learning Temporal Bird's-Eye-View Pyramid for Joint Perception and Prediction in Vision-Centric Autonomous Driving", CVPR, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2303.09998)][[Code (in construction)](https://github.com/MediaBrain-SJTU/TBP-Former)]
    * **BAEFormer**: "BAEFormer: Bi-directional and Early Interaction Transformers for Bird’s Eye View Semantic Segmentation", CVPR, 2023 (*Horizon Robotics*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Pan_BAEFormer_Bi-Directional_and_Early_Interaction_Transformers_for_Birds_Eye_View_CVPR_2023_paper.html)]
    * **BAAM**: "BAAM: Monocular 3D Pose and Shape Reconstruction With Bi-Contextual Attention Module and Attention-Guided Modeling", CVPR, 2023 (*Chungnam National University, Korea*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Lee_BAAM_Monocular_3D_Pose_and_Shape_Reconstruction_With_Bi-Contextual_Attention_CVPR_2023_paper.html)][[PyTorch](https://github.com/gywns6287/BAAM)]
    * **Pix2Map**: "Pix2Map: Cross-modal Retrieval for Inferring Street Maps from Images", CVPR, 2023 (*CMU*). [[Paper](https://arxiv.org/abs/2301.04224)][[Website](https://pix2map.github.io/)]
    * **UniAD**: "Planning-oriented Autonomous Driving", CVPR, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2212.10156)][[PyTorch](https://github.com/OpenDriveLab/UniAD)][[Website](https://opendrivelab.github.io/UniAD/)]
    * **Multiverse-Transformer**: "Multiverse Transformer: 1st Place Solution for Waymo Open Sim Agents Challenge 2023", CVPRW, 2023 (*Pegasus*). [[Paper](https://arxiv.org/abs/2306.11868)][[Website](https://multiverse-transformer.github.io/sim-agents/)]
    * **UniFormer**: "UniFormer: Unified Multi-view Fusion Transformer for Spatial-Temporal Representation in Bird's-Eye-View", ICCV, 2023 (*Zhejiang University*). [[Paper](https://arxiv.org/abs/2207.08536)]
    * **SegMiF**: "Multi-interactive Feature Learning and a Full-time Multi-modality Benchmark for Image Fusion and Segmentation", ICCV, 2023 (*Dalian University of Technology*). [[Paper](https://arxiv.org/abs/2308.02097)][[Code (in construction)](https://github.com/JinyuanLiu-CV/SegMiF)]
    * **VTD**: "Video Task Decathlon: Unifying Image and Video Tasks in Autonomous Driving", ICCV, 2023 (*ETHZ*). [[Paper](https://arxiv.org/abs/2309.04422)]
    * **HM-ViT**: "HM-ViT: Hetero-modal Vehicle-to-Vehicle Cooperative perception with vision transformer", ICCV, 2023 (*UCLA*). [[Paper](https://arxiv.org/abs/2304.10628)][[Code (in construction)](https://github.com/XHwind/HM-ViT)]
    * **UP-VL**: "Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous Driving", ICCV, 2023 (*Waymo*). [[Paper](https://arxiv.org/abs/2309.14491)]
    * **GameFormer**: "GameFormer: Game-theoretic Modeling and Learning of Transformer-based Interactive Prediction and Planning for Autonomous Driving", ICCV, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2303.05760)][[PyTorch](https://github.com/MCZhi/GameFormer)][[Website](https://mczhi.github.io/GameFormer/)]
    * **GeoMIM**: "GeoMIM: Towards Better 3D Knowledge Transfer via Masked Image Modeling for Multi-view 3D Understanding", ICCV, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2303.11325)][[PyTorch](https://github.com/Sense-X/GeoMIM)]
    * **LiDARFormer**: "LiDARFormer: A Unified Transformer-based Multi-task Network for LiDAR Perception", arXiv, 2023 (*TuSimple*). [[Paper](https://arxiv.org/abs/2303.12194)]
    * **VoxelFormer**: "VoxelFormer: Bird's-Eye-View Feature Generation based on Dual-view Attention for Multi-view 3D Object Detection", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2304.01054)][[PyTorch](https://github.com/Lizhuoling/VoxelFormer-public)]
    * **LCTGen**: "Language Conditioned Traffic Generation", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2307.07947)][[Website](https://ariostgx.github.io/lctgen/)]
    * **UniWorld**: "UniWorld: Autonomous Driving Pre-training via World Models", arXiv, 2023 (*Peking*). [[Paper](https://arxiv.org/abs/2308.07234)][[Code (in construction)](https://github.com/chaytonmin/UniWorld)]
    * **PromptTrack**: "Language Prompt for Autonomous Driving", arXiv, 2023 (*Megvii*). [[Paper](https://arxiv.org/abs/2309.04379)]
    * **HiLM-D**: "HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving", arXiv, 2023 (*Huawei*). [[Paper](https://arxiv.org/abs/2309.05186)]
    * **DiffPrompter**: "DiffPrompter: Differentiable Implicit Visual Prompts for Semantic-Segmentation in Adverse Conditions", arXiv, 2023 (*IIIT Hyderabad*). [[Paper](https://arxiv.org/abs/2310.04181)][[PyTorch](https://github.com/DiffPrompter/diff-prompter)][[Website](https://diffprompter.github.io/)]
    * **OccWorld**: "OccWorld: Learning a 3D Occupancy World Model for Autonomous Driving", arXiv, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2311.16038)][[PyTorch](https://github.com/wzzheng/OccWorld)][[Website](https://wzzheng.net/OccWorld/)]
    * **VehicleMAE**: "Structural Information Guided Multimodal Pre-training for Vehicle-centric Perception", AAAI, 2024 (*Anhui University*). [[Paper](https://arxiv.org/abs/2312.09812)]
* Traffic (LLM-based):
    * **AVIS**: "AVIS: Autonomous Visual Information Seeking with Large Language Models", NeurIPS, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2306.08129)]
    * **DriveGPT4**: "DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model", arXiv, 2023 (*HKU*). [[Paper](https://arxiv.org/abs/2310.01412)][[Website](https://tonyxuqaq.github.io/projects/DriveGPT4/)]
    * **?**: "Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving", arXiv, 2023 (*Wayve*). [[Paper](https://arxiv.org/abs/2310.01957)][[PyTorch](https://github.com/wayveai/Driving-with-LLMs)]
    * **GPT4V-AD**: "On the Road with GPT-4V(ision): Early Explorations of Visual-Language Model on Autonomous Driving", arXiv, 2023 (*Shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2311.05332)][[Code (in construction)](https://github.com/PJLab-ADG/GPT4V-AD-Exploration)]
    * **Agent-Driver**: "A Language Agent for Autonomous Driving", arXiv, 2023 (*USC*). [[Paper](https://arxiv.org/abs/2311.10813)][[Code (in construction)](https://github.com/USC-GVL/Agent-Driver)][[Website](https://usc-gvl.github.io/Agent-Driver/)]
    * **ADriver-I**: "ADriver-I: A General World Model for Autonomous Driving", arXiv, 2023 (*Megvii*). [[Paper](https://arxiv.org/abs/2311.13549)]
    * **Dolphins**: "Dolphins: Multimodal Language Model for Driving", arXiv, 2023 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2312.00438)][[Code (in construction)](https://github.com/vlm-driver/Dolphins)][[Website](https://vlm-driver.github.io/)]
    * **LMDrive**: "LMDrive: Closed-Loop End-to-End Driving with Large Language Models", arXiv, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2312.07488)]
    * **DriveMLM**: "DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving", arXiv, 2023 (*shanghai AI Lab*). [[Paper](https://arxiv.org/abs/2312.09245)][[Code (in construction)](https://github.com/OpenGVLab/DriveMLM)]
    * **DriveLM**: "DriveLM: Driving with Graph Visual Question Answering", arXiv, 2023 (*OpenDriveLab, China*). [[Paper](https://arxiv.org/abs/2312.14150)][[Code](https://github.com/OpenDriveLab/DriveLM)]
    * **VLP**: "VLP: Vision Language Planning for Autonomous Driving", arXiv, 2024 (*Bosch*). [[Paper](https://arxiv.org/abs/2401.05577)]
* Trajectory Prediction:
    * **mmTransformer**: "Multimodal Motion Prediction with Stacked Transformers", CVPR, 2021 (*CUHK + SenseTime*). [[Paper](https://arxiv.org/abs/2103.11624)][[Code (in construction)](https://github.com/decisionforce/mmTransformer)][[Website](https://decisionforce.github.io/mmTransformer/)]
    * **AgentFormer**: "AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting", ICCV, 2021 (*CMU*). [[Paper](https://arxiv.org/abs/2103.14023)][[PyTorch](https://github.com/Khrylx/AgentFormer)][[Website](https://www.ye-yuan.com/agentformer/)]
    * **S2TNet**: "S2TNet: Spatio-Temporal Transformer Networks for Trajectory Prediction in Autonomous Driving", ACML, 2021 (*Xi'an Jiaotong University*). [[Paper](https://arxiv.org/abs/2206.10902)][[PyTorch](https://github.com/chenghuang66/s2tnet)]
    * **MRT**: "Multi-Person 3D Motion Prediction with Multi-Range Transformers", NeurIPS, 2021 (*UCSD + Berkeley*). [[Paper](https://arxiv.org/abs/2111.12073)][[PyTorch](https://github.com/jiashunwang/MRT)][[Website](https://jiashunwang.github.io/MRT/)]
    * **?**: "Latent Variable Sequential Set Transformers for Joint Multi-Agent Motion Prediction", ICLR, 2022 (*MILA*). [[Paper](https://openreview.net/forum?id=Dup_dDqkZC5)]
    * **Scene-Transformer**: "Scene Transformer: A unified architecture for predicting multiple agent trajectories", ICLR, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2106.08417)]
    * **ST-MR**: "Graph-based Spatial Transformer with Memory Replay for Multi-Future Pedestrian Trajectory Prediction", CVPR, 2022 (*University of New South Wales, Australia*). [[Paper](https://arxiv.org/abs/2206.05712)][[Tensorflow](https://github.com/Jacobieee/ST-MR)]
    * **HiVT**: "HiVT: Hierarchical Vector Transformer for Multi-Agent Motion Prediction", CVPR, 2022 (*CUHK*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Zhou_HiVT_Hierarchical_Vector_Transformer_for_Multi-Agent_Motion_Prediction_CVPR_2022_paper.html)]
    * **EF-Transformer**: "Entry-Flipped Transformer for Inference and Prediction of Participant Behavior", ECCV, 2022 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2207.06235)]
    * **Social-SSL**: "Social-SSL: Self-Supervised Cross-Sequence Representation Learning Based on Transformers for Multi-Agent Trajectory Prediction", ECCV, 2022 (*NYCU*). [[Paper](https://basiclab.lab.nycu.edu.tw/assets/Social-SSL.pdf)][[PyTorch](https://github.com/Sigta678/Social-SSL)]
    * **LatentFormer**: "LatentFormer: Multi-Agent Transformer-Based Interaction Modeling and Trajectory Prediction", arXiv, 2022 (*Huawei*). [[Paper](https://arxiv.org/abs/2203.01880)]
    * **PreTR**: "PreTR: Spatio-Temporal Non-Autoregressive Trajectory Prediction Transformer", arXiv, 2022 (*Stellantis, France*). [[Paper](https://arxiv.org/abs/2203.09293)]
    * **Wayformer**: "Wayformer: Motion Forecasting via Simple & Efficient Attention Networks", arXiv, 2022 (*Waymo*). [[Paper](https://arxiv.org/abs/2207.05844)]
    * **LaTTe**: "LaTTe: Language Trajectory TransformEr", arXiv, 2022 (*TUM*). [[Paper](https://arxiv.org/abs/2208.02918)][[Tensorflow](https://github.com/arthurfenderbucker/NL_trajectory_reshaper)]
    * **SoMoFormer**: "SoMoFormer: Social-Aware Motion Transformer for Multi-Person Motion Prediction", arXiv, 2022 (*Hangzhou Dianzi University*). [[Paper](https://arxiv.org/abs/2208.09224)]
    * **ViewBirdiformer**: "ViewBirdiformer: Learning to recover ground-plane crowd trajectories and ego-motion from a single ego-centric view", arXiv, 2022 (*Kyoto University*). [[Paper](https://arxiv.org/abs/2210.06332)]
    * **PedFormer**: "PedFormer: Pedestrian Behavior Prediction via Cross-Modal Attention Modulation and Gated Multitask Learning", arXiv, 2022 (*Huawei*). [[Paper](https://arxiv.org/abs/2210.07886)]
    * **TAMFormer**: "TAMFormer: Multi-Modal Transformer with Learned Attention Mask for Early Intent Prediction", arXiv, 2022 (*University of Padova, Italy*). [[Paper](https://arxiv.org/abs/2210.14714)]
    * **QCNet**: "Query-Centric Trajectory Prediction", CVPR, 2023 (*CUHK*). [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zhou_Query-Centric_Trajectory_Prediction_CVPR_2023_paper.html)][[Code (in construction)](https://github.com/ZikangZhou/QCNet)]
    * **ViP3D**: "ViP3D: End-to-end Visual Trajectory Prediction via 3D Agent Queries", CVPR, 2023 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2208.01582)][[PyTorch](https://github.com/Tsinghua-MARS-Lab/ViP3D)][[Website](https://tsinghua-mars-lab.github.io/ViP3D/)]
    * **USST**: "Uncertainty-aware State Space Transformer for Egocentric 3D Hand Trajectory Forecasting", ICCV, 2023 (*OPPO*). [[Paper](https://arxiv.org/abs/2307.08243)][[PyTorch](https://github.com/oppo-us-research/USST)][[Website](https://actionlab-cv.github.io/EgoHandTrajPred/)]
    * **JRTransformer**: "Joint-Relation Transformer for Multi-Person Motion Prediction", ICCV, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2308.04808)][[PyTorch](https://github.com/MediaBrain-SJTU/JRTransformer)]
    * **Forecast-MAE**: "Forecast-MAE: Self-supervised Pre-training for Motion Forecasting with Masked Autoencoders", ICCV, 2023 (*HKUST*). [[Paper](https://arxiv.org/abs/2308.09882)][[PyTorch](https://github.com/jchengai/forecast-mae)]
    * **MotionLM**: "MotionLM: Multi-Agent Motion Forecasting as Language Modeling", ICCV, 2023 (*Waymo*). [[Paper](https://arxiv.org/abs/2309.16534)]
    * **OccFormer**: "OccFormer: Dual-path Transformer for Vision-based 3D Semantic Occupancy Prediction", ICCV, 2023 (*PhiGent Robotics, China*). [[Paper](https://arxiv.org/abs/2304.05316)][[PyTorch](https://github.com/zhangyp15/OccFormer)]
    * **Traj-MAE**: "Traj-MAE: Masked Autoencoders for Trajectory Prediction", ICCV, 2023 (*CUHK*). [[Paper](https://arxiv.org/abs/2303.06697)]
    * **R-Pred**: "R-Pred: Two-Stage Motion Prediction Via Tube-Query Attention-Based Trajectory Refinement", ICCV, 2023 (*Hanyang University, Korea*). [[Paper](https://arxiv.org/abs/2211.08609)]
    * **MacFormer**: "MacFormer: Map-Agent Coupled Transformer for Real-time and Robust Trajectory Prediction", RAL, 2023 (*HKUST*). [[Paper](https://arxiv.org/abs/2308.10280)]
    * **HPTR**: "Real-Time Motion Prediction via Heterogeneous Polyline Transformer with Relative Pose Encoding", NeurIPS, 2023 (*ETHZ*). [[Paper](https://arxiv.org/abs/2310.12970)][[PyTorch](https://github.com/zhejz/HPTR)]
    * **InCrowdFormer**: "InCrowdFormer: On-Ground Pedestrian World Model From Egocentric Views", arXiv, 2023 (*Kyoto University*). [[Paper](https://arxiv.org/abs/2303.09534)]
    * **MTR++**: "MTR++: Multi-Agent Motion Prediction with Symmetric Scene Modeling and Guided Intention Querying", arXiv, 2023 (*MPI*). [[Paper](https://arxiv.org/abs/2306.17770)]
* Visual Counting:
    * **CC-AV**: "Audio-Visual Transformer Based Crowd Counting", ICCVW, 2021 (*University of Kansas*). [[Paper](https://arxiv.org/abs/2109.01926)]
    * **TransCrowd**: "TransCrowd: Weakly-Supervised Crowd Counting with Transformer", arXiv, 2021 (*Huazhong University of Science and Technology*). [[Paper](https://arxiv.org/abs/2104.09116)][[PyTorch](https://github.com/dk-liang/TransCrowd)]
    * **TAM-RTM**: "Boosting Crowd Counting with Transformers", arXiv, 2021 (*ETHZ*). [[Paper](https://arxiv.org/abs/2105.10926)]
    * **CCTrans**: "CCTrans: Simplifying and Improving Crowd Counting with Transformer", arXiv, 2021 (*Meituan*). [[Paper](https://arxiv.org/abs/2109.14483)]
    * **MAN**: "Boosting Crowd Counting via Multifaceted Attention", CVPR, 2022 (*Xi'an Jiaotong*). [[Paper](https://arxiv.org/abs/2203.02636)][[PyTorch](https://github.com/LoraLinH/Boosting-Crowd-Counting-via-Multifaceted-Attention)]
    * **CLTR**: "An End-to-End Transformer Model for Crowd Localization", ECCV, 2022 (*Huazhong University of Science and Technology*). [[Paper](https://arxiv.org/abs/2202.13065)][[PyTorch](https://github.com/dk-liang/CLTR)][[Website](https://dk-liang.github.io/CLTR/)]
    * **SAANet**: "Scene-Adaptive Attention Network for Crowd Counting", arXiv, 2022 (*Xi'an Jiaotong*). [[Paper](https://arxiv.org/abs/2112.15509)]
    * **JCTNet**: "Joint CNN and Transformer Network via weakly supervised Learning for efficient crowd counting", arXiv, 2022 (*Chongqing University*). [[Paper](https://arxiv.org/abs/2203.06388)]
    * **CrowdMLP**: "CrowdMLP: Weakly-Supervised Crowd Counting via Multi-Granularity MLP", arXiv, 2022 (*University of Guelph, Canada*). [[Paper](https://arxiv.org/abs/2203.08219)]
    * **CounTR**: "CounTR: Transformer-based Generalised Visual Counting", arXiv, 2022 (*Shanghai Jiao Tong University*). [[Paper](https://arxiv.org/abs/2208.13721)][[Website](https://verg-avesta.github.io/CounTR_Webpage/)]
    * **CrowdCLIP**: "CrowdCLIP: Unsupervised Crowd Counting via Vision-Language Model", CVPR, 2023 (*Baidu*). [[Paper](https://arxiv.org/abs/2304.04231)][[Code (in construction)](https://github.com/dk-liang/CrowdCLIP)]
    * **PET**: "Point-Query Quadtree for Crowd Counting, Localization, and More", ICCV, 2023 (*Huazhong University of Science and Technology*). [[Paper](https://arxiv.org/abs/2308.13814)][[PyTorch](https://github.com/cxliu0/PET)]
    * **CLIP-Count**: "CLIP-Count: Towards Text-Guided Zero-Shot Object Counting", arXiv, 2023 (*The Hong Kong Polytechnic University*). [[Paper](https://arxiv.org/abs/2305.07304)][[Code (in construction)](https://github.com/songrise/CLIP-Count)]
    * **?**: "Training-free Object Counting with Prompts", arXiv, 2023 (*A\⋆STAR*). [[Paper](https://arxiv.org/abs/2307.00038)][[PyTorch](https://github.com/shizenglin/training-free-object-counter)]
    * **T-Rex**: "T-Rex: Counting by Visual Prompting", arXiv, 2023 (*IDEA*). [[Paper](https://arxiv.org/abs/2311.13596)][[Website](https://trex-counting.github.io/)]
    * **VLCounter**: "VLCounter: Text-aware VIsual Representation for Zero-Shot Object Counting", AAAI, 2024 (*Sungkyunkwan University, Korea*). [[Paper](https://arxiv.org/abs/2312.16580)][[PyTorch](https://github.com/Seunggu0305/VLCounter)]
    * **Gramformer**: "Gramformer: Learning Crowd Counting via Graph-Modulated Transformer", AAAI, 2024 (*Xi'an Jiaotong*). [[Paper](https://arxiv.org/abs/2401.03870)][[Code (in construction)](https://github.com/LoraLinH/Gramformer)]
* Visual Quality Assessment:
    * **TRIQ**: "Transformer for Image Quality Assessment", arXiv, 2020 (*NORCE*). [[Paper](https://arxiv.org/abs/2101.01097)][[Tensorflow-Keras](https://github.com/junyongyou/triq)]
    * **IQT**: "Perceptual Image Quality Assessment with Transformers", CVPRW, 2021 (*LG*). [[Paper](https://arxiv.org/abs/2104.14730)][[Code (in construction)](https://github.com/manricheon/IQT)]
    * **MUSIQ**: "MUSIQ: Multi-scale Image Quality Transformer", ICCV, 2021 (*Google*). [[Paper](https://arxiv.org/abs/2108.05997)]
    * **TranSLA**: "Saliency-Guided Transformer Network Combined With Local Embedding for No-Reference Image Quality Assessment", ICCVW, 2021 (*Hikvision*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Zhu_Saliency-Guided_Transformer_Network_Combined_With_Local_Embedding_for_No-Reference_Image_ICCVW_2021_paper.html)]
    * **TReS**: "No-Reference Image Quality Assessment via Transformers, Relative Ranking, and Self-Consistency", WACV, 2022 (*CMU*). [[Paper](https://arxiv.org/abs/2108.06858)]
    * **IQA-Conformer**: "Conformer and Blind Noisy Students for Improved Image Quality Assessment", CVPRW, 2022 (*University of Wurzburg, Germany*). [[Paper](https://arxiv.org/abs/2204.12819)][[PyTorch](https://github.com/burchim/IQA-Conformer-BNS)]
    * **SwinIQA**: "SwinIQA: Learned Swin Distance for Compressed Image Quality Assessment", CVPRW, 2022 (*USTC, China*). [[Paper](https://arxiv.org/abs/2205.04264)]
    * **DCVQE**: "DCVQE: A Hierarchical Transformer for Video Quality Assessment", ACCV, 2022 (*Weibo*). [[Paper](https://arxiv.org/abs/2210.04377)]
    * **MCAS-IQA**: "Visual Mechanisms Inspired Efficient Transformers for Image and Video Quality Assessment", arXiv, 2022 (*Norwegian Research Centre, Norway*). [[Paper](https://arxiv.org/abs/2203.14557)]
    * **MSTRIQ**: "MSTRIQ: No Reference Image Quality Assessment Based on Swin Transformer with Multi-Stage Fusion", arXiv, 2022 (*ByteDance*). [[Paper](https://arxiv.org/abs/2205.10101)]
    * **DisCoVQA**: "DisCoVQA: Temporal Distortion-Content Transformers for Video Quality Assessment", arXiv, 2022 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2206.09853)]
    * **LIQE**: "Blind Image Quality Assessment via Vision-Language Correspondence: A Multitask Learning Perspective", CVPR, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2303.14968)][[PyTorch](https://github.com/zwx8981/LIQE)]
    * **MRET**: "MRET: Multi-resolution Transformer for Video Quality Assessment", arXiv, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2303.07489)]
    * **SAM-IQA**: "SAM-IQA: Can Segment Anything Boost Image Quality Assessment?", arXiv, 2023 (*Megvii*). [[Paper](https://arxiv.org/abs/2307.04455)][[Code (in construction)](https://github.com/Hedlen/SAM-IQA)]
    * **LoDa**: "Local Distortion Aware Efficient Transformer Adaptation for Image Quality Assessment", arXiv, 2023 (*Wuhan University*). [[Paper](https://arxiv.org/abs/2308.12001)]
    * **Q-Align**: "Q-Align: Teaching LMMs for Visual Scoring via Discrete Text-Defined Levels", arXiv, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2312.17090)][[PyTorch](https://github.com/Q-Future/Q-Align)][[Website](https://q-align.github.io/)]
    * **SAMA**: "Scaling and Masking: A New Paradigm of Data Sampling for Image and Video Quality Assessment", AAAI, 2024 (*Xidian University*). [[Paper](https://arxiv.org/abs/2401.02614)][[PyTorch](https://github.com/Sissuire/SAMA)]
* Visual Reasoning:
    * **SAViR-T**: "SAViR-T: Spatially Attentive Visual Reasoning with Transformers", arXiv, 2022 (*Rutgers University*). [[Paper](https://arxiv.org/abs/2206.09265)]
* Wide-angle lenses:
    * **DarSwin**: "DarSwin: Distortion Aware Radial Swin Transformer", ICCV, 2023 (*Laval University, Canada*). [[Paper](https://arxiv.org/abs/2304.09691)][[PyTorch](https://github.com/thalesgroup/darswin)][[Website](https://lvsn.github.io/darswin/)]
* 3D Human Texture Estimation:
    * **Texformer**: "3D Human Texture Estimation from a Single Image with Transformers", ICCV, 2021 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2109.02563)][[PyTorch](https://github.com/xuxy09/Texformer)][[Website](https://www.mmlab-ntu.com/project/texformer/)]
* 3D Motion Synthesis:
    * **ACTOR**: "Action-Conditioned 3D Human Motion Synthesis with Transformer VAE", ICCV, 2021 (*Univ Gustave Eiffel*). [[Paper](https://arxiv.org/abs/2104.05670)][[PyTorch](https://github.com/Mathux/ACTOR)][[Website](https://imagine.enpc.fr/~petrovim/actor/)]
    * **RTVAE**: "Recurrent Transformer Variational Autoencoders for Multi-Action Motion Synthesis", CVPRW, 2022 (*Amazon*). [[Paper](https://arxiv.org/abs/2206.06741)]
    * **MotionCLIP**: "MotionCLIP: Exposing Human Motion Generation to CLIP Space", ECCV, 2022 (*Tel Aviv*). [[Paper](https://arxiv.org/abs/2203.08063)]
    * **CLIP-Actor**: "CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes", ECCV, 2022 (*POSTECH*). [[Paper](https://arxiv.org/abs/2206.04382)][[PyTorch](https://github.com/postech-ami/CLIP-Actor)][[Website](https://clip-actor.github.io/)]
    * **PoseGPT**: "PoseGPT: Quantization-based 3D Human Motion Generation and Forecasting", ECCV, 2022 (*NAVER*). [[Paper](https://arxiv.org/abs/2210.10542)]
    * **TEMOS**: "TEMOS: Generating diverse human motions from textual descriptions", ECCV, 2022 (*MPI*). [[Paper](https://arxiv.org/abs/2204.14109)][[PyTorch](https://github.com/Mathux/TEMOS)][[Website](https://mathis.petrovich.fr/temos/)]
    * **TM2T**: "TM2T: Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts", ECCV, 2022 (*University of Alberta, Canada*). [[Paper](https://arxiv.org/abs/2207.01696)][[PyTorch](https://github.com/EricGuo5513/TM2T)][[Website](https://ericguo5513.github.io/TM2T/)]
    * **HUMANISE**: "HUMANISE: Language-conditioned Human Motion Generation in 3D Scenes", NeurIPS, 2022 (*Beijing Institute of Technology*). [[Paper](https://arxiv.org/abs/2210.09729)][[GitHub](https://github.com/Silverster98/HUMANISE)][[Website](https://silvester.wang/HUMANISE/)]
    * **?**: "Diverse Dance Synthesis via Keyframes with Transformer Controllers", arXiv, 2022 (*Beihang University*). [[Paper](https://arxiv.org/abs/2207.05906)]
    * **MARIONET**: "NEURAL MARIONETTE: A Transformer-based Multi-action Human Motion Synthesis System", arXiv, 2022 (*Wuhan University*). [[Paper](https://arxiv.org/abs/2209.13204)]
    * **Action-GPT**: "Action-GPT: Leveraging Large-scale Language Models for Improved and Generalized Zero Shot Action Generation", arXiv, 2022 (*IIIT Hyderabad*). [[Paper](https://arxiv.org/abs/2211.15603)][[Website](https://actiongpt.github.io/)]
    * **MDM**: "Human Motion Diffusion Model", ICLR, 2023 (*Tel Aviv University*). [[Paper](https://arxiv.org/abs/2209.14916)][[PyTorch](https://github.com/GuyTevet/motion-diffusion-model)][[Website](https://guytevet.github.io/mdm-page/)]
    * **POTTER**: "POTTER: Pooling Attention Transformer for Efficient Human Mesh Recovery", CVPR, 2023 (*OPPO*). [[Paper](https://arxiv.org/abs/2303.13357)][[PyTorch](https://github.com/zczcwh/POTTER)][[Website](https://zczcwh.github.io/potter_page/)]
    * **Optimus**: "Transformer-Based Learned Optimization", CVPR, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2212.01055)]
    * **CITL**: "Continuous Intermediate Token Learning with Implicit Motion Manifold for Keyframe Based Motion Interpolation", CVPR, 2023 (*The University of Sydney*). [[Paper](https://arxiv.org/abs/2303.14926)][[PyTorch](https://github.com/MiniEval/CITL)]
    * **OOHMG**: "Being Comes from Not-being: Open-vocabulary Text-to-Motion Generation with Wordless Training", CVPR, 2023 (*Sun Yat-Sen University*). [[Paper](https://arxiv.org/abs/2210.15929)][[Code (in construction)](https://github.com/junfanlin/oohmg)]
    * **AttT2M**: "AttT2M: Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism", ICCV, 2023 (*CAS*). [[Paper](https://arxiv.org/abs/2309.00796)][[PyTorch](https://github.com/ZcyMonkey/AttT2M)]
    * **ActFormer**: "ActFormer: A GAN Transformer Framework towards General Action-Conditioned 3D Human Motion Generation", ICCV, 2023 (*SenseTime*). [[Paper](https://arxiv.org/abs/2203.07706)]
    * **AvatarJLM**: "Realistic Full-Body Tracking from Sparse Observations via Joint-Level Modeling", ICCV, 2023 (*ByteDance*). [[Paper](https://arxiv.org/abs/2308.08855)][[PyTorch](https://github.com/zxz267/AvatarJLM)][[Website](https://zxz267.github.io/AvatarJLM/)]
    * **Fg-T2M**: "Fg-T2M: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model", ICCV, 2023 (*Beihang*). [[Paper](https://arxiv.org/abs/2309.06284)]
    * **TMR**: "TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis", ICCV, 2023 (*Gustave Eiffel University*). [[Paper](https://arxiv.org/abs/2305.00976)][[PyTorch](https://github.com/Mathux/TMR)][[Website](https://mathis.petrovich.fr/tmr/)]
    * **Make-An-Animation**: "Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation", ICCV, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2305.09662)]
    * **ATOM**: "Language-guided Human Motion Synthesis with Atomic Actions", ACMMM, 2023 (*University at Buffalo*). [[Paper](https://arxiv.org/abs/2308.09611)][[Code (in construction)](https://github.com/yhZhai/ATOM)]
    * **MotionGPT**: "MotionGPT: Human Motion as a Foreign Language", NeurIPS, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2306.14795)][[PyTorch](https://github.com/OpenMotionLab/MotionGPT)][[Website](https://motion-gpt.github.io/)]
    * **FineMoGen**: "FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing", NeurIPS, 2023 (*NTU, Singapore*). [[Paper](https://arxiv.org/abs/2312.15004)][[PyTorch](https://github.com/mingyuan-zhang/FineMoGen)][[Website](https://mingyuan-zhang.github.io/projects/FineMoGen.html)]
    * **DDT**: "DDT: A Diffusion-Driven Transformer-based Framework for Human Mesh Recovery from a Video", arXiv, 2023 (*OPPO*). [[Paper](https://arxiv.org/abs/2303.13397)]
    * **MotionGPT**: "MotionGPT: Finetuned LLMs are General-Purpose Motion Generators", arXiv, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2306.10900)][[PyTorch (in construction)](https://github.com/qiqiApink/MotionGPT)][[Website](https://qiqiapink.github.io/MotionGPT/)]
    * **UNIMASK-M**: "A Unified Masked Autoencoder with Patchified Skeletons for Motion Synthesis", arXiv, 2023 (*Technische Universitat Wien (TUWien), Austria*). [[Paper](https://arxiv.org/abs/2308.07301)][[Website](https://sites.google.com/view/estevevallsmascaro/publications/unimask-m)]
    * **MMM**: "MMM: Generative Masked Motion Model", arXiv, 2023 (*UNC*). [[Paper](https://arxiv.org/abs/2312.03596)][[Code (in construction)](https://github.com/exitudio/MMM/)][[Website](https://exitudio.github.io/MMM-page/)]
    * **HOI-Diff**: "HOI-Diff: Text-Driven Synthesis of 3D Human-Object Interactions using Diffusion Models", arXiv, 2023 (*Northeastern*). [[Paper](https://arxiv.org/abs/2312.06553)][[Website](https://neu-vi.github.io/HOI-Diff/)]
    * **OMG**: "OMG: Towards Open-vocabulary Motion Generation via Mixture of Controllers", arXiv, 2023 (*ShanghaiTech*). [[Paper](https://arxiv.org/abs/2312.08985)]
    * **LEMON**: "LEMON: Learning 3D Human-Object Interaction Relation from 2D Images", arXiv, 2023 (*USTC*). [[Paper](https://arxiv.org/abs/2312.08963)][[Code (in construction)](https://github.com/yyvhang/lemon_3d)][[Website](https://yyvhang.github.io/LEMON/)]
    * **RoHM**: "RoHM: Robust Human Motion Reconstruction via Diffusion", arXiv, 2024 (*Meta*). [[Paper](https://arxiv.org/abs/2401.08570)][[Code (in construction)](https://github.com/sanweiliti/RoHM)][[Website](https://sanweiliti.github.io/ROHM/ROHM.html)]
    * **STMC**: "Multi-Track Timeline Control for Text-Driven 3D Human Motion Generation", arXiv, 2024 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2401.08559)][[Website](https://mathis.petrovich.fr/stmc/)]
* 3D Object Recognition:
    * **MVT**: "MVT: Multi-view Vision Transformer for 3D Object Recognition", BMVC, 2021 (*Baidu*). [[Paper](https://arxiv.org/abs/2110.13083)]
* 3D Reconstruction:
    * **PlaneTR**: "PlaneTR: Structure-Guided Transformers for 3D Plane Recovery", ICCV, 2021 (*Wuhan University*). [[Paper](https://arxiv.org/abs/2107.13108)][[PyTorch](https://github.com/IceTTTb/PlaneTR3D)]
    * **CO3D**: "CommonObjects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction", ICCV, 2021 (*Facebook*). [[Paper](https://arxiv.org/abs/2109.00512)][[PyTorch](https://github.com/facebookresearch/co3d)]
    * **VolT**: "Multi-view 3D Reconstruction with Transformer", ICCV, 2021 (*University of British Columbia*). [[Paper](https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Multi-View_3D_Reconstruction_With_Transformers_ICCV_2021_paper.html)]
    * **3D-RETR**: "3D-RETR: End-to-End Single and Multi-View 3D Reconstruction with Transformers", BMVC, 2021 (*ETHZ*). [[Paper](https://arxiv.org/abs/2110.08861)][[PyTorch](https://github.com/FomalhautB/3D-RETR)]
    * **TransformerFusion**: "TransformerFusion: Monocular RGB Scene Reconstruction using Transformers", NeurIPS, 2021 (*TUM*). [[Paper](https://arxiv.org/abs/2107.02191)][[Website](https://aljazbozic.github.io/transformerfusion/)]
    * **LegoFormer**: "LegoFormer: Transformers for Block-by-Block Multi-view 3D Reconstruction", arXiv, 2021 (*TUM + Google*). [[Paper](https://arxiv.org/abs/2106.12102)]
    * **PlaneFormers**: "PlaneFormers: From Sparse View Planes to 3D Reconstruction", ECCV, 2022 (*UMich*). [[Paper](https://arxiv.org/abs/2208.04307)][[PyTorch](https://github.com/samiragarwala/PlaneFormers)][[Website](https://samiragarwala.github.io/PlaneFormers/)]
    * **3D-C2FT**: "3D-C2FT: Coarse-to-fine Transformer for Multi-view 3D Reconstruction", arXiv, 2022 (*Korea Institute of Science and Technology*). [[Paper](https://arxiv.org/abs/2205.14575)]
    * **SDF-Former**: "Monocular Scene Reconstruction with 3D SDF Transformers", ICLR, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2301.13510)][[Website](https://weihaosky.github.io/sdfformer/)]
    * **AMVUR**: "A Probabilistic Attention Model with Occlusion-aware Texture Regression for 3D Hand Reconstruction from a Single RGB Image", CVPR, 2023 (*Lancaster University, UK*). [[Paper](https://arxiv.org/abs/2304.14299)]
    * **LIST**: "LIST: Learning Implicitly from Spatial Transformers for Single-View 3D Reconstruction", ICCV, 2023 (*UT Arlington*). [[Paper](https://arxiv.org/abs/2307.12194)]
    * **LRGT**: "Long-Range Grouping Transformer for Multi-View 3D Reconstruction", ICCV, 2023 (*Macau University of Science and Technology*). [[Paper](https://arxiv.org/abs/2308.08724)][[PyTorch (in construction)](https://github.com/LiyingCV/Long-Range-Grouping-Transformer)]
    * **Spectral-Graphormer**: "Spectral Graphormer: Spectral Graph-based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images", ICCV, 2023 (*Google*). [[Paper](https://arxiv.org/abs/2308.11015)]
    * **UMIFormer**: "UMIFormer: Mining the Correlations between Similar Tokens for Multi-View 3D Reconstruction", ICCV, 2023 (*Macau University of Science and Technology*). [[Paper](https://arxiv.org/abs/2302.13987)][[PyTorch](https://github.com/GaryZhu1996/UMIFormer)]
    * **PlaneRecTR**: "PlaneRecTR: Unified Query Learning for 3D Plane Recovery from a Single View", ICCV, 2023 (*National University of Defense Technology, China*). [[Paper](https://arxiv.org/abs/2307.13756)][[PyTorch](https://github.com/SJingjia/PlaneRecTR)]
    * **HaMeR**: "Reconstructing Hands in 3D with Transformers", arXiv, 2023 (*Berkeley*). [[Paper](https://arxiv.org/abs/2312.05251)][[PyTorch](https://github.com/geopavlakos/hamer)][[Website](https://geopavlakos.github.io/hamer/)]
* 360 Scene:
    * **?**: "Improving 360 Monocular Depth Estimation via Non-local Dense Prediction Transformer and Joint Supervised and Self-supervised Learning", AAAI, 2022 (*Seoul National University*). [[Paper](https://arxiv.org/abs/2109.10563)][[PyTorch](https://github.com/yuniw18/Joint_360depth)]
    * **PAVER**: "Panoramic Vision Transformer for Saliency Detection in 360° Videos", ECCV, 2022 (*Seoul National University*). [[Paper](https://arxiv.org/abs/2209.08956)]
    * **PanoFormer**: "PanoFormer: Panorama Transformer for Indoor 360° Depth Estimation", ECCV, 2022 (*Beijing Jiaotong University*). [[Paper](https://arxiv.org/abs/2203.09283)]
    * **CoVisPose**: "CoVisPose: Co-Visibility Pose Transformer for Wide-Baseline Relative Pose Estimation in 360° Indoor Panoramas", ECCV, 2022 (*Zillow*). [[Paper](https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/5330_ECCV_2022_paper.php)]
    * **SPH**: "Spherical Transformer", arXiv, 2022 (*Chung-Ang University, Korea*). [[Paper](https://arxiv.org/abs/2202.04942)]
    * **PanoSwin**: "PanoSwin: a Pano-style Swin Transformer for Panorama Understanding", CVPR, 2023 (*Fudan*). [[Paper](https://arxiv.org/abs/2308.14726)][[PyTorch](https://github.com/1069066484/PanoSwinTransformerObjectDetection)]
    * **SalViT360**: "Spherical Vision Transformer for 360-degree Video Saliency Prediction", BMVC, 2023 (*Koc University, Turkey*). [[Paper](https://arxiv.org/abs/2308.13004)]
    * **PanoContext-Former**: "PanoContext-Former: Panoramic Total Scene Understanding with a Transformer", arXiv, 2023 (*Alibaba*). [[Paper](https://arxiv.org/abs/2305.12497)]
* Others:
    * **?**: "Connecting Compression Spaces with Transformer for Approximate Nearest Neighbor Search", ECCV, 2022 (*Intellifusion, China*). [[Paper](https://arxiv.org/abs/2107.14415)]
    * **?**: "Strong Gravitational Lensing Parameter Estimation with Vision Transformer", ECCVW, 2022 (*CMU*). [[Paper](https://arxiv.org/abs/2210.04143)][[PyTorch](https://github.com/kuanweih/strong_lensing_vit_resnet)]
    * **Transformer-DR**: "Transformer-based dimensionality reduction", arXiv, 2022 (*Chongqing Normal University, China*). [[Paper](https://arxiv.org/abs/2210.08288)]
    * **?**: "mm-Wave Radar Hand Shape Classification Using Deformable Transformers", arXiv, 2022 (*Intel*). [[Paper](https://arxiv.org/abs/2210.13079)]
    * **?**: "Fully-attentive and interpretable: vision and video vision transformers for pain detection", NeurIPSW, 2022 (*Utrecht University, Netherlands*). [[Paper](https://arxiv.org/abs/2210.15769)][[Code (in construction)](https://github.com/IPDTFE/ViT-McMaster)]
    * **CQFormer**: "Name Your Colour For the Task: Artificially Discover Colour Naming via Colour Quantisation Transformer", ICCV, 2023 (*Shanghai Jiao Tong*). [[Paper](https://arxiv.org/abs/2212.03434)][[PyTorch](https://github.com/ryeocthiv/CQFormer)]
    * **CircuitFormer**: "Circuit as Set of Points", NeurIPS, 2023 (*Horizon Robotics*). [[Paper](https://arxiv.org/abs/2310.17418)][[PyTorch](https://github.com/hustvl/circuitformer)]

[[Back to Overview](#overview)]

---

## Attention Mechanisms in Vision/NLP
### Attention for Vision
* **AA**: "Attention Augmented Convolutional Networks", ICCV, 2019 (*Google*). [[Paper](https://arxiv.org/abs/1904.09925)][[PyTorch (Unofficial)](https://github.com/leaderj1001/Attention-Augmented-Conv2d)][[Tensorflow (Unofficial)](https://github.com/titu1994/keras-attention-augmented-convs)]
* **LR-Net**: "Local Relation Networks for Image Recognition", ICCV, 2019 (*Microsoft*). [[Paper](https://arxiv.org/abs/1904.11491)][[PyTorch (Unofficial)](https://github.com/gan3sh500/local-relational-nets)]
* **CCNet**: "CCNet: Criss-Cross Attention for Semantic Segmentation", ICCV, 2019 (& TPAMI 2020) (*Horizon*). [[Paper](https://arxiv.org/abs/1811.11721)][[PyTorch](https://github.com/speedinghzl/CCNet)]
* **GCNet**: "Global Context Networks", ICCVW, 2019 (& TPAMI 2020) (*Microsoft*). [[Paper](https://arxiv.org/abs/2012.13375)][[PyTorch](https://github.com/xvjiarui/GCNet)]
* **SASA**: "Stand-Alone Self-Attention in Vision Models", NeurIPS, 2019 (*Google*). [[Paper](https://arxiv.org/abs/1906.05909)][[PyTorch-1 (Unofficial)](https://github.com/leaderj1001/Stand-Alone-Self-Attention)][[PyTorch-2 (Unofficial)](https://github.com/MerHS/SASA-pytorch)]
    * key message: attention module is more efficient than conv & provide comparable accuracy
* **Axial-Transformer**: "Axial Attention in Multidimensional Transformers", arXiv, 2019 (*Google*). [[Paper](https://openreview.net/forum?id=H1e5GJBtDr)][[PyTorch (Unofficial)](https://github.com/lucidrains/axial-attention)]
* **Attention-CNN**: "On the Relationship between Self-Attention and Convolutional Layers", ICLR, 2020 (*EPFL*). [[Paper](https://openreview.net/forum?id=HJlnC1rKPB)][[PyTorch](https://github.com/epfml/attention-cnn)][[Website](https://epfml.github.io/attention-cnn/)]
* **SAN**: "Exploring Self-attention for Image Recognition", CVPR, 2020 (*CUHK + Intel*). [[Paper](https://arxiv.org/abs/2004.13621)][[PyTorch](https://github.com/hszhao/SAN)]
* **BA-Transform**: "Non-Local Neural Networks With Grouped Bilinear Attentional Transforms", CVPR, 2020 (*ByteDance*). [[Paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Chi_Non-Local_Neural_Networks_With_Grouped_Bilinear_Attentional_Transforms_CVPR_2020_paper.html)]
* **Axial-DeepLab**: "Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation", ECCV, 2020 (*Google*). [[Paper](https://arxiv.org/abs/2003.07853)][[PyTorch](https://github.com/csrhddlam/axial-deeplab)]
* **GSA**: "Global Self-Attention Networks for Image Recognition", arXiv, 2020 (*Google*). [[Paper](https://openreview.net/forum?id=KiFeuZu24k)][[PyTorch (Unofficial)](https://github.com/lucidrains/global-self-attention-network)]
* **EA**: "Efficient Attention: Attention with Linear Complexities", WACV, 2021 (*SenseTime*). [[Paper](https://arxiv.org/abs/1812.01243)][[PyTorch](https://github.com/cmsflash/efficient-attention)]
* **LambdaNetworks**: "LambdaNetworks: Modeling long-range Interactions without Attention", ICLR, 2021 (*Google*). [[Paper](https://openreview.net/forum?id=xTJEN-ggl1b)][[PyTorch-1 (Unofficial)](https://github.com/lucidrains/lambda-networks)][[PyTorch-2 (Unofficial)](https://github.com/leaderj1001/LambdaNetworks)]
* **GSA-Nets**: "Group Equivariant Stand-Alone Self-Attention For Vision", ICLR, 2021 (*EPFL*). [[Paper](https://openreview.net/forum?id=JkfYjnOEo6M)]
* **Hamburger**: "Is Attention Better Than Matrix Decomposition?", ICLR, 2021 (*Peking*). [[Paper](https://openreview.net/forum?id=1FvkSpWosOl)][[PyTorch (Unofficial)](https://github.com/lucidrains/hamburger-pytorch)]
* **HaloNet**: "Scaling Local Self-Attention For Parameter Efficient Visual Backbones", CVPR, 2021 (*Google*). [[Paper](https://arxiv.org/abs/2103.12731)]
* **BoTNet**: "Bottleneck Transformers for Visual Recognition", CVPR, 2021 (*Google*). [[Paper](https://arxiv.org/abs/2101.11605)]
* **SSAN**: "SSAN: Separable Self-Attention Network for Video Representation Learning", CVPR, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2105.13033)]
* **CoTNet**: "Contextual Transformer Networks for Visual Recognition", CVPRW, 2021 (*JD*). [[Paper](https://arxiv.org/abs/2107.12292)][[PyTorch](https://github.com/JDAI-CV/CoTNet)]
* **Involution**: "Involution: Inverting the Inherence of Convolution for Visual Recognition", CVPR, 2021 (*HKUST*). [[Paper](https://arxiv.org/abs/2103.06255)][[PyTorch](https://github.com/d-li14/involution)]
* **Perceiver**: "Perceiver: General Perception with Iterative Attention", ICML, 2021 (*DeepMind*). [[Paper](https://arxiv.org/abs/2103.03206)][[PyTorch (lucidrains)](https://github.com/lucidrains/perceiver-pytorch)]
* **SNL**: "Unifying Nonlocal Blocks for Neural Networks", ICCV, 2021 (*Peking + Bytedance*). [[Paper](https://arxiv.org/abs/2108.02451)]
* **External-Attention**: "Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks", arXiv, 2021 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2105.02358)]
* **Container**: "Container: Context Aggregation Network", arXiv, 2021 (*AI2*). [[Paper](https://arxiv.org/abs/2106.01401)]
* **X-volution**: "X-volution: On the unification of convolution and self-attention", arXiv, 2021 (*Huawei Hisilicon*). [[Paper](https://arxiv.org/abs/2106.02253)]
* **Invertible-Attention**: "Invertible Attention", arXiv, 2021 (*ANU*). [[Paper](https://arxiv.org/abs/2106.09003)]
* **VOLO**: "VOLO: Vision Outlooker for Visual Recognition", arXiv, 2021 (*Sea AI Lab + NUS, Singapore*). [[Paper](https://arxiv.org/abs/2106.13112)][[PyTorch](https://github.com/sail-sg/volo)]
* **LESA**: "Locally Enhanced Self-Attention: Rethinking Self-Attention as Local and Context Terms", arXiv, 2021 (*Johns Hopkins*). [[Paper](https://arxiv.org/abs/2107.05637)]
* **PS-Attention**: "Pale Transformer: A General Vision Transformer Backbone with Pale-Shaped Attention", AAAI, 2022 (*Baidu*). [[Paper](https://arxiv.org/abs/2112.14000)][[Paddle](https://github.com/BR-IDL/PaddleViT)]
* **QuadTree**: "QuadTree Attention for Vision Transformers", ICLR, 2022 (*Simon Fraser + Alibaba*). [[Paper](https://arxiv.org/abs/2201.02767)][[PyTorch](https://github.com/Tangshitao/QuadtreeAttention)]
* **QnA**: "Learned Queries for Efficient Local Attention", CVPR, 2022 (*Tel-Aviv*). [[Paper](https://arxiv.org/abs/2112.11435)][[JAX](https://github.com/moabarar/qna)]
* **?**: "Fair Comparison between Efficient Attentions", CVPRW, 2022 (*Kyungpook National University, Korea*). [[Paper](https://arxiv.org/abs/2206.00244)][[PyTorch](https://github.com/CreamNuts/Fair-Comparison-between-Efficient-Attentions)]
* **KVT**: "KVT: k-NN Attention for Boosting Vision Transformers", ECCV, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2106.00515)][[PyTorch](https://github.com/damo-cv/KVT)]
* **Hydra**: "Hydra Attention: Efficient Attention with Many Heads", ECCVW, 2022 (*Meta*). [[Paper](https://arxiv.org/abs/2209.07484)]
* **HiP**: "Hierarchical Perceiver", arXiv, 2022 (*DeepMind*). [[Paper](https://arxiv.org/abs/2202.10890)]
* **AttendNeXt**: "Faster Attention Is What You Need: A Fast Self-Attention Neural Network Backbone Architecture for the Edge via Double-Condensing Attention Condensers", arXiv, 2022 (*University of Waterloo, Canada*). [[Paper](https://arxiv.org/abs/2208.06980)]
* **Token-Mixing-Adaptive-FNO**: "Efficient Token Mixing for Transformers via Adaptive Fourier Neural Operators", ICLR, 2022 (*NVIDIA + Caltech + Stanford*). [[Paper](https://openreview.net/forum?id=EXHG-A3jlM)][[PyTorch](https://github.com/NVlabs/AFNO-transformer)]
* **KV-Transformer**: "Key-Value Transformer", arXiv, 2023 (*Quintic AI*). [[Paper](https://arxiv.org/abs/2305.19129)]

[[Back to Overview](#overview)]

### Attention for NLP
* **T-DMCA**: "Generating Wikipedia by Summarizing Long Sequences", ICLR, 2018 (*Google*). [[Paper](https://openreview.net/forum?id=Hyg0vbWC-)]
* **LSRA**: "Lite Transformer with Long-Short Range Attention", ICLR, 2020 (*MIT*). [[Paper](https://openreview.net/forum?id=ByeMPlHKPH)][[PyTorch](https://github.com/mit-han-lab/lite-transformer)]
* **ETC**: "ETC: Encoding Long and Structured Inputs in Transformers", EMNLP, 2020 (*Google*). [[Paper](https://arxiv.org/abs/2004.08483)][[Tensorflow](https://github.com/google-research/google-research/tree/master/etcmodel)]
* **BlockBERT**: "Blockwise Self-Attention for Long Document Understanding", EMNLP Findings, 2020 (*Facebook*). [[Paper](https://arxiv.org/abs/1911.02972)][[GitHub](https://github.com/xptree/BlockBERT)]
* **Clustered-Attention**: "Fast Transformers with Clustered Attention", NeurIPS, 2020 (*Idiap*). [[Paper](https://arxiv.org/abs/2007.04825)][[PyTorch](https://github.com/idiap/fast-transformers)][[Website](https://clustered-transformers.github.io/)]
* **BigBird**: "Big Bird: Transformers for Longer Sequences", NeurIPS, 2020 (*Google*). [[Paper](https://arxiv.org/abs/2007.14062)][[Tensorflow](https://github.com/google-research/bigbird)]
* **Longformer**: "Longformer: The Long-Document Transformer", arXiv, 2020 (*AI2*). [[Paper](https://arxiv.org/abs/2004.05150)][[PyTorch](https://github.com/allenai/longformer)]
* **Linformer**: "Linformer: Self-Attention with Linear Complexity", arXiv, 2020 (*Facebook*). [[Paper](https://arxiv.org/abs/2006.04768)][[PyTorch (Unofficial)](https://github.com/tatp22/linformer-pytorch)]
* **Nystromformer**: "Nyströmformer: A Nyström-Based Algorithm for Approximating Self-Attention", AAAI, 2021 (*UW-Madison*). [[Paper](https://arxiv.org/abs/2102.03902)][[PyTorch](https://github.com/mlpen/Nystromformer)]
* **RFA**: "Random Feature Attention", ICLR, 2021 (*DeepMind*). [[Paper](https://openreview.net/forum?id=QtTKTdVrFBB)]
* **Performer**: "Rethinking Attention with Performers", ICLR, 2021 (*Google*). [[Paper](https://openreview.net/forum?id=Ua6zuk0WRH)][[Code](https://github.com/google-research/google-research/tree/master/performer)][[Blog](https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html)]
* **DeLight**: "DeLighT: Deep and Light-weight Transformer", ICLR, 2021 (*UW*). [[Paper](https://openreview.net/forum?id=ujmgfuxSLrO)]
* **Synthesizer**: "Synthesizer: Rethinking Self-Attention for Transformer Models", ICML, 2021 (*Google*). [[Paper](https://arxiv.org/abs/2005.00743)][[Tensorflow](https://github.com/tensorflow/mesh)][[PyTorch (leaderj1001)](https://github.com/leaderj1001/Synthesizer-Rethinking-Self-Attention-Transformer-Models)]
* **Poolingformer**: "Poolingformer: Long Document Modeling with Pooling Attention", ICML, 2021 (*Microsoft*). [[Paper](https://arxiv.org/abs/2105.04371)]
* **Hi-Transformer**: "Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling", ACL, 2021 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2106.01040)]
* **Smart-Bird**: "Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer", arXiv, 2021 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2108.09193)]
* **Fastformer**: "Fastformer: Additive Attention is All You Need", arXiv, 2021 (*Tsinghua*). [[Paper](https://arxiv.org/abs/2108.09084)]
* **∞-former**: "∞-former: Infinite Memory Transformer", arXiv, 2021 (*Instituto de Telecomunicações, Portugal*). [[Paper](https://arxiv.org/abs/2109.00301)]
* **cosFormer**: "cosFormer: Rethinking Softmax In Attention", ICLR, 2022 (*SenseTime*). [[Paper](https://openreview.net/forum?id=Bl8CQrx2Up4)][[PyTorch (davidsvy)](https://github.com/davidsvy/cosformer-pytorch)]
* **MGK**: "Improving Transformers with Probabilistic Attention Keys", ICML, 2022 (*UCLA*). [[Paper](https://arxiv.org/abs/2110.08678)]
* **FNet**: "FNet: Mixing Tokens with Fourier Transforms", NAACL, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2105.03824)]
* **RetNet**: "Retentive Network: A Successor to Transformer for Large Language Models", arXiv, 2023 (*Microsoft*). [[Paper](https://arxiv.org/abs/2307.08621)][[PyTorch (in construction)](https://github.com/microsoft/unilm/tree/master/retnet)]
* **ReBased**: "Linear Transformers with Learnable Kernel Functions are Better In-Context Models", arXiv, 2024 (*Tinkoff*). [[Paper](https://arxiv.org/abs/2402.10644)]

[[Back to Overview](#overview)]

### Attention for Both
* **Sparse-Transformer**: "Generating Long Sequences with Sparse Transformers", arXiv, 2019 (*OpenAI*). [[Paper](https://arxiv.org/abs/1904.10509)][[Tensorflow](https://github.com/openai/sparse_attention)][[Blog](https://openai.com/blog/sparse-transformer/)]
* **Reformer**: "Reformer: The Efficient Transformer", ICLR, 2020 (*Google*). [[Paper](https://openreview.net/forum?id=rkgNKkHtvB)][[Tensorflow](https://github.com/google/trax/tree/master/trax/models/reformer)][[Blog](https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html)] 
* **Sinkhorn-Transformer**: "Sparse Sinkhorn Attention", ICML, 2020 (*Google*). [[Paper](https://arxiv.org/abs/2002.11296)][[PyTorch (Unofficial)](https://github.com/lucidrains/sinkhorn-transformer)]
* **Linear-Transformer**: "Transformers are rnns: Fast autoregressive transformers with linear attention", ICML, 2020 (*Idiap*). [[Paper](https://arxiv.org/abs/2006.16236)][[PyTorch](https://github.com/idiap/fast-transformers)][[Website](https://linear-transformers.com/)]
* **SMYRF**: "SMYRF: Efficient Attention using Asymmetric Clustering", NeurIPS, 2020 (*UT Austin + Google*). [[Paper](https://arxiv.org/abs/2010.05315)][[PyTorch](https://github.com/giannisdaras/smyrf)]
* **Routing-Transformer**: "Efficient Content-Based Sparse Attention with Routing Transformers", TACL, 2021 (*Google*). [[Paper](https://arxiv.org/abs/2003.05997)][[Tensorflow](https://github.com/google-research/google-research/tree/master/routing_transformer)][[PyTorch (Unofficial)](https://github.com/lucidrains/routing-transformer)][[Slides](https://drive.google.com/file/d/1maX-UQbtnVtxQqLmHvWVN6LNYtnBaTd9/view)]
* **LRA**: "Long Range Arena: A Benchmark for Efficient Transformers", ICLR, 2021 (*Google*). [[Paper](https://openreview.net/forum?id=qVyeW-grC2k)][[Tensorflow](https://github.com/google-research/long-range-arena)]
* **OmniNet**: "OmniNet: Omnidirectional Representations from Transformers", ICML, 2021 (*Google*). [[Paper](https://arxiv.org/abs/2103.01075)]
* **Evolving-Attention**: "Evolving Attention with Residual Convolutions", ICML, 2021 (*Peking + Microsoft*). [[Paper](https://arxiv.org/abs/2102.12895)]
* **H-Transformer-1D**: "H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences", ACL, 2021 (*Google*). [[Paper](https://arxiv.org/abs/2107.11906)]
* **Combiner**: "Combiner: Full Attention Transformer with Sparse Computation Cost", NeurIPS, 2021 (*Google*). [[Paper](https://arxiv.org/abs/2107.05768)]
* **Centroid-Transformer**: "Centroid Transformers: Learning to Abstract with Attention", arXiv, 2021 (*UT Austin*). [[Paper](https://arxiv.org/abs/2102.08606)]
* **AFT**: "An Attention Free Transformer", arXiv, 2021 (*Apple*). [[Paper](https://arxiv.org/abs/2105.14103)]
* **Luna**: "Luna: Linear Unified Nested Attention", arXiv, 2021 (*USC + CMU + Facebook*). [[Paper](https://arxiv.org/abs/2106.01540)]
* **Transformer-LS**: "Long-Short Transformer: Efficient Transformers for Language and Vision", arXiv, 2021 (*NVIDIA*). [[Paper](https://arxiv.org/abs/2107.02192)]
* **PoNet**: "PoNet: Pooling Network for Efficient Token Mixing in Long Sequences", ICLR, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2110.02442)]
* **Paramixer**: "Paramixer: Parameterizing Mixing Links in Sparse Factors Works Better Than Dot-Product Self-Attention", CVPR, 2022 (*Norwegian University of Science and Technology, Norway*). [[Paper](https://openaccess.thecvf.com/content/CVPR2022/html/Yu_Paramixer_Parameterizing_Mixing_Links_in_Sparse_Factors_Works_Better_Than_CVPR_2022_paper.html)]
* **FNet**: "FNet: Mixing Tokens with Fourier Transforms", NAACL, 2022 (*Google*). [[Paper](https://arxiv.org/abs/2105.03824)][[JAX](https://github.com/google-research/google-research/tree/master/f_net)]
* **ContextPool**: "Efficient Representation Learning via Adaptive Context Pooling", ICML, 2022 (*Apple*). [[Paper](https://arxiv.org/abs/2207.01844)]
* **LARA**: "Linear Complexity Randomized Self-attention Mechanism", ICML, 2022 (*Bytedance*). [[Paper](https://arxiv.org/abs/2204.04667)]
* **Flowformer**: "Flowformer: Linearizing Transformers with Conservation Flows", ICML, 2022 (*Tsinghua University*). [[Paper](https://arxiv.org/abs/2202.06258)][[PyTorch](https://github.com/thuml/Flowformer)]
* **MRA**: "Multi Resolution Analysis (MRA) for Approximate Self-Attention", ICML, 2022 (*University of Wisconsin, Madison*). [[Paper](https://proceedings.mlr.press/v162/zeng22a.html)][[PyTorch](https://github.com/mlpen/mra-attention)]
* **EcoFormer**: "EcoFormer: Energy-Saving Attention with Linear Complexity", NeurIPS, 2022 (*Monash University*). [[Paper](https://arxiv.org/abs/2209.09004)][[PyTorch](https://github.com/ziplab/EcoFormer)]
* **SBM-Transformer**: "Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost", NeurIPS, 2022 (*LG*). [[Paper](https://arxiv.org/abs/2210.15541)][[PyTorch](https://github.com/sc782/SBM-Transformer)]
* **?**: "Horizontal and Vertical Attention in Transformers", arXiv, 2022 (*University of Technology Sydney*). [[Paper](https://arxiv.org/abs/2207.04399)]
* **MRL**: "MRL: Learning to Mix with Attention and Convolutions", arXiv, 2022 (*Sony*). [[Paper](https://arxiv.org/abs/2208.13975)]
* **RSA**: "Encoding Recurrence into Transformers", ICLR, 2023 (*HKU*). [[Paper](https://openreview.net/forum?id=7YfHla7IxBJ)]
* **EVA**: "Efficient Attention via Control Variates", ICLR, 2023 (*HKU*). [[Paper](https://openreview.net/forum?id=G-uNfHKrj46)]
* **STTABT**: "Sparse Token Transformer with Attention Back Tracking", ICLR, 2023 (*KAIST*). [[Paper](https://openreview.net/forum?id=VV0hSE8AxCw)]
* **Mega**: "Mega: Moving Average Equipped Gated Attention", ICLR, 2023 (*Meta*). [[Paper](https://arxiv.org/abs/2209.10655)][[PyTorch](https://github.com/facebookresearch/mega)]
* **SeTformer**: "SeTformer is What You Need for Vision and Language", AAAI, 2024 (*East China Normal University*). [[Paper](https://arxiv.org/abs/2401.03540)]
* **Hedgehog**: "The Hedgehog & the Porcupine: Expressive Linear Attentions with Softmax Mimicry", arXiv, 2024 (*Stanford University*). [[Paper](https://arxiv.org/abs/2402.04347)]

[[Back to Overview](#overview)]

### Attention for Others
* **Informer**: "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting", AAAI, 2021 (*Beihang University*). [[Paper](https://arxiv.org/abs/2012.07436)][[PyTorch](https://github.com/zhouhaoyi/Informer2020)]
* **Attention-Rank-Collapse**: "Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth", ICML, 2021 (*Google + EPFL*). [[Paper](https://arxiv.org/abs/2103.03404)][[PyTorch](https://github.com/twistedcubic/attention-rank-collapse)]
* **NPT**: "Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning", arXiv, 2021 (*Oxford*). [[Paper](https://arxiv.org/abs/2106.02584)]
* **FEDformer**: "FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting", ICML, 2022 (*Alibaba*). [[Paper](https://arxiv.org/abs/2201.12740)][[PyTorch](https://github.com/DAMO-DI-ML/ICML2022-FEDformer)]
* **?**: "Generalizable Memory-driven Transformer for Multivariate Long Sequence Time-series Forecasting", arXiv, 2022 (*University of Technology Sydney*). [[Paper](https://arxiv.org/abs/2207.07827)]

[[Back to Overview](#overview)]
